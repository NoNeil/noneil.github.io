<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Keras安装（CPU版）（macOS 10.13.4）]]></title>
    <url>%2F2018%2F04%2F22%2FKeras%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[安装Miniconda在清华镜像站下载Miniconda安装包，地址：https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/ 我下载的最新版：Miniconda3-latest-MacOSX-x86_64.sh123$ cd ~/Download$ chmod +x Miniconda3-latest-MacOSX-x86_64.sh$ ./Miniconda3-latest-MacOSX-x86_64.sh 根据提示进行安装即可，该脚本会自动将~/Miniconda3/bin目录添加到环境变量中。123456$ cat ~/.bash_profile# added by Miniconda3 installerexport PATH="/Users/Neil/miniconda3/bin:$PATH"# 使环境变量生效$ source ~/.bash_profile 安装keras1$ pip install keras keras默认的深度学习包是TensorFlow，如果要修改成Theano，编辑~/.keras/keras.json1234567$ cat ~/.keras/keras.json&#123; "image_data_format": "channels_last", "floatx": "float32", "epsilon": 1e-07, "backend": "tensorflow"&#125;% 安装TensorFlow1$ pip install tensorflow 验证是否安装成功123456789101112➜ bin pythonPython 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 12:04:33)[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwinType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; import kerasUsing TensorFlow backend.&gt;&gt;&gt; print (keras.__version__)2.1.5&gt;&gt;&gt; import tensorflow as tf&gt;&gt;&gt; print (tf.__version__)1.7.0&gt;&gt;&gt; 下载Keras example123$ git clone https://github.com/fchollet/keras.git$ cd keras/examples/$ python mnist_mlp.py 附Keras中文文档：http://keras-cn.readthedocs.io/en/latest/]]></content>
      <categories>
        <category>Keras</category>
      </categories>
      <tags>
        <tag>Miniconda</tag>
        <tag>Keras</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李宏毅机器学习2017学习笔记:8.Backpropagation]]></title>
    <url>%2F2018%2F04%2F21%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02017%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-8-Backpropagation%2F</url>
    <content type="text"><![CDATA[链式求导法则Case 2: 当改变s时，会通过函数g(s)和h(s)改变x和y，进而通过k(x,y)改变z。 反向传播（Backpropagation）假设损失函数为$L(\theta)$： 计算参数w的偏导：$\partial z / \partial w$，称之为向前传播。 计算激活值z的偏导：$\partial C / \partial z$，称之为向后传播。 向前传播$z = w*x$，因此$\partial z / \partial w = x$ 向后传播假设： 激活函数是sigmoid函数； 只有一个隐含层； 每一层只有2个神经元。 那么： $\partial a / \partial z$，也就是对激活函数求偏导； $\partial z / \partial a = w$，这个很直观； 要求左边的$\partial C / \partial z$，必须先求右边的$\partial C / \partial z’$和$\partial C / \partial z’’$。因此，可以从右往左，依次求解$\partial C / \partial z$。 整理一下，可以得到：其中，$\sigma’(z)=\sigma(z)(1-\sigma(z))$可以轻松求解。 可以把『反向传播』想象成另一个神经元： 输入是：后面的$\partial C / \partial z$ 激活函数是：乘上一个已知的数$\delta’(z)$，类似『放大器』的功能 如果是最后一层:直接求解$\partial C / \partial z$ 如果不是最后一层:依次递归求解，直到最后一层。『向后传播』时，类似一个反向的神经网络。从右往左计算，计算量跟『向前传播』的计算量一样。 小结 在向前传播中，我们求得了$\partial z / \partial w$ 在向后传播中，我们求得了$\partial C / \partial z$ 将上述两者相乘，就得到了参数的微分：$\partial C / \partial w$ 总结 链式求导法则 向前传播求解$\partial z / \partial w$，向后传播求解$\partial C / \partial z$，两者相乘就是参数w的微分 向后传播可以看做一个反向的神经网络，能从右向左依次求出$\partial C / \partial z$]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>李宏毅</tag>
        <tag>BP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李宏毅机器学习2017学习笔记:7.Brief Introduction of Deep Learning]]></title>
    <url>%2F2018%2F04%2F21%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02017%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-7-Brief-Introduction-of-Deep-Learning%2F</url>
    <content type="text"><![CDATA[深度学习的发展史 深度学习方法Deep Learning 的三个步骤跟机器学习的三个步骤一样： 定义一个模型 评价模型的好坏（损失函数） 求解模型的最优参数备注：function set是指一个模型；一个function是指参数已经确定的模型，给定一个输入，就会有个输出。 定义一个模型最简单的深度学习模型就是『全连接前馈网络』，由一个输入层、多个隐含层、一个输出层组成。 各种DL模型的层数对比： 2012，AlexNet，8层，16.4%的错误率； 2014，VGG，19层，7.3%的错误率； 2014，GoogleNet，6.7%的错误率； 2015，Residual Net，3.57%的错误率。 我们可以把中间隐含层部分看作一个特征提取的模块。最后一个输出层用Softmax。 举个例子：手写数字识别需要设计每层的神经元的个数。 FAQ： Q:选择多少层？每层多少个神经元？A: 根据训练处的误差，凭直觉进行调整 Q:网络结构能自动设置吗？A: 能，例如：Evolutionary Artificial Neural Network Q: 能否设计其他的网络结构？A: 能，例如：CNN 评价模型的好坏（损失函数）『手写字符识别』的例子中，用Cross Entropy作损失函数。 求解模型的最优参数使用Gradient Descent求解最优的参数。 有很多深度学习框架来帮你计算微分：TensorFlow、torch、theano、Caffe、Microsoft CNTK、chainer、DSSTNE、mxnet、libdnn 为什么要用深度学习？ 有些实验表明，网络越深，效果越好。 理论证明，一个隐含层的网络能表示任意函数。理论上，不一定要『深』，也可以『宽』。 总结： 介绍了深度学习的发展史 深度学习的3个步骤，与机器学习的三个步骤一样。以全连接的前馈神经网络为例阐述着三个步骤：建模（定义网络结构）、定义损失函数、求解最优参数。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>李宏毅</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李宏毅机器学习2017学习笔记:5.Logistics Regression]]></title>
    <url>%2F2018%2F04%2F06%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02017%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-5-Logistic-Regression%2F</url>
    <content type="text"><![CDATA[Logistic 回归 V.S. Linear 回归Logistic 回归 模型：$f_{w.b}(x) = \delta \left( w \cdot x+b \right)$，输出值的范围是：$(0, 1)$ 损失函数：$ L(f) = \sum_{j=1}^n C(f(x_j), \hat y^j) $， 其中$ C(f(x_j), \hat y^j) = -\left[ \hat y^j ln f(x^j) + (1-y^j) ln (1-f(x^j))\right]$ 优化方法：$ w_i = w_i - \eta \sum_{j=1}^n -(\hat y^j - f(x^j)) \cdot x_i $ Linear 回归 模型：$f_{w.b}(x) = w \cdot x+b $，输出值的范围是：$(-\infty, \infty)$ 损失函数：$ L(f) = \frac 1 2 \sum_i (f(x_i) - \hat y^i)^2 $ 优化方法：$ w_i = w_i - \eta \sum_{j=1}^n -(\hat y^j - f(x^j)) \cdot x_i $ 如下图，Logistic 回归与Linear 回归的区别： 模型不一样：前者是sigmoid函数，后者是线性函数。 损失函数不一样，前者一般采用交叉熵，后者使用平方误差。 参数更新方式的数学表达式的形式上一样。 Logistic 回归的损失函数是两个伯努利的交叉熵。交叉熵代表的是$p(x)$与$q(x)$两个分布有多接近（也就是说模型的输出值与label一致）。如果两个分布一样的话，交叉熵的值为0. 如果$\hat y^i = 1$，且$f(x^i) = 1$，那么$H(p,q)=0$. 如果$\hat y^i = 0$，且$f(x^i) = 0$，那么$H(p,q)=0$. Logistic 回归的优化方法记住，$\sigma(z)$对$z$的导数为：$$ \frac {\partial \sigma(z)} {\partial z} = \sigma(z) (1-\sigma(z))$$ 备注：怎么记sigmoid函数的偏导？如下图，sigmoid函数的陡峭程度从左往右先逐渐增加，到x=0的位置最大，然后逐渐减小。所以sigmoid函数的导数是个开口朝下的函数，并且左右对称。当$\delta(z)=0.5$时，sigmoid函数的导数取最大值，也就是说sigmoid函数的导数以x=0.5的坐标轴左右对称。基于以上分析，sigmoid的导数是$\sigma(z) (1-\sigma(z))$的形式。 那么，$ln \sigma(z)$对$z$的导数为：$$ \frac {\partial ln\sigma(z)} {\partial z} = \frac 1 {\sigma(z)} \sigma(z) (1-\sigma(z)) = (1-\sigma(z))$$ 那么，$1 - ln \sigma(z)$对$z$的导数为：$$ \frac {\partial (1-ln\sigma(z))} {\partial z} = - \frac 1 {1 - \sigma(z)} \sigma(z) (1-\sigma(z)) = -\sigma(z)$$ 其中，$z=wx+b$，所以$z$对$w$的偏导为$x$：$ \frac {\partial z} {\partial w} = x$ 可以很容易的得到$-lnL(w,b)$对$w$的偏导：$\hat y$与$f_{w,b}(x)$的差异越大（预测结果与目标越大），梯度越大。 Logistic 回归 + Square Error如果使用平方误差作为Logistic回归的损失函数.损失函数和梯度如下：这个梯度有个问题：当$\hat y = 1$时，无论$f_{w,b}(x)$为1还是0，梯度都为0. 当$f_{w,b}(x)=1$时，$\partial L / \partial w = 0$ 当$f_{w,b}(x)=0$时，$\partial L / \partial w = 0$ 交叉熵 V.S. 平方误差分别使用交叉熵和平方误差作为损失函数，当参数变化时，损失函数的变化如下图所示： 交叉熵比较陡峭，随着参数变化而变化很大；平方误差则比较平坦。 平方误差的微分总是很小，不好优化。当微分值很小时，可能离目标很远，我们需要调大步长；但是实际上可能离目标很近，应该调小步长。 判别式模型 V.S. 产生式模型两种模型的比较两种模型都是sigmoid函数，不同的是参数优化方法不同，对数据分布的假设不同。 判别式模型：Logistic 回归 直接用梯度下降法，迭代求出$w$和$b$。 对数据的分布没有任何假设。 产生式模型：概率高斯模型 先求出两类数据的均值，和共同的方差，然后求出的$w$和$b$。 假设数据的分布服从高斯分布、假设服从伯努利分布、假设属性之间不相关。 这两种方法的模型一样，求解参数的方法不一样，求出的$w$和$b$也不一样。因为两种方法对数据分布的假设不一样。两种方法的正确率也不一样。LR对数据分布假设的依赖很小，它的效果更好。当我们不知道数据的分布情况时，可以试试LR，一般能得到不错的效果。 一般而言，产生式模型的分类效果比产生式模型的更好。举个例子： 类1：有1个样本，x1和x2的取值都为1； 类2：有12个样本，其中，4个样本x1和x2的取值分别为1和0，4个样本x1和x2的取值分别为0和1，4个样本x1和x2的取值都为0. 问：一个x1和x2的取值都为1的样本为哪类？ 求解出来的$P(C_1|x) &lt; 0.5$，属于类2.因为朴素贝叶斯的假设是：x1和x2是不相关的。 所以,在类2中，虽然没有x1和x2都为1的样本，但是x1和x2都为1的概率为1/9 而在类1中，x1和x2都为1的概率为1，远远高于类2中出现x1和x2都为1的概率。 但是，朴素贝叶斯还考虑$P(C_1)$和$P(C_2)$的大小，分别为1/13和12/13，综合考虑的话，这个样本属于类2的概率比类1的概率还高。 有时候，产生式模型的效果更好： 训练数据很少的情况。对训练数据有假设的话，模型能对数据起到补充的效果。 对噪声比较鲁棒。比如上面举的一个例子，类1中的那个样本可能是个噪声。产生式模型能对这个噪声分为类2，但是LR坚持将它分为类1. 先验概率和类条件概率可以通过不同的数据源计算得来。 多分类的问题对于多个类别($K&gt;2$)的情况，我们有$$ P(C_k|x) = \frac {P(x|C_k)P(C_k)} {\sum_i P(x|C_i) P(C_i)} = \frac {exp(z_k)} {\sum_iexp(z_i)} $$它被称为归一化指数（normalized exponential），可以被当做logistic sigmoid函数对于多类情况的推广。其中，$z_k = lnP(x|C_k)P(C_k) $归一化指数也被称为softmax函数，因为它表示max函数的一个平滑版本。对于所有的$i \neq k$都有$z_k &gt;&gt; z_i$，那么$P(C_k|x) \simeq 1$且$P(C_k|i) \simeq 0$。 Logistic回归的局限性LR是线性的，无法解决线性不可分的问题。 特征变换一种解决办法是，进行特征变换。变换后的点，能线性可分。但是实际情况下，很难找到一种合适的变换。 新的特征第一个维度$x_1^{‘}$表示为：点到(0, 0)的距离；新的特征第一个维度$x_2^{‘}$表示为：点到(1, 1)的距离； 级联Logistic 回归 模型级联2层LR模型，第一个LR的作用是“特征变换”，第二个LR的作用是“分类”。第一层的输出值$(x_1^{‘}, x_2^{‘})$线性可分。（四个点分别为(0.73,0.05),(0.27,0.27),(0.27,0.27),(0.05,0.73)）. Deep Learning级联多层LR模型，得到的就是深度神经网络。前面几层的作用是“特征变换”，最后一层的作用是分类。其中一个LR模型，就是一个“神经元”。 总结 对比了一下Logistic回归与线性回归的相同点与不同点。不同点是模型函数不同，输出值的范围不同；相同点是参数的梯度的形式一样。 交叉熵衡量的是两个分布的差异，如果两个分布相同，那么他们的交叉熵的值最小，为0。如果Logistic回归用平方误差作为损失函数的话，损失函数随着参数的变化而变化比较缓慢，不好优化，很难得到理想的结果。 Logistic回归与线性回归两种模型都是sigmoid函数，不同的是参数优化方法不同，对数据分布的假设不同。 分类问题中，一般判别式模型比产生式模型的效果更好。产生式模型能生成样本。 Logistic 回归无法解决线性无可分的问题。可通过“特征变换”和“级联LR”解决线性不可分的问题。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>Logistic Regression</tag>
        <tag>Neural Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[top命令]]></title>
    <url>%2F2018%2F04%2F04%2Ftop%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[信息说明统计信息说明：敲top命令，进入如下视图： 第1行：Top 任务队列信息(系统运行状态及平均负载)，与uptime命令结果相同。 第1段：系统当前时间，例如：10:01:23 第2段：系统运行时间，未重启的时间，时间越长系统越稳定。 格式：up xx days, HH:MM例如：126 days, 14:29, 表示连续运行了126天14小时29分钟 第3段：当前登录用户数，例如：2 user，表示当前只有2个用户登录 第4段：load average 系统负载，即任务队列的平均长度，3个数值分别统计最近1，5，15分钟的系统平均负载。 系统平均负载：单核CPU情况下，0.00 表示没有任何负荷，1.00表示刚好满负荷，超过1侧表示超负荷，理想值是0.7；如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了多核CPU负载：CPU核数 * 理想值0.7 = 理想负荷，例如：4核CPU负载不超过2.8则表示没有出现高负载。 第2行：Tasks 进程相关信息 第1段：进程总数，例如：Tasks: 183 total, 表示总共运行231个进程 第2段：正在运行的进程数，例如：1 running, 第3段：睡眠的进程数，例如：182 sleeping, 第4段：停止的进程数，例如：0 stopped, 第5段：僵尸进程数，例如：0 zombie 第3行：Cpus CPU相关信息，如果是多核CPU，按数字1可显示各核CPU信息，此时1行将转为Cpu核数行，数字1可以来回切换。 第1段：us 用户空间占用CPU百分比，例如：Cpu(s): 6.7%us, 第2段：sy 内核空间占用CPU百分比，例如：0.4%sy, 第3段：ni 用户进程空间内改变过优先级的进程占用CPU百分比，例如：0.0%ni, 第4段：id 空闲CPU百分比，例如：92.9%id, 第5段：wa 等待输入输出的CPU时间百分比，例如：0.0%wa, 第6段：hi CPU服务于硬件中断所耗费的时间总额，例如：0.0%hi, 第7段：si CPU服务软中断所耗费的时间总额，例如：0.0%si, 第8段：st Steal time 虚拟机被hypervisor偷去的CPU时间（如果当前处于一个hypervisor下的vm，实际上hypervisor也是要消耗一部分CPU处理时间的） 第4行：Mem 内存相关信息（Mem: 8306544k total, 7775876k used, 530668k free, 79236k buffers） 第1段：物理内存总量，例如：Mem: 8306544k total, 第2段：使用的物理内存总量，例如：7775876k used, 第3段：空闲内存总量，例如：Mem: 530668k free, 第4段：用作内核缓存的内存量，例如：79236k buffers 第5行：Swap 交换分区相关信息（Swap: 2031608k total, 2556k used, 2029052k free, 4231276k cached） 第1段：交换区总量，例如：Swap: 2031608k total, 第2段：使用的交换区总量，例如：2556k used, 第3段：空闲交换区总量，例如：2029052k free, 第4段：缓冲的交换区总量，4231276k cached windows的内存概念与Linux的不一样，如果按windows的方式此台服务器“危矣”：8G的内存总量只剩下530M的可用内存。Linux的内存管理有其特殊性，复杂点需要一本书来说明，这里只是简单说点和我们传统概念（windows）的不同。 第四行中使用中的内存总量（used）指的是现在系统内核控制的内存数，空闲内存总量（free）是内核还未纳入其管控范围的数量。纳入内核管理的内存不见得都在使用中，还包括过去使用过的现在可以被重复利用的内存，内核并不把这些可被重新使用的内存交还到free中去，因此在linux上free内存会越来越少，但不用为此担心。 如果出于习惯去计算可用内存数，这里有个近似的计算公式：第四行的free + 第四行的buffers + 第五行的cached，按这个公式此台服务器的可用内存：530668+79236+4231276 = 4.7GB。 对于内存监控，在top里我们要时刻监控第五行swap交换分区的used，如果这个数值在不断的变化，说明内核在不断进行内存和swap的数据交换，这是真正的内存不够用了。 进程信息：在top命令中按f按可以查看显示的列信息，按对应字母来开启/关闭列，大写字母表示开启，小写字母表示关闭。带*号的是默认列。 列名 含义 A PID = (Process Id) 进程Id； E USER = (User Name) 进程所有者的用户名； H PR = (Priority) 优先级 I NI = (Nice value) nice值。负值表示高优先级，正值表示低优先级 O VIRT = (Virtual Image (kb)) 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES Q RES = (Resident size (kb)) 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA T SHR = (Shared Mem size (kb)) 共享内存大小，单位kb W S = (Process Status) 进程状态。D=不可中断的睡眠状态,R=运行,S=睡眠,T=跟踪/停止,Z=僵尸进程 K %CPU = (CPU usage) 上次更新到现在的CPU时间占用百分比 N %MEM = (Memory usage (RES)) 进程使用的物理内存百分比 M TIME+ = (CPU Time, hundredths) 进程使用的CPU时间总计，单位1/100秒 b PPID = (Parent Process Pid) 父进程Id c RUSER = (Real user name) d UID = (User Id) 进程所有者的用户id f GROUP = (Group Name) 进程所有者的组名 g TTY = (Controlling Tty) 启动进程的终端名。不是从终端启动的进程则显示为 ? j P = (Last used cpu (SMP)) 最后使用的CPU，仅在多CPU环境下有意义 p SWAP = (Swapped size (kb)) 进程使用的虚拟内存中，被换出的大小，单位kb l TIME = (CPU Time) 进程使用的CPU时间总计，单位秒 r CODE = (Code size (kb)) 可执行代码占用的物理内存大小，单位kb s DATA = (Data+Stack size (kb)) 可执行代码以外的部分(数据段+栈)占用的物理内存大小，单位kb u nFLT = (Page Fault count) 页面错误次数 v nDRT = (Dirty Pages count) 最后一次写入到现在，被修改过的页面数 y WCHAN = (Sleeping in Function) 若该进程在睡眠，则显示睡眠中的系统函数名 z Flags = (Task Flags &lt;sched.h&gt;) 任务标志，参考 sched.h X COMMAND = (Command name/line) 命令名/命令行 交互操作技巧多U多核CPU监控在top基本视图中，按键盘数字“1”，可监控每个逻辑CPU的状况：观察下图【top视图 01】，服务器有16个逻辑CPU，实际上是4个物理CPU。 进程字段排序默认进入top时，各进程是按照CPU的占用量来排序的，在【top视图 01】中进程ID为14210的java进程排在第一（cpu占用100%），进程ID为14183的java进程排在第二（cpu占用12%）。 可通过键盘指令来改变排序字段，比如想监控哪个进程占用MEM最多，我一般的使用方法如下： 敲击键盘“b”（打开/关闭加亮效果），top的视图变化如下：我们发现进程id为10704的“top”进程被加亮了，top进程就是视图第二行显示的唯一的运行态（runing）的那个进程，可以通过敲击“y”键关闭或打开运行态进程的加亮效果。 敲击键盘“x”（打开/关闭排序列的加亮效果），top的视图变化如下：可以看到，top默认的排序列是“%CPU”。 通过”shift + &gt;”或”shift + &lt;”可以向右或左改变排序列，下图是按一次”shift + &gt;”的效果图：视图现在已经按照%MEM来排序了。 改变进程显示字段 敲击“f”键，top进入另一个视图，在这里可以编排基本视图中的显示字段：这里列出了所有可在top基本视图中显示的进程字段，有””并且标注为大写字母的字段是可显示的，没有””并且是小写字母的字段是不显示的。如果要在基本视图中显示“CODE”和“DATA”两个字段，可以通过敲击“r”和“s”键： “回车”返回基本视图，可以看到多了“CODE”和“DATA”两个字段： top命令的补充top命令是Linux上进行系统监控的首选命令，但有时候却达不到我们的要求，比如当前这台服务器，top监控有很大的局限性。这台服务器运行着websphere集群，有两个节点服务，就是【top视图 01】中的老大、老二两个java进程，top命令的监控最小单位是进程，所以看不到我关心的java线程数和客户连接数，而这两个指标是java的web服务非常重要的指标，通常我用ps和netstate两个命令来补充top的不足。 12345# 监控java线程数：$ ps -eLf | grep java | wc -l# 监控网络客户连接数：$ netstat -n | grep tcp | grep 侦听端口 | wc -l 上面两个命令，可改动grep的参数，来达到更细致的监控要求。 在Linux系统“一切都是文件”的思想贯彻指导下，所有进程的运行状态都可以用文件来获取。 系统根目录/proc中，每一个数字子目录的名字都是运行中的进程的PID，进入任一个进程目录，可通过其中文件或目录来观察进程的各项运行指标，例如task目录就是用来描述进程中线程的，因此也可以通过下面的方法获取某进程中运行中的线程数量（PID指的是进程ID）：1$ ls /proc/PID/task | wc -l 在linux中还有一个命令pmap，来输出进程的内存状况，可以用来分析线程堆栈：1$ pmap PID 参考： https://www.linuxidc.com/Linux/2016-08/133871.htm https://blog.csdn.net/dxl342/article/details/53507673]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>top</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李宏毅机器学习2017学习笔记-4.Classification]]></title>
    <url>%2F2018%2F04%2F04%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02017%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4-Classification%2F</url>
    <content type="text"><![CDATA[引言如何做分类呢？一个理想的方法是： 函数（模型）：找到一个函数$f(x)$，其中内建一个函数$g(x)$，如果$g(x)&gt;0$则为类1，否则为类0； 损失函数：$L(f) = \sum_n \delta(f(x^n) \neq \hat y^n)$，分类正确的个数。 优化方法：感知机、SVM等 产生式模型举个例子有2个盒子，盒子1中有4个蓝1球、1个绿球，盒子2中有2个蓝球、3个绿球。选择盒子1的概率为2/3，选择盒子2的概率为1/3。已知抽到了一个蓝球，问这个蓝球是从哪个盒子里抽出来的。 贝叶斯公式：$$P(A|B) = \frac {P(B|A) * P(A)} {P(B)}$$ 我们可以利用贝叶斯公式，分别算出$P(B_1|Blue)$ 和 $P(B_2|Blue)$的大小，哪个大，蓝球就来自哪个盒子。 $ P(Blue|B_1)P(B_1) = 4/5 * 2/3 = 8 / 15 $ $ P(Blue|B_2)P(B_2) = 2/5 * 1/3 = 2 / 15 $ $ P(B_1|Blue) = 8/15 / (8/15+2/15) = 4 / 5$ 更加普适一点：我们设一个样本来自于类i的概率为$P(C_i)$，在类i中，抽中某个样本的概率为$P(x|C_i)$.给定$x$，它来自哪一类的概率为：$$ P(C_i|x) = \frac {P(x|C_i)P(C_i)} {\sum_{i=1}^n P(x|C_i) P(C_i)}$$那么要求$x$来自哪一类，就看哪个$P(C_i|x)$最大。 产生式模型：我们从训练数据中能得到上图4个红框框中的表达式，用这4个东西就能求出$x$出现的几率，就能产生式$x$:$$ P(x) = P(x|C_1)P(C_1) + P(x|C_2)P(C_2) $$ 求解一下宝可梦属于水系还是一般系训练数据中有79只『水系』的和61只『一般系』宝可梦，其中$ P(C_1) $ 和 $ P(C_2)$很好求出：$ P(C_1) = 79 / (79 + 61) = 0.56 $$ P(C_2) = 61 / (79 + 61) = 0.44 $ 但是，如何求$ P(x|C_1) $ 呢？ 我们选择『防御力』和『特殊防御力』两个属性来描述宝可梦，那么特征的维度为2维。 把79只『水系』的宝可梦画在坐标系中，如下图所示：如上图红色圆圈所示，假设宝可梦的特征值服从高斯分布。求出均值和方差，就能根据$f_{\mu,\Sigma}(x)$ 求出 $P(x|C_i)$。 使用最大似然估计，求$\mu$和$\Sigma$呢？从一个均值为$\mu$、方差为$\Sigma$的高斯分布中取79个样本的概率为：$$ L(\mu, \Sigma) = f_{\mu,\Sigma}(x^1) \cdot f_{\mu,\Sigma}(x^2) \cdot f_{\mu,\Sigma}(x^3)…f_{\mu,\Sigma}(x^{79})$$ 使似然函数取得最大值的解，就是最优解：$ \mu^\ast, \Sigma^\ast = arg \max_{\mu, \Sigma} L(\mu, \Sigma) $最优解就是训练数据的均值和方差：$\mu^\ast = \frac 1 {79} \sum_{n=1}^{79} x^n $$\Sigma^\ast = \frac 1 {79} \sum_{n=1}^{79} (x^n - \mu\ast)(x^n - \mu^\ast)^T $ 『水系』和『一般系』两类样本的分布如下： 根据$P(x|C_1) = f_{\mu^1, \Sigma^1}(x)$，求出$P(C_1|x)$: 如果$P(C_1|x) &gt; 0.5 $，那么$x$属于第一类『水系』。这样，在测试集上的正确率为47%。如下图，蓝色区域的点会被分为『水系』，红色区域的点会被分为『一般系』。 改进方法： 增加特征，将全部7种属性都作为宝可梦的特征，正确率为54%。结果还是不理想。 假设『水系』和『一般系』两类样本的 $\Sigma$ 一样，$\Sigma = (79/140)\Sigma^1 + (61/140)\Sigma^2 $，正确率提升到73%。 比较： 两类不共用同一个$\Sigma$的话，分界面是曲线的。 两类共用同一个$\Sigma$的话，分界面变成了线性的。（这里可引出Logistics Regression） 解决分类问题需要3步使用概率产生式模型进行分类，共3步： 建模； 用似然函数定义参数的好坏； 用极大似然估计来计算最优参数。 朴素贝叶斯假设产生x的每一维特征都是不相关的，那么$P(x|C_1) = P(x_1|C_1) \cdot P(x_2|C_1) … P(x_k|C_1)$. (实际上，特征的各个维度之间是相关的。比如『防御力』比较大的，一般『攻击力』比较小)。 不是所有的情况都使用高斯分布，如果某个特征是二值的，用伯努利分布比较好！ 后验概率令$ z = ln \frac {P(x|C_1) P(C_1)} {P(x|C_2) P(C_2)}$，类$C_1$的后验概率可以写成$$ P(C_1|x) = \frac {P(C_1|x)P(c_1)} {P(C_1|x)P(c_1) + P(C_2|x)P(c_2)} = \frac 1 {1+exp(-z)} = \delta(z)$$“sigmoid”的意思是“S形”。这种函数有时被称为“挤压函数”，因为它把整个实数轴映射到了一个有限的区间中。 它满足围绕点(0, 0.5)中心对称，$\delta(-z) = 1- \delta(z)$ $\delta(z)$是logistic sigmoid函数，定义为$$\delta(z) = \frac 1 {1+exp(-z)}$$ logistic sigmoid的反函数为logit函数：$$ z = ln (\frac \delta {1-\delta})$$它表示两类概率比值的对数，也被称为log odds。 odds，中文翻译为“胜算”、“发生比”。$odds = \frac {P(x|C_1) P(C_1)} {P(x|C_2) P(C_2)} = \frac p {1-p}$ 用$exp(-z)$ 表示 $\frac {P(x|C_1) P(C_1)} {P(x|C_2) P(C_2)}$，因为 $p/(1-p)$的取值范围：$[0, + \infty)$。($p$可以取零) $ln (p/(1-p))$的取值范围：$(-\infty, + \infty)$。($p$不可以取零) $z=wx+b$的取值范围：$(-\infty, + \infty)$ $exp(-z)$的取值范围都是$(0, + \infty)$. 求解z过程如下： $z$与$x$线性相关得到$z$与$x$是线性相关的：$z=w^T \cdot x + b$进而得到：$P(C_1|x) = \delta(w \cdot x + b)$，其中$\delta$是sigmoid函数。也就是说，基于概率高斯模型（多元高斯混合模型）的假设可以推导出后验概率等于$x$的线性表达的sigmoid形式。 总结： 通过一个从两个盒子中取篮球和绿球的例子，引出产生式模型。 产生式模型中，我们对类条件概率密度$P(x|C_i)$和类先验概率分布$P(C_i)$建模，然后使用这两个概率密度通过贝叶斯定理计算后验概率密度$P(C_i|x)$。 朴素贝叶斯分类器的假设是，$x$的特征的各个维度是不相关的。 在产生式模型的基础上，假设数据服从高斯分布。后验概率能表示为一个sigmoid函数：$P(C_i|x)=\delta(z)$，且这个$z$又与$x$线性相关。所以，$P(C_i|x) = \delta(w \cdot x + b)$。(Logistic回归也是一个sigmoid函数的形式，但是它并没有假设数据服从高斯分布)]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>产生式模型</tag>
        <tag>贝叶斯</tag>
        <tag>概率高斯模型</tag>
        <tag>高斯混合模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vi常用命令]]></title>
    <url>%2F2018%2F04%2F02%2Fvi%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[vi有3个模式：插入模式、命令模式、低行模式。 插入模式：在此模式下可以输入字符，按ESC将回到命令模式。 命令模式：可以移动光标、删除字符等。 低行模式：可以保存文件、退出vi、设置vi、查找等功能(低行模式也可以看作是命令模式里的)。 打开文件、保存、关闭文件(vi命令模式下使用)vi filename // 打开filename文件:w // 保存文件:w filename // 保存至filename文件:q // 退出编辑器，如果文件已修改请使用下面的命令:q! // 退出编辑器，且不保存:wq // 退出编辑器，且保存文件 插入文本或行(vi命令模式下使用，执行下面命令后将进入插入模式，按ESC键可退出插入模式) a // 在当前光标位置的 右边 添加文本i // 在当前光标位置的 左边 添加文本A // 在当前行的 行尾 添加文本I // 在当前行的 行尾 添加文本(非空字符的行首)O // 在当前行的 上面新建一行o // 在当前行的 下面新建一行R // 替换(覆盖)当前光标位置及后面的若干文本J // 合并光标所在行及下一行为一行 移动光标(vi命令模式下使用)1、使用上下左右方向键 2、命令模式下： h（向左）、j（向下）、k（向上）、l（向右） 空格键（向右）、Backspace（向左）、Enter（移动到下一行首）、-（移动到上一行首） 删除、恢复字符或行(vi命令模式下使用)x // 删除当前字符nx // 删除从光标开始的n个字符dd // 删除当前行ndd // 向下删除当前行在内的n行u // 【撤销】 撤销上一步操作U // 撤销对当前行的所有操作 搜索(vi命令模式下使用)/vpser // 向光标下搜索vpser字符串?vpser // 向光标上搜索vpser字符串n // 向下搜索前一个搜素动作N // 向上搜索前一个搜索动作 跳至指定行(vi命令模式下使用)n+ // 向下跳n行n- // 向上跳n行nG // 跳到行号为n的行G // 跳至文件的底部 设置行号(vi命令模式下使用):set nu // 显示行号:set nonu // 取消显示行号 复制、粘贴(vi命令模式下使用)yy // 【复制】 将当前行复制到缓存区，也可以用“ayy” 复制，”a”为缓冲区，”a”也可以替换为a到z的任意字母，可以完成多个复制任务。nyy // 将当前行向下n行复制到缓冲区，也可以用 ”anyy”复制，”a”为缓冲区，a也可以替换为a到z的任意字母，可以完成多个复制任务。yw // 复制从光标开始到词尾的字符。nyw // 复制从光标开始的n个单词。y^ // 复制从光标到行首的内容。y$ // 复制从光标到行尾的内容。p // 【粘贴】 粘贴剪切板里的内容在光标后，如果使用了前面的自定义缓冲区，建议使用”ap”进行粘贴。P // 粘贴剪切板里的内容在光标前，如果使用了前面的自定义缓冲区，建议使用”aP”进行粘贴。 替换(vi命令模式下使用):s/old/new // 用new替换行中首次出现的old:s/old/new/g // 用new替换行中所有的old:n,m s/old/new/g // 用new替换从n到m行里所有的old:%s/old/new/g // 用new替换当前文件里所有的old 编辑其他文件:e otherfilename // 编辑文件名为otherfilename的文件 修改文件格式:set fileformat=unix // 将文件修改为unix格式，如win下面的文本文件在linux下会出现^M。 Tips： 当不知道自己处在什么模式时可以按2次Esc键即可回到命令模式。 附Word版，方便下载打印出来。 点此下载：Vi常用命令.docx 本文转载自：VPS侦探 本文链接地址：https://www.vpser.net/manage/vi.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>vi</tag>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李宏毅机器学习2017学习笔记:3-1.Gradient Descent]]></title>
    <url>%2F2018%2F03%2F28%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02017%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3-1-Gradient-Descent%2F</url>
    <content type="text"><![CDATA[对于以下优化问题，我们需要找到最优的参数$\theta ^ \ast$，使得损失函数最小：$$ \theta ^ \ast = arg \min_{\theta} L(\theta) $$ 使用梯度下降的方法进行优化。 梯度是损失函数的等高线的法线方向，每次更新参数时，往梯度方向的反方向走。$$ \theta^{t+1} = \theta^t - \eta * \nabla L(\theta^t)$$ 梯度下降的几个小技巧调整learning rate将Loss的变化趋势可视化出来。 如果learning rate比较小，那么Loss降低得很慢； 如果learning rate比较大，那么Loss会出现在极小值附近震荡的现象； 如果learning rate太大，那么Loss会剧烈变化，飞出去了； 自动调整learning rate 流行且简单的方法是，逐渐减小learning rate 刚开始的时候，离最优解比较远，采用比较大的learning rate 随着迭代次数增加，离最优解越来越紧，我们慢慢减小learning rate 例如：$ \eta^t = \eta / {\sqrt{t+1}}$ 每个不同的参数，采用不同的learning rate Adagrad每个参数的learning rate都除上过去所有微分值的均方根（root mean square of its previous derivatives） 普通的Gradient Descent：（对于每个参数）$$ w^{t+1} = w^t - \eta^t g^t $$Adagrad:（对于每个参数）$$ w^{t+1} = w^t - \frac {\eta^t} {\delta^t} g^t $$其中: $ \eta^t = \frac \eta {\sqrt{t+1}} $ $ g^t = \frac {\partial L(\theta ^ t)} {\partial w} $ ${\delta^t}$是过去所有微分值的均方根。 消掉$ {\sqrt{t+1}} $之后：$$ w^{t+1} = w^t - \frac \eta {\sqrt{\sum_{i=0}^t (g^i)^2}} g^t $$再比较一下普通的梯度下降与Adagrad：普通的Gradient Descent（对于每个参数）：$$ w^{t+1} = w^t - \eta^t g^t = w^t - \eta \frac {g^t} {\sqrt{t+1}}$$learning rate随着t的增大而减小，$g$越大时，参数变化得越大。 Adagrad（对于每个参数）：$$w^{t+1} = w^t - \frac \eta {\sqrt{\sum_{i=0}^t (g^i)^2}} g^t= w^t - \eta \frac {g^t} {\sqrt{\sum_{i=0}^t (g^i)^2}}$$learning rate与过去所有的微分有关，$g$越大时，如果过去所有的$g$也很大，那么参数变化得不大；如果过去所有的$g$很小，那么参数变化得就很大。 Adagrad每次更新参数时，不仅考虑了当前的梯度$g^t$，而且还考虑了过去所有的$g$，即$\sqrt{\sum_{i=0}^t (g^i)^2}$。 这个$\sqrt{\sum_{i=0}^t (g^i)^2}$ 能造成反差的效果。如下图： TODO：添加解释用过去所有微分值的均方根来替代二次微分。 随机梯度下降(Stochastic) 普通的梯度下降（所有的样本）： 损失函数：$ L = \sum_{i=1}^n \left( \hat y^i - w*x \right)^2$ 参数更新：$ \theta^{t+1} = \theta^t - \eta \nabla L(\theta^t) $ 看完了所有的样本，才更新参数。 随机梯度下降（选择一个样本）： 损失函数：$ L^i = \left( \hat y^i - w*x \right)^2$ 参数更新：$ \theta^{t+1} = \theta^t - \eta \nabla L^i(\theta^t) $ 没看完一个样本，都更新一次参数。虽然每次用一个样本更新参数的效果不如普通的梯度下降，但是所有样本更新完之后，效果就比普通的梯度下降方法更好了。 特征归一化(Feature Scaling)把特征的各个维度进行归一化，使各个特征的取值范围一致。具体做法： 取第i维特征$x_i$； 求均值$m_i$、标准差$\delta_i$； 对第r样本，更新数值$ x_i^r = \frac {x_i^r - m_i} \delta_i $ 梯度下降的理论基础Formal Derivation（形式误差）给定一个起始点 $\theta^0$，我可以在其一定范围内找到一个最小的点，并更新为 $\theta^1$。如此迭代下去，知道找到最优解。 Taylor Series（泰勒级数）定理：如果 $h(x)$ 在 $x_0$ 附近是无限可微的，那么 $h(x)$ 可以展开成：$$h(x) = \sum_{k=0}^\infty \frac {h^k({x_0})} {k!} (x-{x_0})^k= h(x_0) + h’(x_0)(x-x_0) + \frac {h’’(x_0)} {2!} (x-x_0)^2 + …$$当 $x$ 很接近 $x_0$ 时，$ h(x) \approx h(x_0) + h’(x_0)(x-x_0) $ 多个参数的泰勒展开：$$h(x,y) = h(x_0,y_0) + \frac {\partial h(x_0,y_0)} {\partial x} (x - x_0) + \frac {\partial h(x_0,y_0)} {\partial y} (y - y_0) + …$$当 $(x,y)$ 很接近 $(x_0,y_0)$ 时，$ h(x,y) \approx h(x_0,y_0) + \frac {\partial h(x_0,y_0)} {\partial x} (x - x_0) + \frac {\partial h(x_0,y_0)} {\partial y} (y - y_0) $ 梯度下降法的由来如图，假设红色圆圈(半径d)很小很小，那么损失函数在红色圆圈内满足：$L(\theta) \approx s + u(\theta_1 - a) + v(\theta_2 - b)$ 问题转换为：在红色圆圈内，求损失函数的最小值。 如下图，损失函数的各个部分中，$s, u, v$是固定的，我们要求的是 $\Delta \theta_1$ 和 $\Delta \theta_2$ .从图中，很容易看出，当 $(\Delta \theta_1$ , $\Delta \theta_2)$ 与 $(u, v)$的方向正好相反时，$L(\theta)$最小。 也就是说 $\theta^{t-1} - \theta^t$ 的结果是 $(u, v)$ 的反方向。如下图，转换一下之后，就得到了梯度下降的公式。 Note：如果把泰勒级数的二次微分考虑进来的话，就是牛顿法了。 梯度下降法的局限性容易卡在局部最小值点和鞍点，他们的共同点都是微分值为0的点。计算过程中，当微分值特别小的时候，我们可能会认为这是到了局部最小值，就提前结束循环。实际上，我们可能只是遇到了鞍点。 总结： 梯度下降法的三个tip： 使用Adagrad动态调整learning rate 随机梯度下降 特征归一化 通过泰勒级数推出梯度下降的方法。 梯度下降法的局限性：容易卡在局部最小值点和鞍点。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>李宏毅</tag>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李宏毅机器学习2017学习笔记:2.Bias and Variance]]></title>
    <url>%2F2018%2F03%2F26%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02017%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2-Bias-and-Variance%2F</url>
    <content type="text"><![CDATA[Error来自哪里？ 来自于bias 来自于variance Estimator(估计量)在估计宝可梦的CP值的例子中，正确的函数为 $\hat f$ ，这个我们无法知道。 我们只能从训练数据中学到一个最好的函数 $f^\ast$ .所以，$f^\ast$ 是 $\hat f$的一个estimator。 『估计量』 的 Bia 和 Variance假设有一个变量$x$，满足：$x$的均值为$\mu$，均方差为$\sigma^2$。 如何估计均值$\mu$呢？取N个点：${x^1, x^2,…,x^N}$N个点取均值，结果不会等于$\mu$，当N无限大时，均值会无限接近$\mu$：$$ m = \frac 1 N \sum_n x^n \neq \mu $$虽然每个$m$与$\mu$都不相等，但是$m$的期望等于均值$\mu$。所以用$m$来估计$\mu$，是无偏的。$$ E[m] = E \left[\frac 1 N \sum_n x^n \right] = \frac 1 N \sum_n E[x^n] = \mu $$ 就像打靶的时候，瞄的点是$\mu$，但是由于风、肌肉抖动等的影响，实际打中的地方会散布在瞄的$\mu$的周围。 散布得多散，取决于$m$的方差：$$ Var(m) = \frac {\sigma^2} N $$ $m$的方差取决于样本的数量： 当N比较小时，散布比较开； 当N比较大时，散布比较紧。 如何估计均方差$\sigma^2$呢？$s^2$表示：$$ s^2 = \frac 1 N \sum_n (x^n - m)^2 $$用$s^2$来估计均方差，是有偏的：$$ E[s^2] = \frac {N-1} N \sigma^2 \neq \sigma^2 $$当N很大时，$s^2$的期望会很接近$\sigma^2$. 小结我们的目标是估测靶的中心$\hat f$，对N组数据分别估测N个$f ^\ast$. 每个$f^\ast$与$\hat f$之间存在误差，这个误差来自于%E[f\ast]%与$\hat f$的bias; 另外一个误差来自于$f^\ast$与$\overline f$的variance. Note：如何得到多个$f^\ast$呢？在不同的数据集上估计$f$。 举例：分别用以下3种模型，学习100次，得到多个$f^\ast$，画图如下：从图中可以看到： 从上到下，模型的复杂程度越来越高； 简单的模型散步得很紧密，复杂的模型散布得比较开； 简单的模型受到数据(x)的影响较小（最极端的例子$f(x)=c$，最简单的模型，完全不受数据影响）。 对5000个 $f^\ast$ 求平均，画出来的蓝色线如下图：比较bias： 左边的模型比较简单，求平均之后离 $\hat f$ 较远，bias较大； 右边的模型比较复杂，求平均之后离 $\hat f$ 较近，bias较小。 比较variance： 左边的模型输出值均在 $\hat f$ 附近，variance较小； 右边的模型输出值散布较开，variance较大。 如何处理bias偏大的情况？分析方法： 如果模型不能很好地拟合训练数据，说明bias很大。【欠拟合】 如果模型能拟合训练数据，但是在测试数据上误差较大，那么很可能variance很大。【过拟合】 如果是bais很大，那么需要使模型更加复杂： 加更多的特征 用更加复杂的模型 如果是variance很大，那么： 收集更大的模型。如下图，100个样本训练的模型比10个样本训练的模型散布更加紧凑。 正则化。如下图，从左往右，正则项系数逐渐增大。 如何选择模型？【交叉验证】作业提供了一个训练集和公开的测试集，提交作业的时候，需要在私有的测试集上对提交的结果进行测试。 如果在训练集上训练了3个模型，在测试集上，『Model 3』 的 Error=0.5 最小，于是将『Model 3』的结果提交上去，结果Error&gt;0.5。怎么办？ N折交叉验证使用『N折交叉验证』来选择最优的模型，过程如下。 将训练集分成N份； 对于每个模型而言，依次将第i份拿出来作验证，在其他的N-1数据上训练，得到N个训练误差。求平均，得到平均训练误差； 对于多个训练模型而言，取平均训练误差最小的模型。 总结 如果在训练集上误差较大，那么bias较大，说明是欠拟合。考虑增加特征，或者换更复杂的模型； 如果在训练集上误差较小，在测试集上误差较大，过拟合。考虑收集更多的数据，或者采用正则化。 多个候选模型之间如何选择？采用N折交叉验证。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>李宏毅</tag>
        <tag>Bias</tag>
        <tag>Variance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李宏毅机器学习2017学习笔记:1.Regression Demo]]></title>
    <url>%2F2018%2F03%2F26%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02017%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-Regression-Demo%2F</url>
    <content type="text"><![CDATA[视频简介视频中先固定learning rate，迭代10w次： 首先，设置了一个比较小的learning rate，lr=1e-6，没有得到最优解，就停止了； 然后，放大lrarning rate，lr=1e-5，结果出现震荡的情况，无法得到最优解。 最后，采用AdaGrad方法调整learning rate，得到了最优解。 固定learning rate123456789lr = 0.000001for i in range(iteration): # calculate gradient b_grad = ... w_grad = ... # update weight b = b - lr * b_grad w = w - lr * w_ grad 动态调整learning rate视频中使用的方法是：AdaGrad 123456789101112131415lr = 1lr_b = 0lr_w = 0for i in range(iteration): # calculate gradient b_grad = ... w_grad = ... # update lr_b, lr_w with AdaGrad lr_b = lr_b + b_grad ** 2 lr_w = lr_w + w_grad ** 2 # update weight b = b - lr/np.sqrt(lr_b) * b_grad w = w - lr/np.sqrt(lr_w) * w_ grad]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>李宏毅</tag>
        <tag>Regression</tag>
        <tag>调参</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李宏毅机器学习2017学习笔记:1.Regression]]></title>
    <url>%2F2018%2F03%2F24%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02017%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-Regression%2F</url>
    <content type="text"><![CDATA[构建最基本的回归模型1.问题描述：如何根据宝可梦的CP值预测进化后的CP值？ 2.模型假设：$$y = b + w * X_{cp}$$ 为方便表示和计算，用$w_0$替代$b$，只需要在$X_{cp}$前面添加一维数值1，模型变为：$$y = w * X_{cp}$$ 3.损失函数：$$L(f) = L(w) = \frac12 \sum_{i=1}^{10} \left(\hat {y}^i - w · x_{cp}^i\right) ^2$$ 4.优化方法（梯度下降法）：目标函数：$$ f^\ast = arg \min_{f}^{} L(f) $$代入模型方程：$$ w^\ast= arg \min_{w} \frac12 \sum_{i=1}^{10}\left(\hat {y}^i - w · x_{cp}^i\right)^2$$ 求梯度:$$ \frac {\partial L} {\partial w} = \sum_{i=1}^{10} \left( \hat {y}^i - w · x_{cp}^i\right) (-x_{cp}^i)$$参数更新:$$w_{t+1} = w_t - \alpha · \frac {\partial L} {\partial w}$$其中，$\alpha$为步长。 5.结果：在测试集上的平均误差为35. 对回归模型进行优化选择更加复杂的模型 增加一维特征$(X_{cp})^2$$$y = b + w_1·X_{cp} + w_2 · (X_{cp})^2$$在训练集上和测试集上的平均误差分别15.4和18.4 再增加一维特征$(X_{cp})^3$在训练集上和测试集上的平均误差分别15.3和18.1 再增加一维特征$(X_{cp})^4$在训练集上和测试集上的平均误差分别14.9和28.8 再增加一维特征$(X_{cp})^5$在训练集上和测试集上的平均误差分别12.8和232.1 上述实验，可以看出： 更复杂的模型，可以得到更小的训练误差； 更复杂的模型，可能导致过拟合，在测试集上表现不好。 解决办法：收集更多的数据； 收集更多的特征后发现，下图中，同一个x，对应两个不同的y，所以推测存在隐藏的因子（宝可梦的种类）。 加上种类属性，重新建模：也就是说，每一类宝可梦分别对应一个回归模型.在训练集和测试集上的误差分别为3.8和14.3 继续猜测，可能还有隐藏因子，例如『高度』、『体重』等。验证一下，过拟合了！ 正则化（Regularization）：以上猜测隐藏因子的方法不一定猜得到，正则化一般来说是有用的。 在损失函数后面添加一项对参数的约束：$$L(f) = L(w) = \frac12 \sum_{i=1}^{10} \left(\hat {y}^i - w · x_{cp}^i\right) ^2 + \lambda \sum(w_i)^2$$ $w_i$很小，意味着函数比较平滑。 什么是平滑的函数？当输入变化很大时，输出的变化不大，则这个函数是平滑的。如下图，当左右两边都加上$\delta x_i$时，如果$w_i$比较小，那么$y$的变化也会比较小。 需要给bias加regularization?不需要，因为bias只影响函数上下移动，并不影响函数的平滑度。 小结： 宝可梦进化后的CP值与进化前的CP值和宝可梦的类型有关，也可能存在其他的隐藏因子； 梯度下降方法； 我们最终在测试集上得到的误差为11.1，那么这个模型应用于新的数据集、更大的数据集上，误差会变大还是变小？ 下一节：误差来自哪里？]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>李宏毅</tag>
        <tag>Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[异地同步博客工程源码]]></title>
    <url>%2F2018%2F03%2F19%2F%E5%BC%82%E5%9C%B0%E5%90%8C%E6%AD%A5%E5%8D%9A%E5%AE%A2%E5%B7%A5%E7%A8%8B%E6%BA%90%E7%A0%81%2F</url>
    <content type="text"><![CDATA[hexo主目录结构12345678|-- _config.yml |-- package.json |-- scaffolds|-- source |-- _posts |-- _drafts |-- themes|-- .gitignore _config.yml，全局配置文件。，网站的很多信息都在这里配置，诸如网站名称，副标题，描述，作者，语言，主题，部署等等参数。 package.json，框架的参数和依赖插件 scaffolds，是“脚手架、骨架”的意思，当你新建一篇文章（hexo new ‘title’）的时候，hexo是根据这个目录下的文件进行构建的。 source目录，博客文件存储的目录。其中，_posts是发布出来的博客的目录，_drafts是草稿存储的目录。 themes，存储主题的目录。 安装其他插件之后，会有其他的目录或文件：1|-- .deploy_git .deploy_git，hexo-deploy-git插件自动生成的目录 db.json，NexT主题生成的文件 需要同步的文件和目录需要同步的文件和目录包括： _config_yml package.json source themes scaffolds 剩下不需要同步的文件，添加到.gitignore文件中：1234567.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/ 使用github进行异地同步本机上传源码我们在xxx.github.io这个repo下建一个source分支，来保存博客源码。 新建source分支 1$ git checkout -b source 上传博客文件 123$ git add --all$ git commit -m "init blog data"$ git push origin source 上传第三方主题文件因为第三方主题文件是从github上clone下来的一个完整的repo，那么在MyBlog目录下是无法直接将主题文件上传到自建的reop上的。如果执行git add ./themes/next/*会报fatal: Pathspec ‘themes/next/_config.yml’ is in submodule的错误。 如果以后不想更新主题，最简单的方式是先清空next主题目录下的.git文件夹，然后清空缓存，最后提交主题文件。12345$ rm -rf ./themes/next/.git$ git rm -rf --cached ./themes/next/$ git commit -m &quot;add theme NexT&quot;$ git push origin master 如果想以后更新主题，那么在添加主题的时候，不能直接git clone第三方主题。 Fork该第三方主题仓库，这样就会在自己账号下生成一个同名的仓库，并对应一个url，我们应该git clone自己账号下的url。 这样做的原因是：我们很有可能在原来主题基础上做一些自定义的小改动，为了保持多终端的同步，我们需要将这些改动提交到远程仓库。而第三方仓库我们是无法直接push的。 这样就会出现git仓库的嵌套问题，我们通过git submodule来解决这个问题.下面这条命令会在themes/next目录下clone自己账号下的hexo-theme-next1$ git submodule add git@github.com:xxx/hexo-theme-next.git themes/next 查看submodule列表：git submodule补充一个删除submodule的方法： git submodule deinit themes/next 将第三方主题提交到source分支：123$ git add ./themes/*$ git commit -m 'add theme next and hueman'$ git push origin source 远程repo的source分支中并不会添加第三方主题的源码，而是类似指针一样，在themes目录下多两个（添加了2个第三方主题）链接到其他repo的文件夹： 修改了第三方主题文件之后，需要切换到第三方主题的目录下提交。1234$ cd ./themes/next$ git add _config.yml$ git commit -m 'config theme next'$ git push origin master 另一台机器下载 安装nodejs和git环境 从source分支clone博客源码无法clone第三方主题文件。 1$ git clone -b source https://github.com/xxx/MyBlogData.git MyBlog 安装hexo相关插件 12$ npm install# 安装package.json文件中配置的依赖包 clone第三方主题第三方主题在submodule里，需要执行下面语句将第三方主题文件clone下来。 12345$ git submodule initSubmodule 'themes/hueman' (git@github.com:xxx/hexo-theme-hueman.git) registered for path 'themes/hueman'Submodule 'themes/next' (git@github.com:xxx/hexo-theme-next.git) registered for path 'themes/next'$ git submodule update 我们修改的第三方主题文件提交到master分支了，需要切换到master分支将最新的代码pull下来。12345678910$ cd ./themes/next$ git branch* (HEAD detached at 9f7f5ae) master$ git checkout master$ git branch* master$ git pull 总结 将源码push在source分支 使用submodule解决第三方主题的问题 修改第三方主题文件的时候，需要切换到第三方主题的目录 参考：关于博客同步的解决办法: https://devtian.me/2015/03/17/blog-sync-solution/]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>git</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Titanic Data Science Solution(翻译)]]></title>
    <url>%2F2018%2F03%2F18%2FTitanic-Data-Science-Solution-%E7%BF%BB%E8%AF%91%2F</url>
    <content type="text"><![CDATA[原文链接：Titanic Data Science Solution 工作流kaggle比赛工作流包含7个阶段： 理解问题； 获取训练数据和测试数据； 数据清理； 分析、确定特征； 建模、训练、预测； 可视化、报告、提出解决问题的步骤和最终的方案； 提交结果。 理解问题仔细审题，理解是分类问题还是回归问题，或者其它。 获取数据123train_df = pd.read_csv('../input/train.csv')test_df = pd.read_csv('../input/test.csv')combine = [train_df, test_df] 数据清理pandas包含一些获取数据描述的方法。 数据有哪些特征？1print(train_df.columns.values) [‘PassengerId’ ‘Survived’ ‘Pclass’ ‘Name’ ‘Sex’ ‘Age’ ‘SibSp’‘Parch’ ‘Ticket’ ‘Fare’ ‘Cabin’ ‘Embarked’] 预览数据：1train_df.head() 哪些特征是分类的？有些特征能将数据集分成多个子集。例如性别。可以对这些特征进行可视化，分析数据的分布。 分类的：Survived（是否幸存），Sex（性别），Embarked（登船口） 序列的：Pclass（舱位等级） 哪些特征是数值的？这些数值特征是离散的、连续的、还是时间序列的？ 连续的：Age（年龄），Fare（票价） 离散的：SibSp，Parch 哪些特征的数据类型是混乱的？有些特征的数据类型既有数字的，也有字母的，这些特征在数据清理环节需要被处理。 Ticket：数字和字母混合的 Cabin（船舱）：字母的 哪些特征包含错误数据？对于大型数据集来说比较困难，但是从较小的数据集中查看一些示例可能得出哪些特性需要改正。 “Name”这个特征可能包含错误数据，因为有很多种方式来描述一个人的名字，如简称，名字字符串也可能附有圆括号或引号 哪些特征包含空值、null、NaN等？包含空值的特征，是具体情况采用不同的方式进行填补。 这三个特征包含空值：Cabin &gt; Age &gt; Embarked Carbin和Age在测试集中不完整 各个特征的数据类型是什么？123train_df.info()print("_" * 50)test_df.info() Output：12345678910111213141516171819202122232425262728293031323334&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;RangeIndex: 891 entries, 0 to 890Data columns (total 12 columns):PassengerId 891 non-null int64Survived 891 non-null int64Pclass 891 non-null int64Name 891 non-null objectSex 891 non-null objectAge 714 non-null float64SibSp 891 non-null int64Parch 891 non-null int64Ticket 891 non-null objectFare 891 non-null float64Cabin 204 non-null objectEmbarked 889 non-null objectdtypes: float64(2), int64(5), object(5)memory usage: 83.6+ KB________________________________________&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;RangeIndex: 418 entries, 0 to 417Data columns (total 11 columns):PassengerId 418 non-null int64Pclass 418 non-null int64Name 418 non-null objectSex 418 non-null objectAge 332 non-null float64SibSp 418 non-null int64Parch 418 non-null int64Ticket 418 non-null objectFare 417 non-null float64Cabin 91 non-null objectEmbarked 418 non-null objectdtypes: float64(2), int64(4), object(5)memory usage: 36.0+ KB 数值特征的分布是什么？这一步在早期分析中，有助于我们充分理解数据。 样本总数为891，占实际所有人数2224的40% “Survived”是一种具有0或1值的分类特征 样本中的幸存率大约是38%，实际的幸存率为32% 大多数乘客（超过75%）没有和父母或孩子一起旅行 近三成的乘客有兄弟姐妹 和/或 配偶 票价差异很大，很少有乘客（&lt;1%）支付高达512美元的费用 年龄在65-80岁之间的老人很少（&lt;1%） 123456train_df.describe()# Review survived rate using `percentiles=[.61, .62]` # knowing our problem description mentions 38% survival rate.# Review Parch distribution using `percentiles=[.75, .8]`# SibSp distribution `[.68, .69]`# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]` Output: PassengerId Survived Pclass Age SibSp Parch Fare count 891.000000 891.000000 891.000000 714.000000 891.000000 891.000000 891.000000 mean 446.000000 0.383838 2.308642 29.699118 0.523008 0.381594 32.204208 std 257.353842 0.486592 0.836071 14.526497 1.102743 0.806057 49.693429 min 1.000000 0.000000 1.000000 0.420000 0.000000 0.000000 0.000000 25% 223.500000 0.000000 2.000000 20.125000 0.000000 0.000000 7.910400 50% 446.000000 0.000000 3.000000 28.000000 0.000000 0.000000 14.454200 75% 668.500000 1.000000 3.000000 38.000000 1.000000 0.000000 31.000000 max 891.000000 1.000000 3.000000 80.000000 8.000000 6.000000 512.329200 分类特征的分布是什么样的？ Name是唯一的（下表中，name的count为891，与样本总数一致） Sex的取值只有两种，其中male占多数，占比577/891=64.9% Cabin(船舱)有重复的，其中204个样本有船舱号，不同的船舱号有104个。所以存在多个样本的船舱号一样的情况。译者注：同一个船舱号中的人可能都幸存。由此甚至可以推出，相同姓氏的人可能都幸存 Embarked（登船口）有三种取值，其中从S口登船的人最多，有664个 Ticket，有(891-681)/891=22%的样本的Tickt信息重复。译者注：可能是登记错误导致的数据错误 1train_df.describe(include=['O']) Output: Name Sex Ticket Cabin Embarked count 891 891 891 204 889 unique 891 2 681 147 3 top Lester, Mr. James male 347082 G6 S freq 1 577 7 4 644 基于数据分析进行假设基于上述简单的分析得到一些假设，然后对数据进行深入的分析，进行验证。 Correlating（寻找特征的关联性） 我们想知道每个特征与结果的关系。我们希望在项目的早期就这样做，并将这些快速的相关性与项目后面的建模相关性进行匹配。 Completing（将缺失数据的特征进行补全） 我们可能想要完整的”Age”特征，因为它肯定与生存相关。 我们可能想要将“Embarked（登船口）”补全，因为它也可能与生存或另一个重要的特征相关。 Correcting（纠正数据） 在我们的分析中，可能要扔掉“Ticket”特征，因为它包含了高比率的重复(22%)，并且Ticket很可能与Survived无关 在训练和测试数据集中，Carbin（舱室）特征可能会被删除，因为它高度不完整或包含许多空值。译者注：训练模型的时候可以把Carbin特征扔掉，后期模型融合的时候这个特征还是可以用的 “PassengerId”特征可以删除，因为它对生存没有帮助。 “Name”特征是相对不标准的，可能不会直接导致生存，所以可能会扔掉。 Creating（创造特征） 我们可能要基于Parch和SibSp创建一个新的特征，叫做“Family”，的家庭，以获得家庭成员的总数。 我们可能要从“Name”特征中提取“Title”作为一个新特征。 我们可能要为年龄层创造新的特征。这将一个连续的数字特征变成一个有序分类的特征。译者注：根据年龄建立直方图，每10岁为一个bin 我们可能还想创建一个Fare range的特征。译者注：与Age特征类似 Classifying（分类）我们还可以根据前面提到的问题描述增加我们的假设。 女性(性=女性)更可能存活。 儿童(Age小于多少)更有可能存活。 舱位等级越高的乘客(Pclass=1)更有可能幸存下来。 分别分析各个特征 Pclass，这个特征具有明显的相关性，Pclass=1的幸存率&gt;0.5(Classifying#3)。可以在模型中使用这个特性。 Sex，女性的幸存率很高，达到74%（Classifying） SibSp和Parch，没有相关性。最好从这些特性(Creating#1)派生一个特性或一组特性。 Pclass1234train_df[['Pclass', 'Survived']] .groupby(['Pclass'], as_index=False) .mean() .sort_values(by='Survived', ascending=False) Output: Pclass Survived 0 1 0.629630 1 2 0.472826 2 3 0.242363 Sex1234train_df[["Sex", "Survived"]] .groupby(['Sex'], as_index=False) .mean() .sort_values(by='Survived', ascending=False) Output: Sex Survived 0 female 0.742038 1 male 0.188908 SibSp1234train_df[["SibSp", "Survived"]] .groupby(['SibSp'], as_index=False) .mean() .sort_values(by='Survived', ascending=False) Output: SibSp Survived 1 1 0.535885 2 2 0.464286 0 0 0.345395 3 3 0.250000 4 4 0.166667 5 5 0.000000 6 8 0.000000 通过可视化来分析校正数值型特征对于特征『Age』，使用sns.FacetGrid分析：12g = sns.FacetGrid(train_df, col='Survived')g.map(plt.hist, 'Age', bins=20) Output: 从图中观察到： 婴儿（小于4岁）的幸存率很高； 最老的乘客（等于80岁）幸存了； 大量的15-25岁的乘客没幸存； 大部分乘客的年龄分布在15-35岁； 结论： 可以将特征『Age』放到最终的模型中； 『Age』列为空的，需要补全； 可以将『Age』特征进行分段； 校正数值型和序数型特征对特征『Pclass』进行分析123456# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)grid.map(plt.hist, 'Age', alpha=.5, bins=20)grid.add_legend(); Output: 从图中观察到： Pclass=3的占大多数，但是大多数都没幸存； 在Pclass=2和Pclass=3中的婴儿，大部分都幸存了； Pclass=1的乘客大多数都幸存了； Pclass在乘客的年龄分布上有所不同。 结论： 考虑将『Pclass』放到最终的模型中。 关联分类型特征对特征『Embarked』进行分析123456# grid = sns.FacetGrid(train_df, col='Embarked')grid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6)grid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')grid.add_legend() Output: 可以观察到： 女性乘客的幸存率较高； 对C和Q口而言，Pclass=3的男性比Pclass=2的有更高的幸存率（译者注：从图上显示的是从C口上船的乘客中，男性比女性幸存率高）； 对Pclass=3的男性乘客而言，Embarked对生存率有不同的影响。 结论： 将『Sex』放到最终的模型中； 补全『Embarked』特征，加到模型中。（译者注：没看懂。） 关联分类型和数值型特征我们还可能希望将分类特性(与非数值)和数字特性关联起来。我们可以考虑关联『Embarked』(非数字分类型)，『Sex』(非数字分类型)，Fare(连续的数值型)，与『Survived』(数字分类型)。 123456# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette=&#123;0: 'k', 1: 'w'&#125;)grid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)grid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)grid.add_legend() Output: 可以从图中观察到： 花费高的乘客具有较高的幸存率； 『Embarked』与幸存率有关。 结论： 考虑将『Fare』特征进行分段。 Wrangle data删除特征删除特征『Cabin』和『Ticket』123456789print("Before", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)train_df = train_df.drop(['Ticket', 'Cabin'], axis=1)test_df = test_df.drop(['Ticket', 'Cabin'], axis=1)combine = [train_df, test_df]print("After", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape) Output:12Before (891, 12) (418, 11) (891, 12) (418, 11)After (891, 10), (418, 9), (891, 10), (418, 9) 创造特征我们在放弃『Name』和『PassengerId』的特征之前，想要分析『Name』特征是否可以被设计来提取『title』并测试『title』与幸存之间的关系。在下面的代码中，我们使用正则表达式从『Name』特征中提取『Title』特征。正则表达式&quot;\w+\.&quot;匹配第一个单词是.结尾的。 12345for dataset in combine: dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\.', expand=False)pd.crosstab(train_df['Title'], train_df['Sex']) Output: Sex female male Title - - Capt 0 1 Col 0 2 Countess 1 0 Don 0 1 Dr 1 6 Jonkheer 0 1 Lady 1 0 Major 0 2 Master 0 40 Miss 182 0 Mlle 2 0 Mme 1 0 Mr 0 517 Mrs 125 0 Ms 1 0 Rev 0 6 Sir 0 1 上表中存在一些title的数量很少的情况，可以将这些数量很少的title用Rare代替：123456789for dataset in combine: dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\ 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare') dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss') dataset['Title'] = dataset['Title'].replace('Ms', 'Miss') dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs') train_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean() Output: Title Survived 0 Master 0.575000 1 Miss 0.702703 2 Mr 0.156673 3 Mrs 0.793651 4 Rare 0.347826 从上表可以观察到： 女性（Miss,Mrs）的幸存率较高 将上述『Title』特征转换成序列特征123456title_mapping = &#123;"Mr": 1, "Miss": 2, "Mrs": 3, "Master": 4, "Rare": 5&#125;for dataset in combine: dataset['Title'] = dataset['Title'].map(title_mapping) dataset['Title'] = dataset['Title'].fillna(0)train_df.head() Output: PassengerId Survived Pclass Name Sex Age SibSp Parch Fare Embarked Title 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 7.2500 S 1 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th… female 38.0 1 0 71.2833 C 3 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 7.9250 S 2 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 53.1000 S 3 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 8.0500 S 1 至此，我们可以删除特征『Name』和『PassengerId』。 1234train_df = train_df.drop(['Name', 'PassengerId'], axis=1)test_df = test_df.drop(['Name'], axis=1)combine = [train_df, test_df]train_df.shape, test_df.shape Output:1((891, 9), (418, 9)) 观察到（译者注：这块没看懂）: Most titles band Age groups accurately. For example: Master title has Age mean of 5 years. Survival among Title Age bands varies slightly. Certain titles mostly survived (Mme, Lady, Sir) or did not (Don, Rev, Jonkheer). 补全连续的数值型特征首先补全『Age』特征，考虑三种方法： 一种简单的方法是在一定的均值和标准差之间生成随机数。 更准确的猜测缺失值的方法是使用其他相关的特性。在我们的案例中，我们注意到年龄、性别和Pclass之间的相关性。根据Pclass和Gender将数据分为多个子集，然后在子集中取年龄的中值。例如，对于Pclass=0且Gender为male的样本，在Pclass=0且Gender为male的子集中，取Age的中值；然后对于Pclass=0且Gender为female的样本以此类推。。。 结合方法1和2。与其直接基于中值法猜测年龄值，不如根据Pclass和Age分类后，再使用均值和标准差之间的随机数。 方法1和方法3将引入随机噪声。实践中，这几个方法略有差异，我们更倾向于方法2. 12345# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')grid = sns.FacetGrid(train_df, row='Pclass', col='Sex', size=2.2, aspect=1.6)grid.map(plt.hist, 'Age', alpha=.5, bins=20)grid.add_legend() Output: Pclass有3种取值，Gender有2种取值，初始化一个2*3的数组，用来存储Age的中值：1234567891011121314151617181920212223242526guess_ages = np.zeros((2,3))guess_agesfor dataset in combine: for i in range(0, 2): for j in range(0, 3): guess_df = dataset[(dataset['Sex'] == i) &amp; \ (dataset['Pclass'] == j+1)]['Age'].dropna() # age_mean = guess_df.mean() # age_std = guess_df.std() # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std) age_guess = guess_df.median() # Convert random age float to nearest .5 age guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5 for i in range(0, 2): for j in range(0, 3): dataset.loc[ (dataset.Age.isnull()) &amp; (dataset.Sex == i) &amp; (dataset.Pclass == j+1),\ 'Age'] = guess_ages[i,j] dataset['Age'] = dataset['Age'].astype(int)#train_df.head() 将年龄分组：12345train_df['AgeBand'] = pd.cut(train_df['Age'], 5)train_df[['AgeBand', 'Survived']] .groupby(['AgeBand'], as_index=False) .mean() .sort_values(by='AgeBand', ascending=True) Output: AgeBand Survived 0 (-0.08, 16.0] 0.550000 1 (16.0, 32.0] 0.337374 2 (32.0, 48.0] 0.412037 3 (48.0, 64.0] 0.434783 4 (64.0, 80.0] 0.090909 将『AgeBand』转换为连续数值型特征：1234567for dataset in combine: dataset.loc[ dataset['Age'] &lt;= 16, 'Age'] = 0 dataset.loc[(dataset['Age'] &gt; 16) &amp; (dataset['Age'] &lt;= 32), 'Age'] = 1 dataset.loc[(dataset['Age'] &gt; 32) &amp; (dataset['Age'] &lt;= 48), 'Age'] = 2 dataset.loc[(dataset['Age'] &gt; 48) &amp; (dataset['Age'] &lt;= 64), 'Age'] = 3 dataset.loc[ dataset['Age'] &gt; 64, 'Age']train_df.head() 删除『AgeBand』特征：123train_df = train_df.drop(['AgeBand'], axis=1)combine = [train_df, test_df]train_df.head() 组合创造出新的特征 我们能将『Parch』和『SibSp』组合创造出新的特征『FamilySize』。 1234567for dataset in combine: dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1train_df[['FamilySize', 'Survived']] .groupby(['FamilySize'], as_index=False) .mean() .sort_values(by='Survived', ascending=False) Output: FamilySize Survived 3 4 0.724138 2 3 0.578431 1 2 0.552795 6 7 0.333333 0 1 0.303538 4 5 0.200000 5 6 0.136364 7 8 0.000000 8 11 0.000000 可以观察到，『FamilySize』特征与是否幸存无线性关系。 创造『IsAlone』特征1234567for dataset in combine: dataset['IsAlone'] = 0 dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1train_df[['IsAlone', 'Survived']] .groupby(['IsAlone'], as_index=False) .mean() Output: IsAlone Survived 0 0 0.505650 1 1 0.303538 至此，可以删掉『Parch』、『SibSp』、『FamilySize』特征，保留『IsAlone』特征。12345train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)test_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)combine = [train_df, test_df]train_df.head() 补全分类型特征『Embarked』特征具有三种取值:S、Q、C。训练集中有2个样本为空值，我们可以简单地用最多的一种取值代替。1234567891011freq_port = train_df.Embarked.dropna().mode()[0]freq_port'S'for dataset in combine: dataset['Embarked'] = dataset['Embarked'].fillna(freq_port) train_df[['Embarked', 'Survived']] .groupby(['Embarked'], as_index=False) .mean() .sort_values(by='Survived', ascending=False) Output: Embarked Survived 0 C 0.553571 1 Q 0.389610 2 S 0.339009 将分类型特征转换成数值型特征将『Embarked』特征转换成数值型特征，起个新名字『Port』。 123for dataset in combine: dataset['Embarked'] = dataset['Embarked'] .map( &#123;'S': 0, 'C': 1, 'Q': 2&#125; ).astype(int) 快速地补全、转换数值型特征用中值补全测试数据集『Fare』特征中唯一一个缺失值。 123test_df['Fare'].fillna( test_df['Fare'].dropna().median(), inplace=True) 创造『FareBand』特征：12345train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)train_df[['FareBand', 'Survived']] .groupby(['FareBand'], as_index=False) .mean() .sort_values(by='FareBand', ascending=True) Output: FareBand Survived 0 (-0.001, 7.91] 0.197309 1 (7.91, 14.454] 0.303571 2 (14.454, 31.0] 0.454955 3 (31.0, 512.329] 0.581081 可见『FareBand』特征与是否幸存线性有关。 将『FareBand』特征转换成连续数值型特征1234567891011for dataset in combine: dataset.loc[ dataset['Fare'] &lt;= 7.91, 'Fare'] = 0 dataset.loc[(dataset['Fare'] &gt; 7.91) &amp; (dataset['Fare'] &lt;= 14.454), 'Fare'] = 1 dataset.loc[(dataset['Fare'] &gt; 14.454) &amp; (dataset['Fare'] &lt;= 31), 'Fare'] = 2 dataset.loc[ dataset['Fare'] &gt; 31, 'Fare'] = 3 dataset['Fare'] = dataset['Fare'].astype(int)train_df = train_df.drop(['FareBand'], axis=1)combine = [train_df, test_df] train_df.head(10) 建模、预测、解决问题现在我们有60多种预测建模算法可供选择。我们必须了解问题的类型和解决方案的要求，以缩小到我们可以评估的少数几个模型。我们的问题是分类和回归问题。我们想要确定输出(Survived)与其他变量或特征(Sex、Age、Port……)之间的关系。我们也在实践一种机器学习方法，称为监督式学习，因为我们正在用给定的数据集训练我们的模型。有了这两个标准——监督学习加分类和回归，我们可以将模型的选择缩小到少数。这些包括: Logistics Regression KNN or k-Nearest Neighbors Support Vector Machines Naive Bayes classifier Decision Tree Random Forrest Perceptron Artificial neural network RVM or Relevance Vector Machine 12345X_train = train_df.drop("Survived", axis=1)Y_train = train_df["Survived"]X_test = test_df.drop("PassengerId", axis=1).copy()X_train.shape, Y_train.shape, X_test.shape((891, 8), (891,), (418, 8)) Logistic Regression12345678# Logistic Regressionlogreg = LogisticRegression()logreg.fit(X_train, Y_train)Y_pred = logreg.predict(X_test)acc_log = round(logreg.score(X_train, Y_train) * 100, 2)acc_log80.359999999999999 我们可以使用Logistic Regression来验证我们的假设和创造的特性。这可以通过分析特征的系数来实现。 正系数增加了响应的对数概率(从而增加了概率)，负系数减小了响应的对数概率(从而降低了概率)。12345coeff_df = pd.DataFrame(train_df.columns.delete(0))coeff_df.columns = ['Feature']coeff_df["Correlation"] = pd.Series(logreg.coef_[0])coeff_df.sort_values(by='Correlation', ascending=False) Output: Feature Correlation 1 Sex 2.201527 5 Title 0.398234 2 Age 0.287163 4 Embarked 0.261762 6 IsAlone 0.129140 3 Fare -0.085150 7 Age*Class -0.311200 0 Pclass -0.749007 『Sex』是最高的正系数，表示随着『Sex』值的增加(男性:0，女性:1)，幸存的概率增加最多。 当『Pclass』增加时，幸存的概率减少最多。 这种『Age*Class』是一个很好的人工特征模型，因为它与生存的负相关系数是第二高的。 『Title』是第二高正相关的。译者注：特征的重要性与系数的绝对值大小有关 SVM12345678# Support Vector Machinessvc = SVC()svc.fit(X_train, Y_train)Y_pred = svc.predict(X_test)acc_svc = round(svc.score(X_train, Y_train) * 100, 2)acc_svc83.840000000000003 KNN123456knn = KNeighborsClassifier(n_neighbors = 3)knn.fit(X_train, Y_train)Y_pred = knn.predict(X_test)acc_knn = round(knn.score(X_train, Y_train) * 100, 2)acc_knn84.739999999999995 Naive Bayes12345678# Gaussian Naive Bayesgaussian = GaussianNB()gaussian.fit(X_train, Y_train)Y_pred = gaussian.predict(X_test)acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)acc_gaussian72.280000000000001 Perceptron12345678# Perceptronperceptron = Perceptron()perceptron.fit(X_train, Y_train)Y_pred = perceptron.predict(X_test)acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)acc_perceptron78.0 Linear SVC12345678# Linear SVClinear_svc = LinearSVC()linear_svc.fit(X_train, Y_train)Y_pred = linear_svc.predict(X_test)acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)acc_linear_svc79.120000000000005 Stochastic Gradient Descent123456sgd = SGDClassifier()sgd.fit(X_train, Y_train)Y_pred = sgd.predict(X_test)acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)acc_sgd77.670000000000002 Decision Tree123456decision_tree = DecisionTreeClassifier()decision_tree.fit(X_train, Y_train)Y_pred = decision_tree.predict(X_test)acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)acc_decision_tree86.760000000000005 Random Forest1234567random_forest = RandomForestClassifier(n_estimators=100)random_forest.fit(X_train, Y_train)Y_pred = random_forest.predict(X_test)random_forest.score(X_train, Y_train)acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)acc_random_forest86.760000000000005 模型评估我们现在可以对所有的模型的结果进行排序，以选出最适合我们的模型。虽然决策树和随机森林得分相同，但我们选择使用随机森林，因为决策树容易过拟合。 123456789models = pd.DataFrame(&#123; 'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', 'Random Forest', 'Naive Bayes', 'Perceptron', 'Stochastic Gradient Decent', 'Linear SVC', 'Decision Tree'], 'Score': [acc_svc, acc_knn, acc_log, acc_random_forest, acc_gaussian, acc_perceptron, acc_sgd, acc_linear_svc, acc_decision_tree]&#125;)models.sort_values(by='Score', ascending=False) Output: Model Score 3 Random Forest 86.76 8 Decision Tree 86.76 1 KNN 84.74 0 Support Vector Machines 83.84 2 Logistic Regression 80.36 7 Linear SVC 79.12 5 Perceptron 78.00 6 Stochastic Gradient Decent 77.67 4 Naive Bayes 72.28 计算结果12345submission = pd.DataFrame(&#123; "PassengerId": test_df["PassengerId"], "Survived": Y_pred &#125;)# submission.to_csv('../output/submission.csv', index=False) 我们提交给比赛网站Kaggle的结果是在6082个样本中命中了3,883个。 参考文献： A journey through Titanic Getting Started with Pandas: Kaggle’s Titanic Competition Titanic Best Working Classifier 【部分表述有待完善。。。】]]></content>
      <categories>
        <category>kaggle</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>kaggle</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo 命令手册]]></title>
    <url>%2F2018%2F03%2F16%2Fhexo%E5%91%BD%E4%BB%A4%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[安装hexo，新建博客，安装server插件，启动博客 12345$ npm install hexo-cli -g$ hexo init blog$ cd blog$ npm install$ hexo server 草稿 123$ hexo new draft &lt;title&gt;$ hexo server --draft$ hexo publish &lt;title&gt; 插件安装与卸载 12$ npm install &lt;plubin name&gt;$ npm uninstall &lt;plugin name&gt;]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>npm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[797.All Paths From Source to Target]]></title>
    <url>%2F2018%2F03%2F16%2F797-All-Paths-From-Source-to-Target%2F</url>
    <content type="text"><![CDATA[LeetCode: 797.All Paths From Source to Target 问题描述Given a directed, acyclic graph of N nodes. Find all possible paths from node 0 to node N-1, and return them in any order. The graph is given as follows: the nodes are 0, 1, …, graph.length - 1. graph[i] is a list of all nodes j for which the edge (i, j) exists. Example:12345678Input: [[1,2], [3], [3], []] Output: [[0,1,3],[0,2,3]] Explanation: The graph looks like this:0---&gt;1| |v v2---&gt;3There are two paths: 0 -&gt; 1 -&gt; 3 and 0 -&gt; 2 -&gt; 3. Note: The number of nodes in the graph will be in the range [2, 15]. You can print different paths in any order, but you should keep the order of nodes inside one path. 分析使用dfs 代码1234567891011121314151617181920212223242526272829303132333435363738class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; allPathsSourceTarget(int[][] graph) &#123; int n = graph.length; // 结点个数 boolean[] visited = new boolean[n]; // 记录i结点是否被访问过 List&lt;Integer&gt; path = new ArrayList&lt;Integer&gt;(); // 路径 path.add(0); // 初始化路径 List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;List&lt;Integer&gt;&gt;(); dfs(graph, visited, path, 0, n - 1, result); return result; &#125; /** * int[] visited 表示节点i是否被访问过 * List&lt;Integer&gt; path 表示路径 * int curr 表示当前访问的结点 * int target 表示目标结点，也就是graph.length-1 * List&lt;List&lt;Integer&gt;&gt; result 存储所有的满足条件的路径 */ public void dfs(int[][] graph, boolean[] visited, List&lt;Integer&gt; path, int curr, int target, List&lt;List&lt;Integer&gt;&gt; result)&#123; //System.out.println("curr:" + curr); if (curr == target)&#123; result.add(new ArrayList&lt;Integer&gt;(path)); return; &#125; for(int i: graph[curr])&#123; //System.out.println("curr:" + curr + ", visite:" + i + ", status:" + visited[i]); if (!visited[i])&#123; // 如果i未被访问 visited[i] = true; // 访问i节点，将i添加到path中 path.add(i); dfs(graph, visited, path, i, target, result); visited[i] = false; // 不访问i节点，将i从path中删除 path.remove(path.size() - 1); &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[553. Optimal Division]]></title>
    <url>%2F2018%2F03%2F15%2F553-Optimal-Division%2F</url>
    <content type="text"><![CDATA[LeetCode: 553. Optimal Division 问题描述Given a list of positive integers, the adjacent integers will perform the float division. For example, [2,3,4] -&gt; 2 / 3 / 4. However, you can add any number of parenthesis at any position to change the priority of operations. You should find out how to add parenthesis to get the maximum result, and return the corresponding expression in string format. Your expression should NOT contain redundant parenthesis. Example: Input: [1000,100,10,2]Output: “1000/(100/10/2)”Explanation:1000/(100/10/2) = 1000/((100/10)/2) = 200However, the bold parenthesis in “1000/((100/10)/2)” are redundant,since they don’t influence the operation priority. So you should return &gt; “1000/(100/10/2)”. Other cases:1000/(100/10)/2 = 501000/(100/(10/2)) = 501000/100/10/2 = 0.51000/100/(10/2) = 2 Note: The length of the input array is [1, 10]. Elements in the given array will be in range [2, 1000]. There is only one optimal division for each test case. 分析用动态规划求解 代码 只求最大值，不用得到表达式 12345678910111213141516171819202122class Solution(object): def __init__(self): self.mat = [] def optimalDivision(self, nums): """ :type nums: List[int] :rtype: str """ n, mat = len(nums), [] for i in range(n): mat.append([0] * n) for margin in range(1, n): for i in range(n - margin): if margin == 1: mat[i][i+margin] = nums[i] / nums[i+margin] #右上角存最大值 mat[i+margin][i] = nums[i] / nums[i+margin] #左下角存最小值 else: mat[i][i+margin] = max(nums[i] / mat[i+margin][i+1], mat[i][i+margin-1] / nums[i+margin]) mat[i+margin][i] = min(nums[i] / mat[i+1][i+margin], mat[i+margin-1][i] / nums[i+margin]) return mat[0][n-1] 用分治，求得表达式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465class Solution(object): def __init__(self): self.mat = [] self.exp = [] def optimalDivision(self, nums): """ :type nums: List[int] :rtype: str """ def dev(nums, start, end): #print nums, 'start:', start, 'end:', end maxDev, minDev, expMax, expMin = 0, 10e8, '', '' if start == end: #print 'minDev:', nums[start], 'maxDev:', nums[start], 'expMin:', nums[start], 'expMax:', nums[start] self.mat[start][end], self.mat[end][start] = nums[start], nums[start] self.exp[start][end], self.exp[end][start] = str(nums[start]), str(nums[start]) maxDev, minDev, expMax, expMin = nums[start], nums[start], str(nums[start]), str(nums[start]) elif start + 1 == end: valDev = float(nums[start]) / nums[end] expDev = str(nums[start]) + "/" + str(nums[end]) self.mat[start][end] = valDev #右上角存最大值 self.mat[end][start] = valDev #左下角存最小值 self.exp[start][end] = expDev #右上角存最大值 self.exp[end][start] = expDev #左下角存最小值 #print 'minDev:', valDev, 'maxDev:', valDev, 'expMin:', expDev, 'expMax:', expDev maxDev, minDev, expMax, expMin = valDev, valDev, expDev, expDev else: for split in range(start, end): if self.mat[start][split] == 0: left = dev(nums, start, split) else: left = self.mat[start][split], self.mat[split][start], self.exp[start][split], self.exp[split][start] if self.mat[split+1][end] == 0: right = dev(nums, split+1, end) else: right = self.mat[split+1][end], self.mat[end][split+1], self.exp[split+1][end], self.exp[end][split+1] valMin, valMax = float(left[1]) / right[0], float(left[0]) / right[1] if valMin &lt; minDev: minDev = valMin if "/" in right[2]: expMin = left[3] + '/(' + right[2] + ')' else: expMin = left[3] + '/' + right[2] if valMax &gt; maxDev: maxDev = valMax if "/" in right[3]: expMax = left[2] + '/(' + right[3] + ')' else: expMin = left[2] + '/' + right[3] self.mat[start][end], self.mat[end][start] = maxDev, minDev self.exp[start][end], self.exp[end][start] = expMax, expMin #print 'minDev:', minDev, 'maxDev:', maxDev, 'expMin:', expMin, 'expMax:', expMax return maxDev, minDev, expMax, expMin n = len(nums) for i in range(n): self.mat.append([0] * n) self.exp.append([''] * n) result = dev(nums, 0, n-1) return result[2]]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[632.Smallest Range]]></title>
    <url>%2F2018%2F03%2F15%2F632-Smallest-Range%2F</url>
    <content type="text"><![CDATA[LeetCode: 632.Smallest Range 问题描述You have k lists of sorted integers in ascending order. Find the smallest range that includes at least one number from each of the k lists. We define the range [a,b] is smaller than range [c,d] if b-a &lt; d-c or a &lt; c if b-a == d-c. Example 1: Input:[[4,10,15,24,26], [0,9,12,20], [5,18,22,30]]Output: [20,24]Explanation:List 1: [4, 10, 15, 24,26], 24 is in range [20,24].List 2: [0, 9, 12, 20], 20 is in range [20,24].List 3: [5, 18, 22, 30], 22 is in range [20,24]. Note: The given list may contain duplicates, so ascending order means &gt;= here. 1 &lt;= k &lt;= 3500 -105 &lt;= value of elements &lt;= 105. For Java users, please note that the input type has been changed to List&lt;List&gt;. And after you reset the code template, you’ll see this point. 题意给定k个数组，找出一个最小的区间，使得区间内包含每个数字内至少一个数。 分析用一个优先队列，里面存k个分别来自k个数组的数。 每次从队列里弹出一个最小值，并从弹出值的数组里，添加下一个值。 每次弹出时，计算但是的range，如果比之前的小，就替换掉之前的range，作为一个新结果。 队列，能满足区间里同时来自k个数组的k个数； 最小区间，通过这个来满足：每次往里队列里添加的都是同budga下一个数（也就是紧接着最小的数），如果当期range小于上一个range就替换之。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445public class Solution &#123; public int[] smallestRange(List&lt;List&lt;Integer&gt;&gt; nums) &#123; PriorityQueue&lt;Element&gt; priorityQueue = new PriorityQueue&lt;Element&gt;(new Comparator&lt;Element&gt;() &#123; @Override public int compare(Element o1, Element o2) &#123; return o1.value - o2.value; &#125; &#125;); int maxValue = Integer.MIN_VALUE; for (int i = 0; i &lt; nums.size(); i++) &#123; Element element = new Element(nums.get(i).get(0), 0, i); priorityQueue.offer(element); maxValue = Math.max(maxValue, nums.get(i).get(0)); &#125; int range = Integer.MAX_VALUE; int start = -1, end = -1; while(priorityQueue.size() == nums.size())&#123; Element popElement = priorityQueue.poll(); if (maxValue - popElement.value &lt; range) &#123; start = popElement.value; end = maxValue; range = maxValue - popElement.value; &#125; int index = popElement.index + 1; if (index &lt; nums.get(popElement.budge).size())&#123; Element element = new Element(nums.get(popElement.budge).get(index), index, popElement.budge); priorityQueue.offer(element); maxValue = Math.max(maxValue, element.value); &#125; &#125; return new int[]&#123;start, end&#125;; &#125;&#125;class Element&#123; public int value; public int index; public int budge; public Element(int v, int i, int b)&#123; this.value = v; this.index = i; this.budge = b; &#125;&#125;]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
</search>
