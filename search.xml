<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[821.Shortest Distance to a Character]]></title>
    <url>%2F2018%2F06%2F24%2F821-Shortest-Distance-to-a-Character%2F</url>
    <content type="text"><![CDATA[LeetCode链接：821.Shortest Distance to a Character 题目给定一个字符串 S 和一个字符 C，返回一组整数，表示字符串中每个字符到字符 C 的最短距离。 样例 Input: S = “loveleetcode”, C = ‘e’Output: [3, 2, 1, 0, 1, 0, 0, 1, 2, 2, 1, 0] 注意： S 的长度在 [1, 10000] 之间. C 是一个单字符, 并且一定在 S 中出现. 所有的字符都是小写字符. 分析用一个数组index记录字符C在S中出现的位置。 index = [3, 5, 6, 11] 遍历i in [0, len-1]，求出i与index中任意一个值的『最小差值』. 遍历[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]，求i与[3, 5, 6, 11]中任意一个值的『最小差值』。两重遍历的话，时间复杂度为$O(n^2)$. 恰巧的是，数组index是有序的，所以只需要用两个角标[left, right]，来分别记录『恰好比i大』和『恰好比i小』的值的索引。时间复杂度为$O(n)$. 代码Python代码如下：123456789101112131415161718192021222324def shortestToChar(self, S, C): """ :type S: str :type C: str :rtype: List[int] """ # 字符C出现的index index = [idx for idx, c in enumerate(S) if c == C] # S中任意一个字符分别到左右两边最近的C的index # 遍历S的过程中，保证：index[left] &lt;= i &lt; index[right] left = right = 0 res = [] for i in range(len(S)): # print (i, left, right) res.append(min(abs(i - index[left]), abs(i - index[right]))) # 保证：index[right] &gt; i if i &gt;= index[right] and right +1 &lt; len(index): right += 1 # 保证：i &gt;= index[left] if left + 1 &lt; len(index) and i &gt;= index[left+1]: left += 1 return res 官方答案 对于每一个索引i，我们从左往右遍历一遍，记录上次C出现的位置prev，i-prev就是字符S[i]到左边最近的C的距离。 同样，再从右往左遍历一遍，prev-i就是字符S[i]到右边最近的C的距离。 取min(i-prev, prev-i)，就是我们想要的结果。 12345678910111213def shortestToChar(self, S, C): prev = float('-inf') ans = [] for i, x in enumerate(S): if x == C: prev = i ans.append(i - prev) prev = float('inf') for i in xrange(len(S) - 1, -1, -1): if S[i] == C: prev = i ans[i] = min(ans[i], prev - i) return ans]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>字符串</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[814.Binary Tree Pruning]]></title>
    <url>%2F2018%2F06%2F24%2F814-Binary-Tree-Pruning%2F</url>
    <content type="text"><![CDATA[LeetCode链接：814.Binary Tree Pruning 题目给定一个二叉树，节点的值要么是0、要么是1。要求把所有节点都为0的子树剪掉。 举例Example 1: Input: [1,null,0,0,1]Output: [1,null,0,null,1] Explanation:Only the red nodes satisfy the property “every subtree not containing a 1”.The diagram on the right represents the answer. Example 2: Input: [1,0,1,0,0,0,1]Output: [1,null,1,null,1] 分析根据题意，如果某个子树的『和』等于0，就可以将其剪掉（node.left=None或者node.right=Node）. 求子树的『和』，可以使用后序遍历。 除了记录子树的『和』之外，还需要记录子树的『节点个数』。只有同时满足『和』=0且『节点个数』&gt;0时，才将其剪掉。 因此，用两个值[sum, num]分别记录以当前节点为根节点的子树的『和』与『节点个数』。这两个值的更新如下：12sum = left_sum + right_sum + node.valnum = left_num + right_num + 1 代码Python代码如下：123456789101112131415161718192021222324252627def pruneTree(self, root): """ :type root: TreeNode :rtype: TreeNode """ def helper(node): if not node: return [0, 0] # 后序遍历，计算[和，节点个数] [left_sum, left_num] = helper(node.left) [right_sum, right_num] = helper(node.right) sum = left_sum + right_sum + node.val num = left_num + right_num + 1 # 判断并剪枝 # 如果左子树『节点个数』&gt; 0 且 『和』= 0 if left_num &gt; 0 and left_sum == 0: node.left = None # 如果又子树『节点个数』&gt; 0 且 『和』= 0 if right_num &gt; 0 and right_sum == 0: node.right = None # 返回：以当前节点为根节点的子树的『和』与『节点个数』 return [sum, num] helper(root) return root]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
        <tag>Binary Tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概述:梯度下降优化算法]]></title>
    <url>%2F2018%2F06%2F17%2F%E6%A6%82%E8%BF%B0-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[梯度下降法是最流行的优化算法之一，也是迄今为止最常用的优化神经网络的方法。同时，每一个最先进的深度学习库都包含了多种优化梯度下降法的实现(如Lasagne、Caffe、Keras)。 梯度是使目标函数增大的方向，梯度下降法就是沿着梯度的反方向去更新参数，使目标函数越来越小。$$\theta_{t+1} = \theta_t - \eta \nabla_\theta J(\theta)$$其中，梯度是决定目标函数改变的方向，$\eta$决定了每次更新的步长。 梯度下降法的三个变种Batch gradient descent（批量梯度下降法）批量梯度下降法每次计算梯度时，都是使用整个训练集的所有样本。$$\theta_{t+1} = \theta_t - \eta \nabla_\theta J(\theta)$$批量梯度下降法每更新一次参数，都需要对所有样本进行一次梯度的计算，所以它比较慢。而且当数据量很大时，对内存的要求也比较高。 也不支持在线更新模型。 Python代码：123for i in range(nb_epochs): params_grad = evaluate_gradient(loss_function, data, params) params = params - learning_rate * params_grad Stochastic gradient descent（随机梯度下降法）批量梯度下降法(SGD)每次计算梯度时，都只使用其中一个训练样本。$$\theta_{t+1} = \theta_t - \eta \nabla_\theta J(\theta; x^{(i)}; y^{(i)})$$ SGD比批量梯度下降法更快，也能用于在线学习。 但是SGD的参数更新太频繁，会导致参数更新时产生较大的方差（参数波动大），从而会使目标函数产生剧烈的波动，如下图。 当批量梯度下降法收敛到局部最小值时，SGD的波动效应，一方面能使参数跳跃到新的或潜在更好的局部最小值；另一方面，这最终会使参数收敛到正确的最小值。然而，已经表明，当我们慢慢地降低学习率时，SGD显示出与批梯度下降相同的收敛行为，几乎可以肯定地收敛到非凸和凸优化的局部或全局最小值。 Python代码：12345for i in range(nb_epochs): np.random.shuffle(data) for example in data: params_grad = evaluate_gradient(loss_function, example, params) params = params - learning_rate * params_grad Mini-batch gradient descent（小批量梯度下降法）小批量梯度下降法是兼顾了批量梯度下降法和SGD，每次更新参数时，使用一小批n个样本来计算梯度。$$\theta_{t+1} = \theta_t - \eta \nabla_\theta J(\theta; x^{(i:i+n)}; y^{(i:i+n)})$$ 优点： 降低了参数更新的方差，会使收敛更加稳定； 小批量数据可以使用矩阵操作来进行优化计算效率。 一般批量的大小选择50-256之间，根据训练集的大小而定。Python代码：12345for i in range(nb_epochs): np.random.shuffle(data) for batch in get_batches(data, batch_size=50): params_grad = evaluate_gradient(loss_function, batch, params) params = params - learning_rate * params_grad 存在的问题小批量梯度下降法不能保证良好的收敛性，在实际使用过程中，面临如下挑战： 如何选择合适的学习率？ 如何对不同的参数应用不同的学习率？因为，当数据稀疏并且特征具有不同的频次时，我们可能不希望将对所有的参数执行相同程度的更新，而是对于很少出现的特征，进行大幅地更新。 如何解决局部最小值和鞍点的问题？Dauphin等人 [19]认为，事实上问题往往并不是由局部最小值产生，而是由于鞍点导致的。 梯度下降优化算法Momentum（动量）SGD在遇到『峡谷』的时候，会表现出在峡谷斜坡上反复摇摆的情况。如下图： 加上『动量』之后，有助于抑制这种振荡。$$v_t = \gamma v_{t-1} + \eta \nabla_\theta J(\theta) $$$$\theta = \theta - v_t$$其中，$\gamma$一般取0.9根据『平行四边形法则』，加上『动量』后的方向，如下图： 『动量』有一个很好的特性：对于梯度方向不变的维度，动量一直增加；对于梯度方向改变的维度，动量减少，参数更新的幅度也小。因此，『动量』能加速收敛，减少振荡。 除此之外，当遇到局部最小值时，如果『动量』足够大，也能越过这个局部最小值点。 Nesterov accelerated gradient（NAG，Nesterov 加速梯度）然而，如果一个滚下山坡的球，盲目地关注于沿着动量方向滚下去的话，而轻视了它『将去的地方』的话，就会对收敛速度产生影响。 比如说，当它再次遇到上升的山坡的时候，会冲上山坡，而不是减速。 Momentum与NAG的区别： 基于动量的方法，在计算算当前的速度方向时，是用上一次的动量+『当前点』的梯度方向。 而NAG在计算当前的速度方向时，是用上一次的动量+『更新后的点』的梯度方向。 为什么NAG用『更新后的点』的梯度方向？因为『更新后的点』比『当前点』更指向极小值点。（也就是下图中『lookahead』的效果） 『更新后的点』的梯度方向未知，所以通过 $\nabla_\theta J(\theta - \gamma v_{t-1})$ 来近似计算。 上图中，右侧 NAG 的红色箭头比左侧 Momentum 的红色箭头的方向更加指向极小值点。 公式如下：$$v_t = \gamma v_{t-1} + \eta \nabla_\theta J(\theta - \gamma v_{t-1}) $$$$\theta = \theta - v_t$$ Adagrad Adagrad对不同维度的参数，使用不同的学习率。 Adagrad对频率高的特征进行小的更新，对频率低的特征进行大的更新。 Dean(Dean, J., Corrado, G. S., Monga, R., Chen, K., Devin, M., Le, Q. V, … Ng, A. Y. (2012). Large Scale Distributed Deep Networks. NIPS 2012: Neural Information Processing Systems, 1–11. )等人发现，Adagrad 极大地提高了 SGD 的鲁棒性，并用它在谷歌(Google)训练大规模神经网络，这些网络学会了在 Youtube 视频中识别猫。 此外，Pennington 等人使用 Adagrad 来训练GloVe word embeddings，因为不频繁的出现的单词需要比常用词获得更大的更新。 我们用 $g_{t,i}$ 表示：第 $t$ 次迭代时，目标函数对参数的第 $i$ 维的偏导。$$ g_{t,i} = \nabla_\theta J(\theta_{t,i}) $$ SGD，每一维参数分别更新的话，公式如下：$$ \theta_{t+1,i} = \theta_{t,i} - \eta \cdot g_{t,i}$$ Adagrad，步长是基于『过去所有的梯度』计算的，每一维的步长都不一样：$$ \theta_{t+1,i} = \theta_{t,i} - \frac \eta {\sqrt{G_{t, ii} + \epsilon}} \cdot g_{t,i}$$其中，$G_{t}$ 是对角矩阵，每个元素 $G_{t,ii}$ 都是『过去所有梯度的平方和』。 $\epsilon$ 是个拉普拉斯平滑常数。这里的平方根运算很重要，如果没有的话，算法的表现会很糟糕。 Adagrad的公式如下： $$ v_t = v_{t-1} + \text{diag}(g_t^2) $$$$ \Delta{\theta_t} = -\frac{\eta}{\sqrt{v_t+\epsilon}} * g_t $$ $$ \theta_{i+1}= \theta_t - \frac{\eta}{\sqrt{v_t+\epsilon}}*g_t $$ 特点： 前期$g_t$较小的时候，regularizer较大，能够放大梯度，加速向极小值点逼近 后期$g_t$较大的时候，regularizer较小，能够约束梯度，在极小值点附近减速 适合处理稀疏梯度。因为它对每一维参数的更新的步长都不一样。对频率高的特征进行小的更新，对频率低的特征进行大的更新。 缺点： 由公式可以看出，仍依赖于人工设置一个全局学习率 $\eta$设置过大的话，会使regularizer过于敏感，对梯度的调节太大 中后期，分母上梯度平方的累加将会越来越大，使$gradient\to0$，使得训练提前结束 RMSProp在 Adagrad 中， $v_t$ 是单调递增的，使得学习率逐渐递减至 0，可能导致训练过程提前结束。为了改进这一缺点，可以考虑在计算二阶动量时不累积全部历史梯度，而只关注最近某一时间窗口(长度为w)内的梯度。 名称理解：RMS，Root Mean Squared，均方根。Prop, Proportion，比例RMSProp，也就是在均方根的基础上，加了一个比例，起到只考虑最近一段时间窗口(w步)内的梯度的作用。但是它不直接存储w步内的梯度，而是使用一个proportion对其进行近似计算。 $$ v_t = \gamma v_{t-1} + (1-\gamma) \cdot \text{diag}(g_t^2) $$$$ \theta_{i+1}= \theta_t -\frac{\eta}{\sqrt{v_t+\epsilon}}*g_t $$ Hinton建议，$\gamma$ = 0.9, $\eta$ = 0.001 优点： RMSprop算是Adagrad的一种发展，和Adadelta的变体，效果趋于二者之间 适合处理非平稳目标 - 对于RNN效果很好 缺点： 依然依赖于人工设置的全局学习率 Adadelta针对梯度平方和累计越来越大的问题，解决方法与RMSProp一致，在此基础上，Adadelta做了一些摆脱超参 $\eta$ 的工作。 最初的方案依然是对学习率进行自适应约束：$$ v_t=\nu v_{t-1}+(1-\nu) \cdot \text{diag}(g_t^2) $$ $$ \Delta{\theta_t} = -\frac{\eta}{\sqrt{v_t+\epsilon}}*g_t $$ 在此处Adadelta其实还是依赖于全局学习率的，但是作者做了一定处理，经过近似牛顿迭代法之后： $$ E|g^2|_t = \rho \cdot E|g^2| _ {t-1} + (1-\rho) \cdot \text{diag}(g_t^2)$$ $$ \Delta{x_t}=-\frac{\sqrt{\sum_{r=1}^{t-1}\Delta{x_r}}}{\sqrt{E|g^2|_t+\epsilon}} $$其中，E代表求期望。 此时，可以看出Adadelta已经不用依赖于全局学习率了。 特点： 训练初中期，加速效果不错，很快 训练后期，反复在局部最小值附近抖动 AdamAdam(Adaptive Moment Estimation)可以认为是 RMSprop 和 Momentum 的结合。和 RMSprop 对二阶动量使用指数移动平均类似，Adam 中对一阶动量也是用指数移动平均计算，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。公式如下： $$ m_t=\mu m_{t-1}+(1-\mu) g_t $$ $$ v_t=\nu*v_{t-1}+(1-\nu) \cdot \text{diag}(g_t^2) $$ 其中，初值 $$ m_0 = 0 $$ $$ v_0 = 0 $$ 注意到，在迭代初始阶段，$m_t$ 和 $v_t$ 有一个向初值的偏移（过多的偏向了 0）。因此，可以对一阶和二阶动量做偏置校正 (bias correction)， $$ \hat{m_t}=\frac{m_t}{1-\mu^t} $$ $$ \hat{v_t}=\frac{v_t}{1-\nu^t} $$ $$ \Delta{\theta_t}=-\frac{\hat{m_t}}{\sqrt{\hat{v_t}}+\epsilon}*\eta $$其中，$m_t$，$v_t$分别是对梯度的一阶矩估计和二阶矩估计，可以看作对期望$E|g_t|$，$E|g_t^2|$的估计；$\hat{m_t}$，$\hat{v_t}$是对$m_t$，$v_t$的校正，这样可以近似为对期望的无偏估计。 可以看出，直接对梯度的矩估计对内存没有额外的要求，而且可以根据梯度进行动态调整，而$-\frac{\hat{m_t}}{\sqrt{\hat{v_t}}+\epsilon}$对学习率形成一个动态约束，而且有明确的范围。 特点： 结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点 对内存需求较小 为不同的参数计算不同的自适应学习率 也适用于大多非凸优化 - 适用于大数据集和高维空间 经验之谈 对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值 SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠 如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。 Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。 在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果 可视化对比收敛速度 收敛速度依次是：Adadelta &gt; Adagrad &gt; RMSProp &gt;&gt; NAG &gt; Momentum &gt;SGD Adagrad、Adadelta 和 RMSprop 几乎立即朝着正确的方向飞去，并且收敛的很快，而 Momentum 和 NAG 则偏离了轨道，就像一个惯性很大的球滚下山坡的情景。 然而，NAG 很快就能够改变它的路线，因为它通过向前看、向最小的方向看增强了它的反应能力。 鞍点 SGD、Momentum和NAG比较难逃脱鞍点 SGD困在了鞍点 Adagrad、RMSProp、Adadelta很快就避开了鞍点 结论 『Mini-batch gradient descent』：小批量梯度下降法无法解决自适应学习率的问题； 『Momentum』：基于物理上的惯性，每一步的梯度等于上一次的运动方向+本次的梯度； 『Nesterov加速梯度』：在『Momentum』的基础上，用『估测的下一步梯度方向』替换掉『本次的梯度方向』，起到了加速的作用； 『Adagrad』：在计算步长时，使用『过去所有梯度的平方和』，是每一维的步长都不一样，达到对稀疏的特征进行大幅的更新，对稠密的特征进行小幅的更新的效果； 『RMSProp』：摒弃『过去所有梯度』，使用一段时间窗内的梯度，解决Adagrad梯度趋近于0的问题； 『Adam』：不依赖于人工定义的全局步长，是首选的方法。 参考文献1. http://ruder.io/optimizing-gradient-descent2. https://blog.csdn.net/google19890102/article/details/699429703. https://zhuanlan.zhihu.com/p/22252270]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Gradient Descent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李宏毅机器学习2017学习笔记:14.Ensemble]]></title>
    <url>%2F2018%2F05%2F19%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02017%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-14-Ensemble%2F</url>
    <content type="text"><![CDATA[Bagging『复杂模型』的特点是Bias较小、Variance较大，容易Overfit。 Bagging的原理就是：通过将多个『复杂模型』求平均，来降低Variance。 做法是： 训练阶段。在包含N个样本的样本集中，做多次重采样，每次采n个； 测试阶段。对多个模型的结果求平均或者投票。 Random ForestDescision Tree只要决策树的深度没有限制，很容易训练出一个错误率为0的决策树模型。但是存在过拟合的缺点。 Random Forest随机森林是基于决策树的Bagging，过程如下： 在包含N个样本的样本集中，通过重采样的方式采n个样本。 对于每次采样，从M个特征中随机选择m个特征（m&lt;&lt;M），在这个样本集上训练一个决策树。 重复K次，产生K个决策树。 通过投票的方式，进行预测。 两个『随机性』随机森林中引入的两个『随机性』：一个是重采样，另一个是随机选择特征。这两个『随机性』使随机森林不容易过拟合，并且具有很好的抗噪能力（比如：对缺省值不敏感）。 两个影响因素随机森林的错误率与两个因素有关： 森林中任意两棵树的相关性：相关性越小，错误率越小； 森林中每棵树的分类能力：分类能力越强，错误率越小。 减小特征选择个数m的大小，会减小树的相关性，同时减小分类能力。增加m，相关性增加，分类能力增加。所以关键问题是如何选择最优的m，这也是随机森林位移的一个参数。 Out-of-bag（OOB）如何选择最优的m，通过oob error来衡量。OOB就是每次没有被采样的样本集合。 随机森林的一个重要优点是，不需要进行交叉验证，或者不需要用一个独立的测试集来获得误差的无偏估计。它可以在内部进行评估，也就是在生成的过程中在OOB上对误差进行无偏估计。 特征选择基于OOB误差对某个特征X的重要性，计算步骤如下： 对每一棵决策树，计算OOB误差，记为errOOB1. 随机对OOB中所有样本的特征X加入噪声干扰，再次计算OOB误差，记为errOOB2。 ​假设森林中有n棵树，则特征X的重要性为∑（errOOB2-errOOB1）/ n。 这个数值之所以能够说明特征的重要性是因为，如果加入随机噪声后，袋外数据准确率大幅度下降（即errOOB2上升），说明这个特征对于样本的预测结果有很大影响，进而说明重要程度比较高。 基于Gini指数Scikit-learn中几种常用的特征选择方法 BoostingBoosting将多个『弱模型』进行组合，得到一个强分类器。 理论前提：如果一个分类器的错误率小于0.5，经过Boosting后，它就能得到一个错误率为0的分类器。 对于二分类而言，我们总是能找到一个$\varepsilon &lt; 0.5$的分类器。因为如果$\varepsilon &gt; 0.5$，只需要将预测结果翻转，就能使$\varepsilon &lt; 0.5$了。 Boosting的框架： 得到第一个弱分类器$f_1(x)$ 找到另一个分类器$f_2(x)$去辅助$f_1(x)$。$f_2(x)$起到对$f_1(x)$弥补的作用。 按照以上方法，依次得到k分类器$f_3(x), f_4(x)$…$f_k(x)$ 备注： Bagging的每个子分类器都是强分类器；Boosting的是弱分类器。 Bagging的分类器是无序的；Boosting的则是有序的。 在上一个分类器的基础上，如何得到不同的分类器？：制造不同的训练数据集。具体方法： 重采样（也可以理解成改变样本的权重） 改变每个样本的权重 在改变权重的基础上，也要响应地修改损失函数. AdaBoostingAdaBoosting的原理是通过改变数据的分布，来使上次被分错的样本被分对。 具体做法是：根据模型的准确率和样本是否被分对，来修改样本的权重。如果某个样本被分错，则增加该样本的权重；否则降低该样本的权重。 假设：$f_1(x)$的错误率为：$\varepsilon_1$$$\varepsilon_1 = \frac {\sum_n u_1^n \delta(f_1(x^n) \neq \hat y^n)} {Z_1}$$其中，$u_1$为样本的权重，$Z_1$是权重的和，$Z_1=\sum_n u_1^n$ 为了使$f_2(x)$对$f_1(x)$起到『互补』的作用，我们要增加分错的样本的权重。同时，更新权重会使$f_1(x)$的错误率上升。上升到什么程度呢？最多达到$\varepsilon_1 = 0.5$，也就是：$$\frac {\sum_n u_2^n \delta(f_1(x^n) \neq \hat y^n)} {Z_2} = 0.5$$更新权重后，训练得到$f_2(x)$. 举个例子，如下图： 第一轮，每个样本的权重都为1，$f_1(x)$的错误率$\varepsilon_1=0.25$。 更新权重后，$f_1(x)$的错误率变为$\varepsilon_1=0.5$，$f_1(x)$的$\varepsilon_1&lt;0.5$ 如何更新权重分错的样本权重增加，分对的样本权重降低。 设:更新前的权重为$u_1$，更新后的权重为$u_2$ 更新前： 更新权重： 更新后，$f_1(x)$的$\varepsilon_1=0.5$： 上式的『分子部分』是分错的部分：『分母部分』是分错+分对两部分： 也就是： 变换，提取出$d_1$： $d_1$的平方，就是『分对样本的个数』除以『分错样本的个数』：所以，可以求出$d_1$，大于1： 设：$\alpha = \ln d$，则权重更新时乘以$exp(\alpha)$或$exp(-\alpha)$，使表达式更简洁。 AdaBoost的步骤 训练T个弱分类器： 将T个弱分类器进行加权平均:对于每个弱分类器而言，错误率越低，$\alpha$越大，表示这个弱分类器的权重更大。这是合情合理的。 最终分类器的错误率最终的分类器为：$$ H(x) = sign(\sum_{t-1}^T \alpha_t f_t(x) $$ 错误率为：$$ \varepsilon = \frac 1 N \sum \delta(H(x^n) \neq \hat y^n)$$令，$g(x) = \sum_{t-1}^T \alpha_t f_t(x)$那么，$$ \varepsilon = \frac 1 N \sum \delta(\hat y^n g(x^n) &lt; 0) $$ 错误率的上限上式错误率有个上限，如下图：$$ \varepsilon \le \frac 1 N \sum exp(-\hat y^n g(x^n)) $$ 错误率的上限等于样本权重的均值样本的权重，可以通过递推公式求出：将求和操作变换到exp的指数上： 于是，可以得到错误率的上限： 样本权重的和随着迭代次数递减样本权重的和，可以通过递推公式求出：左侧为分错的样本，右侧为分对的样本。 $Z_{t}$可以由$Z_{t-1}$求出： 由于$Z_{1}=N$，那么$Z_{T+1}$为： 也就是， 如下图，$4 x (1-x)$在$x=0.5$时取得最大值1，也就是$2\sqrt{ \epsilon (1-\epsilon)} &lt; 1$ 所以，错误率会随着迭代次数的增加，越来越小。 一个重要特性随着弱分类器的个数的增加，尽管training error已经为0，但是testing error依然能够继续降低。 分析：定义一个Margin，$Margin = \hat y g(x)$，其中$g(x) = \sum_{t=1}^T \alpha_t f_t(x)$. 随着弱分类器个数的增加，Margin随之增加，因此能继续降低testing error。 这个Margin有点类似SVM的效果。 如下图，AdaBoost的training error为绿色的线，当所有的$\hat y g(x) &gt; 0$时，training error为0.但是，AdaBoost的training error有个上限，红色的曲线，随着$\hat y g(x)$增加，这个上限任然继续降低。这就是testing error继续降低的原因。 同时，我们也可以看到SVM与Logistic Regression的区别，因为SVM只根据支持向量来计算Margin，LR则是根据所有样本来计算Margin。 更多资料 Introduction of Adaboost:• Freund; Schapire (1999). “A Short Introduction to Boosting“ Multiclass/Regression• Y. Freund, R. Schapire, “A Decision-Theoretic Generalization of on-LineLearning and an Application to Boosting”, 1995.• Robert E. Schapire and Yoram Singer. Improved boosting algorithms using confidence-rated predictions. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, pages 80–91, 1998. Gentle Boost• Schapire, Robert; Singer, Yoram (1999). “Improved Boosting Algorithms Using Confidence-rated Predictions”. Gradient BoostingBoosting方法的一般表达式每次迭代，我们希望找到一个$f_t(x)$和$\alpha_t$与$g_{t-1}(x)$互补。 我们希望$g(x)$与$\hat y$越接近越好，所以我们定义一下$g(x)$的学习目标：$$L(g)=\sum l(\hat y^n, g(x^n)) = \sum exp(-\hat y^n g(x^n)）) $$ Gradient Boosting如果我们已经得到了$g_{t-1}(x)$，下一步如何更新$g(x)$？使用『梯度下降法』从梯度下降的角度，$g(x)$的更新方式如下：上述『Boosting的一般表达式』中，从Boosting的角度，$g(x)$的更新方式是$g_t(x)=g_{t-1}(x) + \alpha_t f_t(x)$ 两者等价，所以『损失函数对$g(x)$的偏导的反方向』应该与$\alpha_t f_t(x)$的『方向相同』。 我们计算一下上述损失函数对$g(x)$的偏导的反方向： 于是，我们的目标变成了使它们的方向相同：等价于，上述最大化目标函数的式子可以看成，对于每一笔训练样本，都使$\hat y^n$与$f_t(x^n)$同号；并且左侧部分是训练样本的权重。 刚好这里权重的表达式与AdaBoost中权重的表达式一致： 所以，在AdaBoost中，每次迭代过程中产生的弱分类器$f_t(x)$，添加到$g_t(x)$中之后，就像梯度下降一样，会使损失函数$\sum exp(-\hat y^n g(x^n)）$越来越小，使更多的$\hat y^n$与$f_t(x^n)$同号。 上述是对$f_t(x)$的解释，那怎么决定$\alpha_t$呢？固定$f_t(x)$，求损失函数对$\alpha_t$的偏导：上式： 左侧部分为样本的权重： 右侧部分$exp(-\hat y^n \alpha_t f_t(x))$拆成『分对』和『分错』两部分，偏导继续化简为： 令损失函数对$\alpha_t$的偏导=0，可以得到$\alpha_t$的结果与AdaBoost的一致： 小结：从Gradient Boosting的角度考虑Boosting方法的话，我们可以选择不同的目标函数，从而推出不一样的Boosting方法。 Stacking投票对多个分类器的结果进行投票，选择『票数最多』的那个结果作为最终的结果。 Stacking 训练数据分为两部分。 把多个分类器的结果作为一个新的特征。在另外一部分训练数据集上，对这个新的特征，训练一个分类器。 #总结 Bagging：每个子分类器都是强分类器，随机森林通过在样本集和特征集上进行采样，达到抑制过拟合的效果。 随机森林可以用OOB误差来代替交叉验证。 Boosting：每个子分类器都是弱分类器，多个弱分类器组合成一个强分类器。 Bagging的分类器是无序的；Boosting的则是有序的。 AdaBoost算法可以看出是Gradient Boost的特殊形式（其中一种损失函数），每次新生成的一个弱分类器都像梯度下降法一样，添加到最终的分类器上之后，使最终的分类器更强。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>Machine Learning</tag>
        <tag>Ensemble</tag>
        <tag>AdaBoost</tag>
        <tag>Random Forest</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李宏毅机器学习2017学习笔记:10.CNN]]></title>
    <url>%2F2018%2F05%2F01%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02017%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-10-CNN%2F</url>
    <content type="text"><![CDATA[为什么要用CNN 识别一个图像时，我们可能更关注局部特征，那么每一个神经元不需要连接整个图像，只需要连接一个小的区域 同样的特征可能出现在不同的位置 对图像进行下采样，并不会改变图像中的物体 其中，第1、3点能减少网络中的参数个数。 CNN的3个属性 有些pattern只占整张图像的很小一部分（通过卷积实现） 同样的pattern可能出现在不同的位置（通过卷积实现） 下采样不会改变图像中的物体（通过池化实现） 卷积、池化、FlattenCNN的几个主要操作包括：卷积、池化、Flatten。中间可以选择添加Dropout操作。 卷积每一个卷积模板就是一种特征的滤波器，用于探测一种类型的特征。 如下图，卷积模板用于探测『右下斜线』的特征，当图像中出现这种特征的时候，会在卷积结果中体现出来。 每一个二维的Matrix经过一个模板进行卷积之后的结果为一个『Feature Map』。经过多个模板卷积之后，结果为多个『Feature Map』，每一个可以看做一个channel。 当输入为彩色图像时，输入的channel为3。经过k个filter卷积之后，得到k个feature map，相当于k个channel。 1model.add(Conv2D(50, (3, 3), activation='relu')) 池化分Max Pooling、Average Pooling等，一般采用Max Pooling。 后面可以选择地接一个Dropout。12model.add(MaxPooling2D(pool_size=(2, 2)))model.add(Dropout(0.25)) Flatten在卷积和池化之后，将输出的多维Matrix拉成一个一维的向量。如下图，向量维度为50*5*5=1250 在后面接一个全连接的网络(后面可以选择地接一个Dropout)，再在最后接一个softmax网络。1234model.add(Flatten())model.add(Dense(100, activation='relu'))model.add(Dropout(0.5))model.add(Dense(num_classes, activation='softmax')) 卷积层的参数个数与连接数使用Keras实现CNN：123456789101112model = Sequential()model.add(Conv2D(25, kernel_size=(3, 3), activation='relu', input_shape=input_shape))model.add(MaxPooling2D(pool_size=(2, 2)))model.add(Conv2D(50, (3, 3), activation='relu'))model.add(MaxPooling2D(pool_size=(2, 2)))model.add(Dropout(0.25))model.add(Flatten())model.add(Dense(100, activation='relu'))model.add(Dropout(0.5))model.add(Dense(num_classes, activation='softmax')) 如果，卷积模板的大小为h*w，输入通道数为in，输出通道数为out，输出通道的大小为H*W： 注意：这个in可以理解成卷积核的深度depth，这个深度与前一层的feature map的个数一样。卷积模板的大小可以看作三维h*w*depth 那么， 参数个数：#paras = (h*w*in+1)*out（其中h×w为卷积模板参数，1为偏置参数） 连接数：#FLOPS = H*W * #paras 第1个卷积层：（25个卷积模板，大小为33，输入1个通道，输出为25个通道，输出大小为2626） 参数个数：每个filter的参数个数为3*3+1=10（其中3×3为卷积模板参数，1为偏置参数），共有25*10=250个参数 连接数：250*26*26 第2个卷积层：（50个卷积模板，大小为33，输入25个通道，输出为50个通道，输出大小为1111） 参数个数：每个filter的参数个数为3*3*25+1=226，共有226*50=11300个参数 连接数：11300*11*11 CNN从图像中学到了什么 我们前面有个猜想：每个filter能探测图像中特定的pattern。那么，反过来想：输入什么样的图像，经过一个filter卷积之后，会使输出值最大。 最大化卷积的输出（feature map）的『输出值的度量』定义，第k个filter的『输出值的度量』：$$a^k = \sum_{i=1}^{dim} \sum_{j=1}^{dim} a_{ij}^k$$ 我们的目标是寻找最优的输入$x^\ast$，使$a^k$最大：$$x^\ast = arg \max_x a^k$$优化方法：梯度下降法得到。 如下图，12个filter分别得到的结果。可以看到不同的filter能探测不同的局部特征：竖条纹、横条纹、斜条纹等。 最大化全连接网络的输出定义，第j个神经元的输出值：$a^j$目标函数：$$x^\ast = arg \max_x a^j$$ 这个神经元探测的就不是局部特征了，而是全局特征。但是用这种方法做出的效果并不好，图像并不像数字。 最大化softmax的输出目标函数：$$x^\ast = arg \max_x y^i$$如下图，可以看出，尽管图像看着像『花屏』的图像，但是CNN任然认为这是数字。 对目标函数加上约束因为，手写字符的图像中，只有字迹的地方的像素值是大于0的。所以，我们可以对像素值$x_{i,j}$添加约束条件。 如下图，得到的结果比之前的好多了，比如数字6。但是结果还不是很好，可以再加一些约束：如相邻的像素具有相同的颜色等。 CNN的一些应用Deep Dram 给定一张图像，机器会加上他看到的东西。(得到的效果很恶心)步骤： 对于一个已经训练好的CNN，输入一张图片，把某个filter的参数或者全连接层的某个神经元的输出调大； 把调大后的值作为目标，调整输入图像，使输出更接近目标。 Deep Style 一个图像放到CNN中，得到filter的输出，表示图像的content 另一个图像放到CNN中，得到filter之间的correlation，表示图像的style 用同一个CNN，照一张image，同时使content像左边的图像、style像右边的图像 论文链接 A Neural Algorithm of Artistic Style ：https://arxiv.org/abs/1508.06576 下围棋用全连接前馈网络也可以，但是用CNN效果更好。显然地，围棋满足CNN的前两个特点。Alpha GO没使用Max Pooling。 语音处理声谱图：横坐标是时间，纵坐标是频率。颜色表示频率的能量的强度，红色表示该频率的强度很高。下面一个图是『你好』的声谱图，左边是『你』，右边是『好』。使用CNN，对每一段声谱进行分类，卷积移动的方向只沿着频率的方向（从上往下进行卷积）。得到音标，然后连接处理成文本。 文本处理把文本当做图像进行处理，横坐标是句子（各个单词），中坐标是单词的word embedding。这里卷积移动的方向是句子的方向（从左往右）。 拓展阅读可视化CNN The methods of visualization in these slideshttps://blog.keras.io/how-convolutional-neural-networks-see-the-world.html More about visualizationhttp://cs231n.github.io/understanding-cnn/ Very cool CNN visualization toolkithttp://yosinski.com/deepvishttp://scs.ryerson.ca/~aharley/vis/conv/ 自动生成图像 PixelRNNhttps://arxiv.org/abs/1601.06759 Variation Autoencoder(VAE)https://arxiv.org/abs/1312.6114 Generative Adversarial Network(GAN)http://arxiv.org/abs/1406.2661 总结 介绍了什么情况下需要用CNN，也就是CNN解决的三个问题。分别是图像中的特征是局部特征、同样的特征可能出现在不同的地方、下采样不影响图像中的物品。 介绍了CNN的几个常用操作：卷积、池化、Flatten等 通过最大化输出，反推输入。分析CNN从图像中学到了什么。 介绍了CNN的几个应用：Deep Style、Alpha GO、语音处理、文本处理。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>Machine Learning</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李宏毅机器学习2017学习笔记:9.Tips for Training DNN]]></title>
    <url>%2F2018%2F04%2F22%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02017%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-9-Tips-for-Training-DNN%2F</url>
    <content type="text"><![CDATA[引言深度学习的步骤跟机器学习一样，也是分三步。当训练好一个模型之后： 如果在训练数据上表现不好，那么是欠拟合。使用『新的激活函数』、『自适应的学习率』等方法。 如果在训练数据表现很好，在测试数据上表现不好，那么是过拟合。使用『早点停止训练』、『正则化』、『Dropout』等方法。 如果在训练数据、测试数据上表现都很好，那么这是个好的模型 新的激活函数在手写字符识别中，如果使用sigmoid函数作为激活函数，那么随着网络深度的增加，正确率反而会下降。 那是因为当网络深度太大时，会存在『梯度消失』的问题。 当固定learnng rate时，后面的参数变化很快，前面的的参数变化很慢。后面的参数已经收敛时，前面的参数几乎没有变化，还是初始时的随机数。 因为sigmoid函数是将输入值$(-\infty, +\infty)$映射到$(-1,+1)$区间，相当于把输入值压扁了，那么在串联多个sigmoid函数的情况下，当改变输入时，没通过一个sigmoid函数，输出就会衰减一次，对最终的输出影响就会很小。 解决办法： 逐层训练 自适应的learning rate ReLURectified Linear Unit, 修正的线性单元特点： z &gt; 0时，输出z z &lt; 0时，输出0 优势： 计算起来很快 有一定的生物学上原因 无穷多个不同bias的sigmoid函数叠加的结果 能解决『梯度消失』的问题 有些激活函数的输出为0，另一些激活函数的输出等于输入。 去掉输出为0的激活函数，能简化网络结构。 剩下的『输出等于输入』的节点，能避免『梯度消失』的问题 ReLU是线性的，由ReLU组成的网络是线性的吗？ ReLU的变体 Leaky ReLU($\alpha = 0.01$) Parametric ReLU($\alpha$也是学习的参数) Maxout就是以n个节点为一组，每组做MaxPooling，选择最大的一个进到下一层，其他的丢弃。 ReLU是Maxout的特殊形式。当每组2个节点，并且其中一个节点的参数都为0，输出$z2=0$时，就是ReLU了。如果其中一个节点的参数不为0，输出$z2 \neq 0$，那么得到了一个变体的ReLU，就得到了一个可学习的激活函数。 可学习的激活函数： maxout中的激活函数可以是任意的分段线性凸函数 多少段取决于每组中节点的个数 训练的过程中，随着参数的变化，有时候$z1&gt;z2$，有时候$z1&lt;z2$，那么所有的参数都能学习到。 自适应的学习率每个参数的learning rate都不一样。 Adagrad 用固定的learning rate除以『过去所有梯度平方和的平方根』 如果梯度较大，那么learning rate较小；如果梯度较小，那么learning rate较大。 RMSProp训练神经网络的时候，Error的等高线图可能会很复杂，如下图中的月牙形。我们需要，在同一个方向上，不同的地点的learning rate也不一样。 需要更加动态地调整learning rate。RMSProp相对AdaGrad而言，加了个参数，来调整『过去所有梯度』与『当前梯度』的权重。 Yann LeCun说，我们没必要太担心『局部最小值』的问题，因为假设一维参数在某个点存在局部最小值的概率为P，对于一个神经网络而言，有成千上万个参数，在高维空间中，必须是所有参数在这个点都取得极小值，那么这个概率就很小了。 听起来似乎很有道理。但是从现实生活中来看，现实生活中的地面就有很多坑坑洼洼，每个坑出现的概率又不是很低呀。 Momentum(惯性) 一般的梯度下降法的移动方向：梯度方向的反方向 每次移动的方向：上次移动的方向+本次的梯度方向的反方向 如下图，$\lambda$越大，上次移动的方向的权重越大。第i次的移动方向，考虑了前面i-1次的梯度方向，是i-1次梯度的加权和。 AdamRMSPros+Momentum 早点停止训练使用Validation Set来验证，找出什么时候停止训练。 Keras中可以调用EarlyStopping，来实现：123from keras.callbacks import EarlyStoppingearly_stopping = EarlyStopping(monitor='val_loss', patience=2)model.fit(x, y, validation_split=0.2, callbacks=[early_stopping]) Keras文档链接 正则化正则化就是在损失函数上添加参数的约束项，使参数的值尽可能得趋近于0. 在神经网络的训练中，正则化的效果往往不是很明显。梯度下降的作用是不断迭代使参数离0越来越远，正则化的效果是让参数不要离0太远，实际上可能减少迭代次数也能达到同样的效果。 L2正则正则化会是函数变得平滑，所以用正则化并不会改变函数的bias求导，合并同类项。$\lambda$一般取很小的值，如0.01。那么$\eta \cdot \lambda$的值趋近于0，$(1- \eta \cdot \lambda)$的值趋近于0.99。也就是说，每次更新参数w时，都让它先乘以0.99，经过多次迭代，就越来越趋近于0.有对参数『惩罚』的效果。再加上后面的微分项，保证参数w不会全部为0. L1正则 当 $w&gt;0$ 时，减掉一个正数，w往0逼近； 当 $w&gt;0$ 时，减掉一个负数，w还是往0逼近； L1 V.S. L2 L2每次更新参数时，都会乘上0.99；L1每次更新参数时，都会加上/减掉一个固定的值。 假设参数为10000，使用两种正则化方法。使用L2时，乘上0.99，那么一次性减掉了100；使用L1时，减掉一个固定的值0.99。也就是说，L2更能避免参数过大的情况。 假设参数为1，使用两种正则化方法。使用L2时，乘上0.99，那么一次性减掉了0.01；使用L1时，减掉一个固定的值0.99。也就是说，L1更能使参数为0. 所以，使用L2时，能使参数不至于过大，使大部分参数都接近0，但等于0的个数不会太多；使用L1时，会出现比较多的参数比较大的情况，也会出现比较多的参数为0的情况。 生物学解释生物学上，刚出生的人脑神经元的链接不较少；6岁的时候出现大量的神经元链接；14岁的时候链接变少，就像『稀疏参数』一样。 Dropout只对『输入层』和『隐含层』进行Dropout操作。 操作方法 训练阶段：对于每个mini-batch，更新参数时，先对所有神经元进行采样，每个神经元以p的概率被丢弃掉。 测试阶段：使用所有的参数，但是每个参数都要乘以1-p。因为训练时，只保留了1-p部分的神经元；但是测试时，使用的是所有神经元，所以直观地需要将参数乘以1-p（如果激活函数是线性函数）。 就像训练的时候，只用一部分神经元来训练，得到的是多个比较弱的分类器。测试的时候，用所有神经元，就能得到更好的效果。 Dropout是一种ensamble方法Ensemble方法（Bagging）：从一个很大的训练集中采样出多个子集（一部分样本），分别训练出多个模型。 如果每个模型都足够复杂，那么这些模型的bias很小，variance很大。多个模型平均之后，variance就很小了。 Dropout方法是： 用每个mini-batch去训练一个神经网络； 有些参数在网络之间是共享的； 当参数个数为M是，可能存在 $2^M$ 个。如果穷举 $2^M$ 种情况，计算量太大了。 在测试阶段： 『Ensemble』是直接对多个模型的输出求平均 『Dropout』是将参数乘以1-p 如果激活函数是线性函数的话，这种将参数乘以1-p的方法是完全可行的。如下图： 神奇的是，即使激活函数不是线性的，也是work的。 所以，对于激活函数比较接近线性的情况，如ReLU、MaxOut，使用Dropout方法时能得到更好从效果。 也就是说，如果你已经决定了使用了Dropout，那么在选择激活函数时，尽量选择比较接近线性的，如ReLU、MaxOut，而不是sigmoid。 总结 如果在训练数据上表现不好，那么是欠拟合。使用『新的激活函数』、『自适应的学习率』等方法。如果在训练数据表现很好，在测试数据上表现不好，那么是过拟合。使用『早点停止训练』、『正则化』、『Dropout』等方法。 新的激活函数：ReLU，Maxout 自适应的学习率：Adagram、RMSProp、Momentum、Adam 早点停止训练：交叉验证 正则化：L1能使更多的参数为0，L2能使更多的参数接近0 Dropout：一种Enseble方法，训练时参数要乘以1-p]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>Machine Learning</tag>
        <tag>DNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keras安装（CPU版）（macOS 10.13.4）]]></title>
    <url>%2F2018%2F04%2F22%2FKeras%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[安装Miniconda在清华镜像站下载Miniconda安装包，地址：https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/ 我下载的最新版：Miniconda3-latest-MacOSX-x86_64.sh123$ cd ~/Download$ chmod +x Miniconda3-latest-MacOSX-x86_64.sh$ ./Miniconda3-latest-MacOSX-x86_64.sh 根据提示进行安装即可，该脚本会自动将/Users/Neil/miniconda3/bin目录添加到环境变量中。123456$ cat ~/.bash_profile# added by Miniconda3 installerexport PATH="/Users/Neil/miniconda3/bin:$PATH"# 使环境变量生效$ source ~/.bash_profile 注意：如果安装了zsh，上述添加环境变量的操作只会临时生效.当新建shell窗口时，还是需要手动执行source ~/.bash_profile. 解决方法：Stackoverflow Run ps -p $$ at the command line to determine that you are, in fact, using a bash shell. Realize that you are in zsh, which means you should be editing your profile in .zshrc. Copy the offending lines from .bash_profile to .zsh12345678910# 1. 确定运行的是bash shell还是zsh shell$ ps -p $$ PID TTY TIME CMD 1365 ttys007 0:00.39 -zsh# 2. 编辑.zshrc$ vim ~/.zshrc# 3. 将/Users/Neil/miniconda3/bin添加到PATH中$ tail -n 2 ~/.zshrc# added by Miniconda3 installerexport PATH="/Users/Neil/miniconda3/bin:$PATH" 安装keras1$ pip install keras keras默认的深度学习包是TensorFlow，如果要修改成Theano，编辑~/.keras/keras.json1234567$ cat ~/.keras/keras.json&#123; "image_data_format": "channels_last", "floatx": "float32", "epsilon": 1e-07, "backend": "tensorflow"&#125;% 安装TensorFlow1$ pip install tensorflow 验证是否安装成功123456789101112➜ bin pythonPython 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 12:04:33)[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwinType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; import kerasUsing TensorFlow backend.&gt;&gt;&gt; print (keras.__version__)2.1.5&gt;&gt;&gt; import tensorflow as tf&gt;&gt;&gt; print (tf.__version__)1.7.0&gt;&gt;&gt; 下载Keras example123$ git clone https://github.com/fchollet/keras.git$ cd keras/examples/$ python mnist_mlp.py 附Keras中文文档：http://keras-cn.readthedocs.io/en/latest/]]></content>
      <categories>
        <category>Keras</category>
      </categories>
      <tags>
        <tag>Miniconda</tag>
        <tag>Keras</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李宏毅机器学习2017学习笔记:8.Backpropagation]]></title>
    <url>%2F2018%2F04%2F21%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02017%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-8-Backpropagation%2F</url>
    <content type="text"><![CDATA[链式求导法则Case 2: 当改变s时，会通过函数g(s)和h(s)改变x和y，进而通过k(x,y)改变z。 反向传播（Backpropagation）假设损失函数为$L(\theta)$： 计算参数w的偏导：$\partial z / \partial w$，称之为向前传播。 计算激活值z的偏导：$\partial C / \partial z$，称之为向后传播。 向前传播$z = w*x$，因此$\partial z / \partial w = x$ 向后传播假设： 激活函数是sigmoid函数； 只有一个隐含层； 每一层只有2个神经元。 那么： $\partial a / \partial z$，也就是对激活函数求偏导； $\partial z / \partial a = w$，这个很直观； 要求左边的$\partial C / \partial z$，必须先求右边的$\partial C / \partial z’$和$\partial C / \partial z’’$。因此，可以从右往左，依次求解$\partial C / \partial z$。 整理一下，可以得到：其中，$\sigma’(z)=\sigma(z)(1-\sigma(z))$可以轻松求解。 可以把『反向传播』想象成另一个神经元： 输入是：后面的$\partial C / \partial z$ 激活函数是：乘上一个已知的数$\delta’(z)$，类似『放大器』的功能 如果是最后一层:直接求解$\partial C / \partial z$ 如果不是最后一层:依次递归求解，直到最后一层。『向后传播』时，类似一个反向的神经网络。从右往左计算，计算量跟『向前传播』的计算量一样。 小结 在向前传播中，我们求得了$\partial z / \partial w$ 在向后传播中，我们求得了$\partial C / \partial z$ 将上述两者相乘，就得到了参数的微分：$\partial C / \partial w$ 总结 链式求导法则 向前传播求解$\partial z / \partial w$，向后传播求解$\partial C / \partial z$，两者相乘就是参数w的微分 向后传播可以看做一个反向的神经网络，能从右向左依次求出$\partial C / \partial z$]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>Machine Learning</tag>
        <tag>BP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李宏毅机器学习2017学习笔记:7.Brief Introduction of Deep Learning]]></title>
    <url>%2F2018%2F04%2F21%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02017%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-7-Brief-Introduction-of-Deep-Learning%2F</url>
    <content type="text"><![CDATA[深度学习的发展史 深度学习方法Deep Learning 的三个步骤跟机器学习的三个步骤一样： 定义一个模型 评价模型的好坏（损失函数） 求解模型的最优参数备注：function set是指一个模型；一个function是指参数已经确定的模型，给定一个输入，就会有个输出。 定义一个模型最简单的深度学习模型就是『全连接前馈网络』，由一个输入层、多个隐含层、一个输出层组成。 各种DL模型的层数对比： 2012，AlexNet，8层，16.4%的错误率； 2014，VGG，19层，7.3%的错误率； 2014，GoogleNet，6.7%的错误率； 2015，Residual Net，3.57%的错误率。 我们可以把中间隐含层部分看作一个特征提取的模块。最后一个输出层用Softmax。 举个例子：手写数字识别需要设计每层的神经元的个数。 FAQ： Q:选择多少层？每层多少个神经元？A: 根据训练处的误差，凭直觉进行调整 Q:网络结构能自动设置吗？A: 能，例如：Evolutionary Artificial Neural Network Q: 能否设计其他的网络结构？A: 能，例如：CNN 评价模型的好坏（损失函数）『手写字符识别』的例子中，用Cross Entropy作损失函数。 求解模型的最优参数使用Gradient Descent求解最优的参数。 有很多深度学习框架来帮你计算微分：TensorFlow、torch、theano、Caffe、Microsoft CNTK、chainer、DSSTNE、mxnet、libdnn 为什么要用深度学习？ 有些实验表明，网络越深，效果越好。 理论证明，一个隐含层的网络能表示任意函数。理论上，不一定要『深』，也可以『宽』。 总结： 介绍了深度学习的发展史 深度学习的3个步骤，与机器学习的三个步骤一样。以全连接的前馈神经网络为例阐述着三个步骤：建模（定义网络结构）、定义损失函数、求解最优参数。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>Machine Learning</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李宏毅机器学习2017学习笔记:5.Logistics Regression]]></title>
    <url>%2F2018%2F04%2F06%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02017%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-5-Logistic-Regression%2F</url>
    <content type="text"><![CDATA[Logistic 回归 V.S. Linear 回归Logistic 回归 模型：$f_{w.b}(x) = \delta \left( w \cdot x+b \right)$，输出值的范围是：$(0, 1)$ 损失函数：$ L(f) = \sum_{j=1}^n C(f(x_j), \hat y^j) $， 其中$ C(f(x_j), \hat y^j) = -\left[ \hat y^j ln f(x^j) + (1-y^j) ln (1-f(x^j))\right]$ 优化方法：$ w_i = w_i - \eta \sum_{j=1}^n -(\hat y^j - f(x^j)) \cdot x_i $ Linear 回归 模型：$f_{w.b}(x) = w \cdot x+b $，输出值的范围是：$(-\infty, \infty)$ 损失函数：$ L(f) = \frac 1 2 \sum_i (f(x_i) - \hat y^i)^2 $ 优化方法：$ w_i = w_i - \eta \sum_{j=1}^n -(\hat y^j - f(x^j)) \cdot x_i $ 如下图，Logistic 回归与Linear 回归的区别： 模型不一样：前者是sigmoid函数，后者是线性函数。 损失函数不一样，前者一般采用交叉熵，后者使用平方误差。 参数更新方式的数学表达式的形式上一样。 Logistic 回归的损失函数是两个伯努利的交叉熵。交叉熵代表的是$p(x)$与$q(x)$两个分布有多接近（也就是说模型的输出值与label一致）。如果两个分布一样的话，交叉熵的值为0. 如果$\hat y^i = 1$，且$f(x^i) = 1$，那么$H(p,q)=0$. 如果$\hat y^i = 0$，且$f(x^i) = 0$，那么$H(p,q)=0$. Logistic 回归的优化方法记住，$\sigma(z)$对$z$的导数为：$$ \frac {\partial \sigma(z)} {\partial z} = \sigma(z) (1-\sigma(z))$$ 备注：怎么记sigmoid函数的偏导？如下图，sigmoid函数的陡峭程度从左往右先逐渐增加，到x=0的位置最大，然后逐渐减小。所以sigmoid函数的导数是个开口朝下的函数，并且左右对称。当$\delta(z)=0.5$时，sigmoid函数的导数取最大值，也就是说sigmoid函数的导数以x=0.5的坐标轴左右对称。基于以上分析，sigmoid的导数是$\sigma(z) (1-\sigma(z))$的形式。 那么，$ln \sigma(z)$对$z$的导数为：$$ \frac {\partial ln\sigma(z)} {\partial z} = \frac 1 {\sigma(z)} \sigma(z) (1-\sigma(z)) = (1-\sigma(z))$$ 那么，$1 - ln \sigma(z)$对$z$的导数为：$$ \frac {\partial (1-ln\sigma(z))} {\partial z} = - \frac 1 {1 - \sigma(z)} \sigma(z) (1-\sigma(z)) = -\sigma(z)$$ 其中，$z=wx+b$，所以$z$对$w$的偏导为$x$：$ \frac {\partial z} {\partial w} = x$ 可以很容易的得到$-lnL(w,b)$对$w$的偏导：$\hat y$与$f_{w,b}(x)$的差异越大（预测结果与目标越大），梯度越大。 Logistic 回归 + Square Error如果使用平方误差作为Logistic回归的损失函数.损失函数和梯度如下：这个梯度有个问题：当$\hat y = 1$时，无论$f_{w,b}(x)$为1还是0，梯度都为0. 当$f_{w,b}(x)=1$时，$\partial L / \partial w = 0$ 当$f_{w,b}(x)=0$时，$\partial L / \partial w = 0$ 交叉熵 V.S. 平方误差分别使用交叉熵和平方误差作为损失函数，当参数变化时，损失函数的变化如下图所示： 交叉熵比较陡峭，随着参数变化而变化很大；平方误差则比较平坦。 平方误差的微分总是很小，不好优化。当微分值很小时，可能离目标很远，我们需要调大步长；但是实际上可能离目标很近，应该调小步长。 判别式模型 V.S. 产生式模型两种模型的比较两种模型都是sigmoid函数，不同的是参数优化方法不同，对数据分布的假设不同。 判别式模型：Logistic 回归 直接用梯度下降法，迭代求出$w$和$b$。 对数据的分布没有任何假设。 产生式模型：概率高斯模型 先求出两类数据的均值，和共同的方差，然后求出的$w$和$b$。 假设数据的分布服从高斯分布、假设服从伯努利分布、假设属性之间不相关。 这两种方法的模型一样，求解参数的方法不一样，求出的$w$和$b$也不一样。因为两种方法对数据分布的假设不一样。两种方法的正确率也不一样。LR对数据分布假设的依赖很小，它的效果更好。当我们不知道数据的分布情况时，可以试试LR，一般能得到不错的效果。 一般而言，产生式模型的分类效果比产生式模型的更好。举个例子： 类1：有1个样本，x1和x2的取值都为1； 类2：有12个样本，其中，4个样本x1和x2的取值分别为1和0，4个样本x1和x2的取值分别为0和1，4个样本x1和x2的取值都为0. 问：一个x1和x2的取值都为1的样本为哪类？ 求解出来的$P(C_1|x) &lt; 0.5$，属于类2.因为朴素贝叶斯的假设是：x1和x2是不相关的。 所以,在类2中，虽然没有x1和x2都为1的样本，但是x1和x2都为1的概率为1/9 而在类1中，x1和x2都为1的概率为1，远远高于类2中出现x1和x2都为1的概率。 但是，朴素贝叶斯还考虑$P(C_1)$和$P(C_2)$的大小，分别为1/13和12/13，综合考虑的话，这个样本属于类2的概率比类1的概率还高。 有时候，产生式模型的效果更好： 训练数据很少的情况。对训练数据有假设的话，模型能对数据起到补充的效果。 对噪声比较鲁棒。比如上面举的一个例子，类1中的那个样本可能是个噪声。产生式模型能对这个噪声分为类2，但是LR坚持将它分为类1. 先验概率和类条件概率可以通过不同的数据源计算得来。 多分类的问题对于多个类别($K&gt;2$)的情况，我们有$$ P(C_k|x) = \frac {P(x|C_k)P(C_k)} {\sum_i P(x|C_i) P(C_i)} = \frac {exp(z_k)} {\sum_iexp(z_i)} $$它被称为归一化指数（normalized exponential），可以被当做logistic sigmoid函数对于多类情况的推广。其中，$z_k = lnP(x|C_k)P(C_k) $归一化指数也被称为softmax函数，因为它表示max函数的一个平滑版本。对于所有的$i \neq k$都有$z_k &gt;&gt; z_i$，那么$P(C_k|x) \simeq 1$且$P(C_k|i) \simeq 0$。 Logistic回归的局限性LR是线性的，无法解决线性不可分的问题。 特征变换一种解决办法是，进行特征变换。变换后的点，能线性可分。但是实际情况下，很难找到一种合适的变换。 新的特征第一个维度$x_1^{‘}$表示为：点到(0, 0)的距离；新的特征第一个维度$x_2^{‘}$表示为：点到(1, 1)的距离； 级联Logistic 回归 模型级联2层LR模型，第一个LR的作用是“特征变换”，第二个LR的作用是“分类”。第一层的输出值$(x_1^{‘}, x_2^{‘})$线性可分。（四个点分别为(0.73,0.05),(0.27,0.27),(0.27,0.27),(0.05,0.73)）. Deep Learning级联多层LR模型，得到的就是深度神经网络。前面几层的作用是“特征变换”，最后一层的作用是分类。其中一个LR模型，就是一个“神经元”。 总结 对比了一下Logistic回归与线性回归的相同点与不同点。不同点是模型函数不同，输出值的范围不同；相同点是参数的梯度的形式一样。 交叉熵衡量的是两个分布的差异，如果两个分布相同，那么他们的交叉熵的值最小，为0。如果Logistic回归用平方误差作为损失函数的话，损失函数随着参数的变化而变化比较缓慢，不好优化，很难得到理想的结果。 Logistic回归与线性回归两种模型都是sigmoid函数，不同的是参数优化方法不同，对数据分布的假设不同。 分类问题中，一般判别式模型比产生式模型的效果更好。产生式模型能生成样本。 Logistic 回归无法解决线性无可分的问题。可通过“特征变换”和“级联LR”解决线性不可分的问题。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>Logistic Regression</tag>
        <tag>Neural Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[top命令]]></title>
    <url>%2F2018%2F04%2F04%2Ftop%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[信息说明统计信息说明：敲top命令，进入如下视图： 第1行：Top 任务队列信息(系统运行状态及平均负载)，与uptime命令结果相同。 第1段：系统当前时间，例如：10:01:23 第2段：系统运行时间，未重启的时间，时间越长系统越稳定。 格式：up xx days, HH:MM例如：126 days, 14:29, 表示连续运行了126天14小时29分钟 第3段：当前登录用户数，例如：2 user，表示当前只有2个用户登录 第4段：load average 系统负载，即任务队列的平均长度，3个数值分别统计最近1，5，15分钟的系统平均负载。 系统平均负载：单核CPU情况下，0.00 表示没有任何负荷，1.00表示刚好满负荷，超过1侧表示超负荷，理想值是0.7；如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了多核CPU负载：CPU核数 * 理想值0.7 = 理想负荷，例如：4核CPU负载不超过2.8则表示没有出现高负载。 第2行：Tasks 进程相关信息 第1段：进程总数，例如：Tasks: 183 total, 表示总共运行231个进程 第2段：正在运行的进程数，例如：1 running, 第3段：睡眠的进程数，例如：182 sleeping, 第4段：停止的进程数，例如：0 stopped, 第5段：僵尸进程数，例如：0 zombie 第3行：Cpus CPU相关信息，如果是多核CPU，按数字1可显示各核CPU信息，此时1行将转为Cpu核数行，数字1可以来回切换。 第1段：us 用户空间占用CPU百分比，例如：Cpu(s): 6.7%us, 第2段：sy 内核空间占用CPU百分比，例如：0.4%sy, 第3段：ni 用户进程空间内改变过优先级的进程占用CPU百分比，例如：0.0%ni, 第4段：id 空闲CPU百分比，例如：92.9%id, 第5段：wa 等待输入输出的CPU时间百分比，例如：0.0%wa, 第6段：hi CPU服务于硬件中断所耗费的时间总额，例如：0.0%hi, 第7段：si CPU服务软中断所耗费的时间总额，例如：0.0%si, 第8段：st Steal time 虚拟机被hypervisor偷去的CPU时间（如果当前处于一个hypervisor下的vm，实际上hypervisor也是要消耗一部分CPU处理时间的） 第4行：Mem 内存相关信息（Mem: 8306544k total, 7775876k used, 530668k free, 79236k buffers） 第1段：物理内存总量，例如：Mem: 8306544k total, 第2段：使用的物理内存总量，例如：7775876k used, 第3段：空闲内存总量，例如：Mem: 530668k free, 第4段：用作内核缓存的内存量，例如：79236k buffers 第5行：Swap 交换分区相关信息（Swap: 2031608k total, 2556k used, 2029052k free, 4231276k cached） 第1段：交换区总量，例如：Swap: 2031608k total, 第2段：使用的交换区总量，例如：2556k used, 第3段：空闲交换区总量，例如：2029052k free, 第4段：缓冲的交换区总量，4231276k cached windows的内存概念与Linux的不一样，如果按windows的方式此台服务器“危矣”：8G的内存总量只剩下530M的可用内存。Linux的内存管理有其特殊性，复杂点需要一本书来说明，这里只是简单说点和我们传统概念（windows）的不同。 第四行中使用中的内存总量（used）指的是现在系统内核控制的内存数，空闲内存总量（free）是内核还未纳入其管控范围的数量。纳入内核管理的内存不见得都在使用中，还包括过去使用过的现在可以被重复利用的内存，内核并不把这些可被重新使用的内存交还到free中去，因此在linux上free内存会越来越少，但不用为此担心。 如果出于习惯去计算可用内存数，这里有个近似的计算公式：第四行的free + 第四行的buffers + 第五行的cached，按这个公式此台服务器的可用内存：530668+79236+4231276 = 4.7GB。 对于内存监控，在top里我们要时刻监控第五行swap交换分区的used，如果这个数值在不断的变化，说明内核在不断进行内存和swap的数据交换，这是真正的内存不够用了。 进程信息：在top命令中按f按可以查看显示的列信息，按对应字母来开启/关闭列，大写字母表示开启，小写字母表示关闭。带*号的是默认列。 列名 含义 A PID = (Process Id) 进程Id； E USER = (User Name) 进程所有者的用户名； H PR = (Priority) 优先级 I NI = (Nice value) nice值。负值表示高优先级，正值表示低优先级 O VIRT = (Virtual Image (kb)) 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES Q RES = (Resident size (kb)) 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA T SHR = (Shared Mem size (kb)) 共享内存大小，单位kb W S = (Process Status) 进程状态。D=不可中断的睡眠状态,R=运行,S=睡眠,T=跟踪/停止,Z=僵尸进程 K %CPU = (CPU usage) 上次更新到现在的CPU时间占用百分比 N %MEM = (Memory usage (RES)) 进程使用的物理内存百分比 M TIME+ = (CPU Time, hundredths) 进程使用的CPU时间总计，单位1/100秒 b PPID = (Parent Process Pid) 父进程Id c RUSER = (Real user name) d UID = (User Id) 进程所有者的用户id f GROUP = (Group Name) 进程所有者的组名 g TTY = (Controlling Tty) 启动进程的终端名。不是从终端启动的进程则显示为 ? j P = (Last used cpu (SMP)) 最后使用的CPU，仅在多CPU环境下有意义 p SWAP = (Swapped size (kb)) 进程使用的虚拟内存中，被换出的大小，单位kb l TIME = (CPU Time) 进程使用的CPU时间总计，单位秒 r CODE = (Code size (kb)) 可执行代码占用的物理内存大小，单位kb s DATA = (Data+Stack size (kb)) 可执行代码以外的部分(数据段+栈)占用的物理内存大小，单位kb u nFLT = (Page Fault count) 页面错误次数 v nDRT = (Dirty Pages count) 最后一次写入到现在，被修改过的页面数 y WCHAN = (Sleeping in Function) 若该进程在睡眠，则显示睡眠中的系统函数名 z Flags = (Task Flags &lt;sched.h&gt;) 任务标志，参考 sched.h X COMMAND = (Command name/line) 命令名/命令行 交互操作技巧多U多核CPU监控在top基本视图中，按键盘数字“1”，可监控每个逻辑CPU的状况：观察下图【top视图 01】，服务器有16个逻辑CPU，实际上是4个物理CPU。 进程字段排序默认进入top时，各进程是按照CPU的占用量来排序的，在【top视图 01】中进程ID为14210的java进程排在第一（cpu占用100%），进程ID为14183的java进程排在第二（cpu占用12%）。 可通过键盘指令来改变排序字段，比如想监控哪个进程占用MEM最多，我一般的使用方法如下： 敲击键盘“b”（打开/关闭加亮效果），top的视图变化如下：我们发现进程id为10704的“top”进程被加亮了，top进程就是视图第二行显示的唯一的运行态（runing）的那个进程，可以通过敲击“y”键关闭或打开运行态进程的加亮效果。 敲击键盘“x”（打开/关闭排序列的加亮效果），top的视图变化如下：可以看到，top默认的排序列是“%CPU”。 通过”shift + &gt;”或”shift + &lt;”可以向右或左改变排序列，下图是按一次”shift + &gt;”的效果图：视图现在已经按照%MEM来排序了。 改变进程显示字段 敲击“f”键，top进入另一个视图，在这里可以编排基本视图中的显示字段：这里列出了所有可在top基本视图中显示的进程字段，有””并且标注为大写字母的字段是可显示的，没有””并且是小写字母的字段是不显示的。如果要在基本视图中显示“CODE”和“DATA”两个字段，可以通过敲击“r”和“s”键： “回车”返回基本视图，可以看到多了“CODE”和“DATA”两个字段： top命令的补充top命令是Linux上进行系统监控的首选命令，但有时候却达不到我们的要求，比如当前这台服务器，top监控有很大的局限性。这台服务器运行着websphere集群，有两个节点服务，就是【top视图 01】中的老大、老二两个java进程，top命令的监控最小单位是进程，所以看不到我关心的java线程数和客户连接数，而这两个指标是java的web服务非常重要的指标，通常我用ps和netstate两个命令来补充top的不足。 12345# 监控java线程数：$ ps -eLf | grep java | wc -l# 监控网络客户连接数：$ netstat -n | grep tcp | grep 侦听端口 | wc -l 上面两个命令，可改动grep的参数，来达到更细致的监控要求。 在Linux系统“一切都是文件”的思想贯彻指导下，所有进程的运行状态都可以用文件来获取。 系统根目录/proc中，每一个数字子目录的名字都是运行中的进程的PID，进入任一个进程目录，可通过其中文件或目录来观察进程的各项运行指标，例如task目录就是用来描述进程中线程的，因此也可以通过下面的方法获取某进程中运行中的线程数量（PID指的是进程ID）：1$ ls /proc/PID/task | wc -l 在linux中还有一个命令pmap，来输出进程的内存状况，可以用来分析线程堆栈：1$ pmap PID 参考： https://www.linuxidc.com/Linux/2016-08/133871.htm https://blog.csdn.net/dxl342/article/details/53507673]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>top</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李宏毅机器学习2017学习笔记-4.Classification]]></title>
    <url>%2F2018%2F04%2F04%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02017%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-4-Classification%2F</url>
    <content type="text"><![CDATA[引言如何做分类呢？一个理想的方法是： 函数（模型）：找到一个函数$f(x)$，其中内建一个函数$g(x)$，如果$g(x)&gt;0$则为类1，否则为类0； 损失函数：$L(f) = \sum_n \delta(f(x^n) \neq \hat y^n)$，分类正确的个数。 优化方法：感知机、SVM等 产生式模型举个例子有2个盒子，盒子1中有4个蓝1球、1个绿球，盒子2中有2个蓝球、3个绿球。选择盒子1的概率为2/3，选择盒子2的概率为1/3。已知抽到了一个蓝球，问这个蓝球是从哪个盒子里抽出来的。 贝叶斯公式：$$P(A|B) = \frac {P(B|A) * P(A)} {P(B)}$$ 我们可以利用贝叶斯公式，分别算出$P(B_1|Blue)$ 和 $P(B_2|Blue)$的大小，哪个大，蓝球就来自哪个盒子。 $ P(Blue|B_1)P(B_1) = 4/5 * 2/3 = 8 / 15 $ $ P(Blue|B_2)P(B_2) = 2/5 * 1/3 = 2 / 15 $ $ P(B_1|Blue) = 8/15 / (8/15+2/15) = 4 / 5$ 更加普适一点：我们设一个样本来自于类i的概率为$P(C_i)$，在类i中，抽中某个样本的概率为$P(x|C_i)$.给定$x$，它来自哪一类的概率为：$$ P(C_i|x) = \frac {P(x|C_i)P(C_i)} {\sum_{i=1}^n P(x|C_i) P(C_i)}$$那么要求$x$来自哪一类，就看哪个$P(C_i|x)$最大。 产生式模型：我们从训练数据中能得到上图4个红框框中的表达式，用这4个东西就能求出$x$出现的几率，就能产生式$x$:$$ P(x) = P(x|C_1)P(C_1) + P(x|C_2)P(C_2) $$ 求解一下宝可梦属于水系还是一般系训练数据中有79只『水系』的和61只『一般系』宝可梦，其中$ P(C_1) $ 和 $ P(C_2)$很好求出：$ P(C_1) = 79 / (79 + 61) = 0.56 $$ P(C_2) = 61 / (79 + 61) = 0.44 $ 但是，如何求$ P(x|C_1) $ 呢？ 我们选择『防御力』和『特殊防御力』两个属性来描述宝可梦，那么特征的维度为2维。 把79只『水系』的宝可梦画在坐标系中，如下图所示：如上图红色圆圈所示，假设宝可梦的特征值服从高斯分布。求出均值和方差，就能根据$f_{\mu,\Sigma}(x)$ 求出 $P(x|C_i)$。 使用最大似然估计，求$\mu$和$\Sigma$呢？从一个均值为$\mu$、方差为$\Sigma$的高斯分布中取79个样本的概率为：$$ L(\mu, \Sigma) = f_{\mu,\Sigma}(x^1) \cdot f_{\mu,\Sigma}(x^2) \cdot f_{\mu,\Sigma}(x^3)…f_{\mu,\Sigma}(x^{79})$$ 使似然函数取得最大值的解，就是最优解：$ \mu^\ast, \Sigma^\ast = arg \max_{\mu, \Sigma} L(\mu, \Sigma) $最优解就是训练数据的均值和方差：$\mu^\ast = \frac 1 {79} \sum_{n=1}^{79} x^n $$\Sigma^\ast = \frac 1 {79} \sum_{n=1}^{79} (x^n - \mu\ast)(x^n - \mu^\ast)^T $ 『水系』和『一般系』两类样本的分布如下： 根据$P(x|C_1) = f_{\mu^1, \Sigma^1}(x)$，求出$P(C_1|x)$: 如果$P(C_1|x) &gt; 0.5 $，那么$x$属于第一类『水系』。这样，在测试集上的正确率为47%。如下图，蓝色区域的点会被分为『水系』，红色区域的点会被分为『一般系』。 改进方法： 增加特征，将全部7种属性都作为宝可梦的特征，正确率为54%。结果还是不理想。 假设『水系』和『一般系』两类样本的 $\Sigma$ 一样，$\Sigma = (79/140)\Sigma^1 + (61/140)\Sigma^2 $，正确率提升到73%。 比较： 两类不共用同一个$\Sigma$的话，分界面是曲线的。 两类共用同一个$\Sigma$的话，分界面变成了线性的。（这里可引出Logistics Regression） 解决分类问题需要3步使用概率产生式模型进行分类，共3步： 建模； 用似然函数定义参数的好坏； 用极大似然估计来计算最优参数。 朴素贝叶斯假设产生x的每一维特征都是不相关的，那么$P(x|C_1) = P(x_1|C_1) \cdot P(x_2|C_1) … P(x_k|C_1)$. (实际上，特征的各个维度之间是相关的。比如『防御力』比较大的，一般『攻击力』比较小)。 不是所有的情况都使用高斯分布，如果某个特征是二值的，用伯努利分布比较好！ 后验概率令$ z = ln \frac {P(x|C_1) P(C_1)} {P(x|C_2) P(C_2)}$，类$C_1$的后验概率可以写成$$ P(C_1|x) = \frac {P(C_1|x)P(c_1)} {P(C_1|x)P(c_1) + P(C_2|x)P(c_2)} = \frac 1 {1+exp(-z)} = \delta(z)$$“sigmoid”的意思是“S形”。这种函数有时被称为“挤压函数”，因为它把整个实数轴映射到了一个有限的区间中。 它满足围绕点(0, 0.5)中心对称，$\delta(-z) = 1- \delta(z)$ $\delta(z)$是logistic sigmoid函数，定义为$$\delta(z) = \frac 1 {1+exp(-z)}$$ logistic sigmoid的反函数为logit函数：$$ z = ln (\frac \delta {1-\delta})$$它表示两类概率比值的对数，也被称为log odds。 odds，中文翻译为“胜算”、“发生比”。$odds = \frac {P(x|C_1) P(C_1)} {P(x|C_2) P(C_2)} = \frac p {1-p}$ 用$exp(-z)$ 表示 $\frac {P(x|C_1) P(C_1)} {P(x|C_2) P(C_2)}$，因为 $p/(1-p)$的取值范围：$[0, + \infty)$。($p$可以取零) $ln (p/(1-p))$的取值范围：$(-\infty, + \infty)$。($p$不可以取零) $z=wx+b$的取值范围：$(-\infty, + \infty)$ $exp(-z)$的取值范围都是$(0, + \infty)$. 求解z过程如下： $z$与$x$线性相关得到$z$与$x$是线性相关的：$z=w^T \cdot x + b$进而得到：$P(C_1|x) = \delta(w \cdot x + b)$，其中$\delta$是sigmoid函数。也就是说，基于概率高斯模型（多元高斯混合模型）的假设可以推导出后验概率等于$x$的线性表达的sigmoid形式。 总结： 通过一个从两个盒子中取篮球和绿球的例子，引出产生式模型。 产生式模型中，我们对类条件概率密度$P(x|C_i)$和类先验概率分布$P(C_i)$建模，然后使用这两个概率密度通过贝叶斯定理计算后验概率密度$P(C_i|x)$。 朴素贝叶斯分类器的假设是，$x$的特征的各个维度是不相关的。 在产生式模型的基础上，假设数据服从高斯分布。后验概率能表示为一个sigmoid函数：$P(C_i|x)=\delta(z)$，且这个$z$又与$x$线性相关。所以，$P(C_i|x) = \delta(w \cdot x + b)$。(Logistic回归也是一个sigmoid函数的形式，但是它并没有假设数据服从高斯分布)]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>产生式模型</tag>
        <tag>贝叶斯</tag>
        <tag>概率高斯模型</tag>
        <tag>高斯混合模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vi常用命令]]></title>
    <url>%2F2018%2F04%2F02%2Fvi%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[vi有3个模式：插入模式、命令模式、低行模式。 插入模式：在此模式下可以输入字符，按ESC将回到命令模式。 命令模式：可以移动光标、删除字符等。 低行模式：可以保存文件、退出vi、设置vi、查找等功能(低行模式也可以看作是命令模式里的)。 打开文件、保存、关闭文件(vi命令模式下使用)vi filename // 打开filename文件:w // 保存文件:w filename // 保存至filename文件:q // 退出编辑器，如果文件已修改请使用下面的命令:q! // 退出编辑器，且不保存:wq // 退出编辑器，且保存文件 插入文本或行(vi命令模式下使用，执行下面命令后将进入插入模式，按ESC键可退出插入模式) a // 在当前光标位置的 右边 添加文本i // 在当前光标位置的 左边 添加文本A // 在当前行的 行尾 添加文本I // 在当前行的 行尾 添加文本(非空字符的行首)O // 在当前行的 上面新建一行o // 在当前行的 下面新建一行R // 替换(覆盖)当前光标位置及后面的若干文本J // 合并光标所在行及下一行为一行 移动光标(vi命令模式下使用)1、使用上下左右方向键 2、命令模式下： h（向左）、j（向下）、k（向上）、l（向右） 空格键（向右）、Backspace（向左）、Enter（移动到下一行首）、-（移动到上一行首） 删除、恢复字符或行(vi命令模式下使用)x // 删除当前字符nx // 删除从光标开始的n个字符dd // 删除当前行ndd // 向下删除当前行在内的n行u // 【撤销】 撤销上一步操作U // 撤销对当前行的所有操作 搜索(vi命令模式下使用)/vpser // 向光标下搜索vpser字符串?vpser // 向光标上搜索vpser字符串n // 向下搜索前一个搜素动作N // 向上搜索前一个搜索动作 跳至指定行(vi命令模式下使用)n+ // 向下跳n行n- // 向上跳n行nG // 跳到行号为n的行G // 跳至文件的底部 设置行号(vi命令模式下使用):set nu // 显示行号:set nonu // 取消显示行号 复制、粘贴(vi命令模式下使用)yy // 【复制】 将当前行复制到缓存区，也可以用“ayy” 复制，”a”为缓冲区，”a”也可以替换为a到z的任意字母，可以完成多个复制任务。nyy // 将当前行向下n行复制到缓冲区，也可以用 ”anyy”复制，”a”为缓冲区，a也可以替换为a到z的任意字母，可以完成多个复制任务。yw // 复制从光标开始到词尾的字符。nyw // 复制从光标开始的n个单词。y^ // 复制从光标到行首的内容。y$ // 复制从光标到行尾的内容。p // 【粘贴】 粘贴剪切板里的内容在光标后，如果使用了前面的自定义缓冲区，建议使用”ap”进行粘贴。P // 粘贴剪切板里的内容在光标前，如果使用了前面的自定义缓冲区，建议使用”aP”进行粘贴。 替换(vi命令模式下使用):s/old/new // 用new替换行中首次出现的old:s/old/new/g // 用new替换行中所有的old:n,m s/old/new/g // 用new替换从n到m行里所有的old:%s/old/new/g // 用new替换当前文件里所有的old 编辑其他文件:e otherfilename // 编辑文件名为otherfilename的文件 修改文件格式:set fileformat=unix // 将文件修改为unix格式，如win下面的文本文件在linux下会出现^M。 Tips： 当不知道自己处在什么模式时可以按2次Esc键即可回到命令模式。 附Word版，方便下载打印出来。 点此下载：Vi常用命令.docx 本文转载自：VPS侦探 本文链接地址：https://www.vpser.net/manage/vi.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>vi</tag>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李宏毅机器学习2017学习笔记:3-1.Gradient Descent]]></title>
    <url>%2F2018%2F03%2F28%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02017%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-3-1-Gradient-Descent%2F</url>
    <content type="text"><![CDATA[对于以下优化问题，我们需要找到最优的参数$\theta ^ \ast$，使得损失函数最小：$$ \theta ^ \ast = arg \min_{\theta} L(\theta) $$ 使用梯度下降的方法进行优化。 梯度是损失函数的等高线的法线方向，每次更新参数时，往梯度方向的反方向走。$$ \theta^{t+1} = \theta^t - \eta * \nabla L(\theta^t)$$ 梯度下降的几个小技巧调整learning rate将Loss的变化趋势可视化出来。 如果learning rate比较小，那么Loss降低得很慢； 如果learning rate比较大，那么Loss会出现在极小值附近震荡的现象； 如果learning rate太大，那么Loss会剧烈变化，飞出去了； 自动调整learning rate 流行且简单的方法是，逐渐减小learning rate 刚开始的时候，离最优解比较远，采用比较大的learning rate 随着迭代次数增加，离最优解越来越紧，我们慢慢减小learning rate 例如：$ \eta^t = \eta / {\sqrt{t+1}}$ 每个不同的参数，采用不同的learning rate Adagrad每个参数的learning rate都除上过去所有微分值的均方根（root mean square of its previous derivatives） 普通的Gradient Descent：（对于每个参数）$$ w^{t+1} = w^t - \eta^t g^t $$Adagrad:（对于每个参数）$$ w^{t+1} = w^t - \frac {\eta^t} {\delta^t} g^t $$其中: $ \eta^t = \frac \eta {\sqrt{t+1}} $ $ g^t = \frac {\partial L(\theta ^ t)} {\partial w} $ ${\delta^t}$是过去所有微分值的均方根。 消掉$ {\sqrt{t+1}} $之后：$$ w^{t+1} = w^t - \frac \eta {\sqrt{\sum_{i=0}^t (g^i)^2}} g^t $$再比较一下普通的梯度下降与Adagrad：普通的Gradient Descent（对于每个参数）：$$ w^{t+1} = w^t - \eta^t g^t = w^t - \eta \frac {g^t} {\sqrt{t+1}}$$learning rate随着t的增大而减小，$g$越大时，参数变化得越大。 Adagrad（对于每个参数）：$$w^{t+1} = w^t - \frac \eta {\sqrt{\sum_{i=0}^t (g^i)^2}} g^t= w^t - \eta \frac {g^t} {\sqrt{\sum_{i=0}^t (g^i)^2}}$$learning rate与过去所有的微分有关，$g$越大时，如果过去所有的$g$也很大，那么参数变化得不大；如果过去所有的$g$很小，那么参数变化得就很大。 Adagrad每次更新参数时，不仅考虑了当前的梯度$g^t$，而且还考虑了过去所有的$g$，即$\sqrt{\sum_{i=0}^t (g^i)^2}$。 这个$\sqrt{\sum_{i=0}^t (g^i)^2}$ 能造成反差的效果。如下图： TODO：添加解释用过去所有微分值的均方根来替代二次微分。 随机梯度下降(Stochastic) 普通的梯度下降（所有的样本）： 损失函数：$ L = \sum_{i=1}^n \left( \hat y^i - w*x \right)^2$ 参数更新：$ \theta^{t+1} = \theta^t - \eta \nabla L(\theta^t) $ 看完了所有的样本，才更新参数。 随机梯度下降（选择一个样本）： 损失函数：$ L^i = \left( \hat y^i - w*x \right)^2$ 参数更新：$ \theta^{t+1} = \theta^t - \eta \nabla L^i(\theta^t) $ 没看完一个样本，都更新一次参数。虽然每次用一个样本更新参数的效果不如普通的梯度下降，但是所有样本更新完之后，效果就比普通的梯度下降方法更好了。 特征归一化(Feature Scaling)把特征的各个维度进行归一化，使各个特征的取值范围一致。具体做法： 取第i维特征$x_i$； 求均值$m_i$、标准差$\delta_i$； 对第r样本，更新数值$ x_i^r = \frac {x_i^r - m_i} \delta_i $ 梯度下降的理论基础Formal Derivation（形式误差）给定一个起始点 $\theta^0$，我可以在其一定范围内找到一个最小的点，并更新为 $\theta^1$。如此迭代下去，知道找到最优解。 Taylor Series（泰勒级数）定理：如果 $h(x)$ 在 $x_0$ 附近是无限可微的，那么 $h(x)$ 可以展开成：$$h(x) = \sum_{k=0}^\infty \frac {h^k({x_0})} {k!} (x-{x_0})^k= h(x_0) + h’(x_0)(x-x_0) + \frac {h’’(x_0)} {2!} (x-x_0)^2 + …$$当 $x$ 很接近 $x_0$ 时，$ h(x) \approx h(x_0) + h’(x_0)(x-x_0) $ 多个参数的泰勒展开：$$h(x,y) = h(x_0,y_0) + \frac {\partial h(x_0,y_0)} {\partial x} (x - x_0) + \frac {\partial h(x_0,y_0)} {\partial y} (y - y_0) + …$$当 $(x,y)$ 很接近 $(x_0,y_0)$ 时，$ h(x,y) \approx h(x_0,y_0) + \frac {\partial h(x_0,y_0)} {\partial x} (x - x_0) + \frac {\partial h(x_0,y_0)} {\partial y} (y - y_0) $ 梯度下降法的由来如图，假设红色圆圈(半径d)很小很小，那么损失函数在红色圆圈内满足：$L(\theta) \approx s + u(\theta_1 - a) + v(\theta_2 - b)$ 问题转换为：在红色圆圈内，求损失函数的最小值。 如下图，损失函数的各个部分中，$s, u, v$是固定的，我们要求的是 $\Delta \theta_1$ 和 $\Delta \theta_2$ .从图中，很容易看出，当 $(\Delta \theta_1$ , $\Delta \theta_2)$ 与 $(u, v)$的方向正好相反时，$L(\theta)$最小。 也就是说 $\theta^{t-1} - \theta^t$ 的结果是 $(u, v)$ 的反方向。如下图，转换一下之后，就得到了梯度下降的公式。 Note：如果把泰勒级数的二次微分考虑进来的话，就是牛顿法了。 梯度下降法的局限性容易卡在局部最小值点和鞍点，他们的共同点都是微分值为0的点。计算过程中，当微分值特别小的时候，我们可能会认为这是到了局部最小值，就提前结束循环。实际上，我们可能只是遇到了鞍点。 总结： 梯度下降法的三个tip： 使用Adagrad动态调整learning rate 随机梯度下降 特征归一化 通过泰勒级数推出梯度下降的方法。 梯度下降法的局限性：容易卡在局部最小值点和鞍点。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>Machine Learning</tag>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李宏毅机器学习2017学习笔记:2.Bias and Variance]]></title>
    <url>%2F2018%2F03%2F26%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02017%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2-Bias-and-Variance%2F</url>
    <content type="text"><![CDATA[Error来自哪里？ 来自于bias 来自于variance Estimator(估计量)在估计宝可梦的CP值的例子中，正确的函数为 $\hat f$ ，这个我们无法知道。 我们只能从训练数据中学到一个最好的函数 $f^\ast$ .所以，$f^\ast$ 是 $\hat f$的一个estimator。 『估计量』 的 Bia 和 Variance假设有一个变量$x$，满足：$x$的均值为$\mu$，均方差为$\sigma^2$。 如何估计均值$\mu$呢？取N个点：${x^1, x^2,…,x^N}$N个点取均值，结果不会等于$\mu$，当N无限大时，均值会无限接近$\mu$：$$ m = \frac 1 N \sum_n x^n \neq \mu $$虽然每个$m$与$\mu$都不相等，但是$m$的期望等于均值$\mu$。所以用$m$来估计$\mu$，是无偏的。$$ E[m] = E \left[\frac 1 N \sum_n x^n \right] = \frac 1 N \sum_n E[x^n] = \mu $$ 就像打靶的时候，瞄的点是$\mu$，但是由于风、肌肉抖动等的影响，实际打中的地方会散布在瞄的$\mu$的周围。 散布得多散，取决于$m$的方差：$$ Var(m) = \frac {\sigma^2} N $$ $m$的方差取决于样本的数量： 当N比较小时，散布比较开； 当N比较大时，散布比较紧。 如何估计均方差$\sigma^2$呢？$s^2$表示：$$ s^2 = \frac 1 N \sum_n (x^n - m)^2 $$用$s^2$来估计均方差，是有偏的：$$ E[s^2] = \frac {N-1} N \sigma^2 \neq \sigma^2 $$当N很大时，$s^2$的期望会很接近$\sigma^2$. 小结我们的目标是估测靶的中心$\hat f$，对N组数据分别估测N个$f ^\ast$. 每个$f^\ast$与$\hat f$之间存在误差，这个误差来自于%E[f\ast]%与$\hat f$的bias; 另外一个误差来自于$f^\ast$与$\overline f$的variance. Note：如何得到多个$f^\ast$呢？在不同的数据集上估计$f$。 举例：分别用以下3种模型，学习100次，得到多个$f^\ast$，画图如下：从图中可以看到： 从上到下，模型的复杂程度越来越高； 简单的模型散步得很紧密，复杂的模型散布得比较开； 简单的模型受到数据(x)的影响较小（最极端的例子$f(x)=c$，最简单的模型，完全不受数据影响）。 对5000个 $f^\ast$ 求平均，画出来的蓝色线如下图：比较bias： 左边的模型比较简单，求平均之后离 $\hat f$ 较远，bias较大； 右边的模型比较复杂，求平均之后离 $\hat f$ 较近，bias较小。 比较variance： 左边的模型输出值均在 $\hat f$ 附近，variance较小； 右边的模型输出值散布较开，variance较大。 如何处理bias偏大的情况？分析方法： 如果模型不能很好地拟合训练数据，说明bias很大。【欠拟合】 如果模型能拟合训练数据，但是在测试数据上误差较大，那么很可能variance很大。【过拟合】 如果是bais很大，那么需要使模型更加复杂： 加更多的特征 用更加复杂的模型 如果是variance很大，那么： 收集更大的模型。如下图，100个样本训练的模型比10个样本训练的模型散布更加紧凑。 正则化。如下图，从左往右，正则项系数逐渐增大。 如何选择模型？【交叉验证】作业提供了一个训练集和公开的测试集，提交作业的时候，需要在私有的测试集上对提交的结果进行测试。 如果在训练集上训练了3个模型，在测试集上，『Model 3』 的 Error=0.5 最小，于是将『Model 3』的结果提交上去，结果Error&gt;0.5。怎么办？ N折交叉验证使用『N折交叉验证』来选择最优的模型，过程如下。 将训练集分成N份； 对于每个模型而言，依次将第i份拿出来作验证，在其他的N-1数据上训练，得到N个训练误差。求平均，得到平均训练误差； 对于多个训练模型而言，取平均训练误差最小的模型。 总结 如果在训练集上误差较大，那么bias较大，说明是欠拟合。考虑增加特征，或者换更复杂的模型； 如果在训练集上误差较小，在测试集上误差较大，过拟合。考虑收集更多的数据，或者采用正则化。 多个候选模型之间如何选择？采用N折交叉验证。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>Machine Learning</tag>
        <tag>Bias</tag>
        <tag>Variance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李宏毅机器学习2017学习笔记:1.Regression Demo]]></title>
    <url>%2F2018%2F03%2F26%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02017%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-Regression-Demo%2F</url>
    <content type="text"><![CDATA[视频简介视频中先固定learning rate，迭代10w次： 首先，设置了一个比较小的learning rate，lr=1e-6，没有得到最优解，就停止了； 然后，放大lrarning rate，lr=1e-5，结果出现震荡的情况，无法得到最优解。 最后，采用AdaGrad方法调整learning rate，得到了最优解。 固定learning rate123456789lr = 0.000001for i in range(iteration): # calculate gradient b_grad = ... w_grad = ... # update weight b = b - lr * b_grad w = w - lr * w_ grad 动态调整learning rate视频中使用的方法是：AdaGrad 123456789101112131415lr = 1lr_b = 0lr_w = 0for i in range(iteration): # calculate gradient b_grad = ... w_grad = ... # update lr_b, lr_w with AdaGrad lr_b = lr_b + b_grad ** 2 lr_w = lr_w + w_grad ** 2 # update weight b = b - lr/np.sqrt(lr_b) * b_grad w = w - lr/np.sqrt(lr_w) * w_ grad]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>Machine Learning</tag>
        <tag>Regression</tag>
        <tag>调参</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李宏毅机器学习2017学习笔记:1.Regression]]></title>
    <url>%2F2018%2F03%2F24%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02017%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-Regression%2F</url>
    <content type="text"><![CDATA[构建最基本的回归模型1.问题描述：如何根据宝可梦的CP值预测进化后的CP值？ 2.模型假设：$$y = b + w * X_{cp}$$ 为方便表示和计算，用$w_0$替代$b$，只需要在$X_{cp}$前面添加一维数值1，模型变为：$$y = w * X_{cp}$$ 3.损失函数：$$L(f) = L(w) = \frac12 \sum_{i=1}^{10} \left(\hat {y}^i - w · x_{cp}^i\right) ^2$$ 4.优化方法（梯度下降法）：目标函数：$$ f^\ast = arg \min_{f}^{} L(f) $$代入模型方程：$$ w^\ast= arg \min_{w} \frac12 \sum_{i=1}^{10}\left(\hat {y}^i - w · x_{cp}^i\right)^2$$ 求梯度:$$ \frac {\partial L} {\partial w} = \sum_{i=1}^{10} \left( \hat {y}^i - w · x_{cp}^i\right) (-x_{cp}^i)$$参数更新:$$w_{t+1} = w_t - \alpha · \frac {\partial L} {\partial w}$$其中，$\alpha$为步长。 5.结果：在测试集上的平均误差为35. 对回归模型进行优化选择更加复杂的模型 增加一维特征$(X_{cp})^2$$$y = b + w_1·X_{cp} + w_2 · (X_{cp})^2$$在训练集上和测试集上的平均误差分别15.4和18.4 再增加一维特征$(X_{cp})^3$在训练集上和测试集上的平均误差分别15.3和18.1 再增加一维特征$(X_{cp})^4$在训练集上和测试集上的平均误差分别14.9和28.8 再增加一维特征$(X_{cp})^5$在训练集上和测试集上的平均误差分别12.8和232.1 上述实验，可以看出： 更复杂的模型，可以得到更小的训练误差； 更复杂的模型，可能导致过拟合，在测试集上表现不好。 解决办法：收集更多的数据； 收集更多的特征后发现，下图中，同一个x，对应两个不同的y，所以推测存在隐藏的因子（宝可梦的种类）。 加上种类属性，重新建模：也就是说，每一类宝可梦分别对应一个回归模型.在训练集和测试集上的误差分别为3.8和14.3 继续猜测，可能还有隐藏因子，例如『高度』、『体重』等。验证一下，过拟合了！ 正则化（Regularization）：以上猜测隐藏因子的方法不一定猜得到，正则化一般来说是有用的。 在损失函数后面添加一项对参数的约束：$$L(f) = L(w) = \frac12 \sum_{i=1}^{10} \left(\hat {y}^i - w · x_{cp}^i\right) ^2 + \lambda \sum(w_i)^2$$ $w_i$很小，意味着函数比较平滑。 什么是平滑的函数？当输入变化很大时，输出的变化不大，则这个函数是平滑的。如下图，当左右两边都加上$\delta x_i$时，如果$w_i$比较小，那么$y$的变化也会比较小。 需要给bias加regularization?不需要，因为bias只影响函数上下移动，并不影响函数的平滑度。 小结： 宝可梦进化后的CP值与进化前的CP值和宝可梦的类型有关，也可能存在其他的隐藏因子； 梯度下降方法； 我们最终在测试集上得到的误差为11.1，那么这个模型应用于新的数据集、更大的数据集上，误差会变大还是变小？ 下一节：误差来自哪里？]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>李宏毅</tag>
        <tag>Machine Learning</tag>
        <tag>Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[异地同步博客工程源码]]></title>
    <url>%2F2018%2F03%2F19%2F%E5%BC%82%E5%9C%B0%E5%90%8C%E6%AD%A5%E5%8D%9A%E5%AE%A2%E5%B7%A5%E7%A8%8B%E6%BA%90%E7%A0%81%2F</url>
    <content type="text"><![CDATA[hexo主目录结构12345678|-- _config.yml |-- package.json |-- scaffolds|-- source |-- _posts |-- _drafts |-- themes|-- .gitignore _config.yml，全局配置文件。，网站的很多信息都在这里配置，诸如网站名称，副标题，描述，作者，语言，主题，部署等等参数。 package.json，框架的参数和依赖插件 scaffolds，是“脚手架、骨架”的意思，当你新建一篇文章（hexo new ‘title’）的时候，hexo是根据这个目录下的文件进行构建的。 source目录，博客文件存储的目录。其中，_posts是发布出来的博客的目录，_drafts是草稿存储的目录。 themes，存储主题的目录。 安装其他插件之后，会有其他的目录或文件：1|-- .deploy_git .deploy_git，hexo-deploy-git插件自动生成的目录 db.json，NexT主题生成的文件 需要同步的文件和目录需要同步的文件和目录包括： _config_yml package.json source themes scaffolds 剩下不需要同步的文件，添加到.gitignore文件中：1234567.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/ 使用github进行异地同步本机上传源码我们在xxx.github.io这个repo下建一个source分支，来保存博客源码。 新建source分支 1$ git checkout -b source 上传博客文件 123$ git add --all$ git commit -m "init blog data"$ git push origin source 上传第三方主题文件因为第三方主题文件是从github上clone下来的一个完整的repo，那么在MyBlog目录下是无法直接将主题文件上传到自建的reop上的。如果执行git add ./themes/next/*会报fatal: Pathspec ‘themes/next/_config.yml’ is in submodule的错误。 如果以后不想更新主题，最简单的方式是先清空next主题目录下的.git文件夹，然后清空缓存，最后提交主题文件。12345$ rm -rf ./themes/next/.git$ git rm -rf --cached ./themes/next/$ git commit -m &quot;add theme NexT&quot;$ git push origin master 如果想以后更新主题，那么在添加主题的时候，不能直接git clone第三方主题。 Fork该第三方主题仓库，这样就会在自己账号下生成一个同名的仓库，并对应一个url，我们应该git clone自己账号下的url。 这样做的原因是：我们很有可能在原来主题基础上做一些自定义的小改动，为了保持多终端的同步，我们需要将这些改动提交到远程仓库。而第三方仓库我们是无法直接push的。 这样就会出现git仓库的嵌套问题，我们通过git submodule来解决这个问题.下面这条命令会在themes/next目录下clone自己账号下的hexo-theme-next1$ git submodule add git@github.com:xxx/hexo-theme-next.git themes/next 查看submodule列表：git submodule补充一个删除submodule的方法： git submodule deinit themes/next 将第三方主题提交到source分支：123$ git add ./themes/*$ git commit -m 'add theme next and hueman'$ git push origin source 远程repo的source分支中并不会添加第三方主题的源码，而是类似指针一样，在themes目录下多两个（添加了2个第三方主题）链接到其他repo的文件夹： 修改了第三方主题文件之后，需要切换到第三方主题的目录下提交。1234$ cd ./themes/next$ git add _config.yml$ git commit -m 'config theme next'$ git push origin master 另一台机器下载 安装nodejs和git环境 从source分支clone博客源码无法clone第三方主题文件。 1$ git clone -b source https://github.com/xxx/MyBlogData.git MyBlog 安装hexo相关插件 12$ npm install# 安装package.json文件中配置的依赖包 clone第三方主题第三方主题在submodule里，需要执行下面语句将第三方主题文件clone下来。 12345$ git submodule initSubmodule 'themes/hueman' (git@github.com:xxx/hexo-theme-hueman.git) registered for path 'themes/hueman'Submodule 'themes/next' (git@github.com:xxx/hexo-theme-next.git) registered for path 'themes/next'$ git submodule update 我们修改的第三方主题文件提交到master分支了，需要切换到master分支将最新的代码pull下来。12345678910$ cd ./themes/next$ git branch* (HEAD detached at 9f7f5ae) master$ git checkout master$ git branch* master$ git pull 总结 将源码push在source分支 使用submodule解决第三方主题的问题 修改第三方主题文件的时候，需要切换到第三方主题的目录 参考：关于博客同步的解决办法: https://devtian.me/2015/03/17/blog-sync-solution/]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>Hexo</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Titanic Data Science Solution(翻译)]]></title>
    <url>%2F2018%2F03%2F18%2FTitanic-Data-Science-Solution-%E7%BF%BB%E8%AF%91%2F</url>
    <content type="text"><![CDATA[原文链接：Titanic Data Science Solution 工作流kaggle比赛工作流包含7个阶段： 理解问题； 获取训练数据和测试数据； 数据清理； 分析、确定特征； 建模、训练、预测； 可视化、报告、提出解决问题的步骤和最终的方案； 提交结果。 理解问题仔细审题，理解是分类问题还是回归问题，或者其它。 获取数据123train_df = pd.read_csv('../input/train.csv')test_df = pd.read_csv('../input/test.csv')combine = [train_df, test_df] 数据清理pandas包含一些获取数据描述的方法。 数据有哪些特征？1print(train_df.columns.values) [‘PassengerId’ ‘Survived’ ‘Pclass’ ‘Name’ ‘Sex’ ‘Age’ ‘SibSp’‘Parch’ ‘Ticket’ ‘Fare’ ‘Cabin’ ‘Embarked’] 预览数据：1train_df.head() 哪些特征是分类的？有些特征能将数据集分成多个子集。例如性别。可以对这些特征进行可视化，分析数据的分布。 分类的：Survived（是否幸存），Sex（性别），Embarked（登船口） 序列的：Pclass（舱位等级） 哪些特征是数值的？这些数值特征是离散的、连续的、还是时间序列的？ 连续的：Age（年龄），Fare（票价） 离散的：SibSp，Parch 哪些特征的数据类型是混乱的？有些特征的数据类型既有数字的，也有字母的，这些特征在数据清理环节需要被处理。 Ticket：数字和字母混合的 Cabin（船舱）：字母的 哪些特征包含错误数据？对于大型数据集来说比较困难，但是从较小的数据集中查看一些示例可能得出哪些特性需要改正。 “Name”这个特征可能包含错误数据，因为有很多种方式来描述一个人的名字，如简称，名字字符串也可能附有圆括号或引号 哪些特征包含空值、null、NaN等？包含空值的特征，是具体情况采用不同的方式进行填补。 这三个特征包含空值：Cabin &gt; Age &gt; Embarked Carbin和Age在测试集中不完整 各个特征的数据类型是什么？123train_df.info()print("_" * 50)test_df.info() Output：12345678910111213141516171819202122232425262728293031323334&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;RangeIndex: 891 entries, 0 to 890Data columns (total 12 columns):PassengerId 891 non-null int64Survived 891 non-null int64Pclass 891 non-null int64Name 891 non-null objectSex 891 non-null objectAge 714 non-null float64SibSp 891 non-null int64Parch 891 non-null int64Ticket 891 non-null objectFare 891 non-null float64Cabin 204 non-null objectEmbarked 889 non-null objectdtypes: float64(2), int64(5), object(5)memory usage: 83.6+ KB________________________________________&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;RangeIndex: 418 entries, 0 to 417Data columns (total 11 columns):PassengerId 418 non-null int64Pclass 418 non-null int64Name 418 non-null objectSex 418 non-null objectAge 332 non-null float64SibSp 418 non-null int64Parch 418 non-null int64Ticket 418 non-null objectFare 417 non-null float64Cabin 91 non-null objectEmbarked 418 non-null objectdtypes: float64(2), int64(4), object(5)memory usage: 36.0+ KB 数值特征的分布是什么？这一步在早期分析中，有助于我们充分理解数据。 样本总数为891，占实际所有人数2224的40% “Survived”是一种具有0或1值的分类特征 样本中的幸存率大约是38%，实际的幸存率为32% 大多数乘客（超过75%）没有和父母或孩子一起旅行 近三成的乘客有兄弟姐妹 和/或 配偶 票价差异很大，很少有乘客（&lt;1%）支付高达512美元的费用 年龄在65-80岁之间的老人很少（&lt;1%） 123456train_df.describe()# Review survived rate using `percentiles=[.61, .62]` # knowing our problem description mentions 38% survival rate.# Review Parch distribution using `percentiles=[.75, .8]`# SibSp distribution `[.68, .69]`# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]` Output: PassengerId Survived Pclass Age SibSp Parch Fare count 891.000000 891.000000 891.000000 714.000000 891.000000 891.000000 891.000000 mean 446.000000 0.383838 2.308642 29.699118 0.523008 0.381594 32.204208 std 257.353842 0.486592 0.836071 14.526497 1.102743 0.806057 49.693429 min 1.000000 0.000000 1.000000 0.420000 0.000000 0.000000 0.000000 25% 223.500000 0.000000 2.000000 20.125000 0.000000 0.000000 7.910400 50% 446.000000 0.000000 3.000000 28.000000 0.000000 0.000000 14.454200 75% 668.500000 1.000000 3.000000 38.000000 1.000000 0.000000 31.000000 max 891.000000 1.000000 3.000000 80.000000 8.000000 6.000000 512.329200 分类特征的分布是什么样的？ Name是唯一的（下表中，name的count为891，与样本总数一致） Sex的取值只有两种，其中male占多数，占比577/891=64.9% Cabin(船舱)有重复的，其中204个样本有船舱号，不同的船舱号有104个。所以存在多个样本的船舱号一样的情况。译者注：同一个船舱号中的人可能都幸存。由此甚至可以推出，相同姓氏的人可能都幸存 Embarked（登船口）有三种取值，其中从S口登船的人最多，有664个 Ticket，有(891-681)/891=22%的样本的Tickt信息重复。译者注：可能是登记错误导致的数据错误 1train_df.describe(include=['O']) Output: Name Sex Ticket Cabin Embarked count 891 891 891 204 889 unique 891 2 681 147 3 top Lester, Mr. James male 347082 G6 S freq 1 577 7 4 644 基于数据分析进行假设基于上述简单的分析得到一些假设，然后对数据进行深入的分析，进行验证。 Correlating（寻找特征的关联性） 我们想知道每个特征与结果的关系。我们希望在项目的早期就这样做，并将这些快速的相关性与项目后面的建模相关性进行匹配。 Completing（将缺失数据的特征进行补全） 我们可能想要完整的”Age”特征，因为它肯定与生存相关。 我们可能想要将“Embarked（登船口）”补全，因为它也可能与生存或另一个重要的特征相关。 Correcting（纠正数据） 在我们的分析中，可能要扔掉“Ticket”特征，因为它包含了高比率的重复(22%)，并且Ticket很可能与Survived无关 在训练和测试数据集中，Carbin（舱室）特征可能会被删除，因为它高度不完整或包含许多空值。译者注：训练模型的时候可以把Carbin特征扔掉，后期模型融合的时候这个特征还是可以用的 “PassengerId”特征可以删除，因为它对生存没有帮助。 “Name”特征是相对不标准的，可能不会直接导致生存，所以可能会扔掉。 Creating（创造特征） 我们可能要基于Parch和SibSp创建一个新的特征，叫做“Family”，的家庭，以获得家庭成员的总数。 我们可能要从“Name”特征中提取“Title”作为一个新特征。 我们可能要为年龄层创造新的特征。这将一个连续的数字特征变成一个有序分类的特征。译者注：根据年龄建立直方图，每10岁为一个bin 我们可能还想创建一个Fare range的特征。译者注：与Age特征类似 Classifying（分类）我们还可以根据前面提到的问题描述增加我们的假设。 女性(性=女性)更可能存活。 儿童(Age小于多少)更有可能存活。 舱位等级越高的乘客(Pclass=1)更有可能幸存下来。 分别分析各个特征 Pclass，这个特征具有明显的相关性，Pclass=1的幸存率&gt;0.5(Classifying#3)。可以在模型中使用这个特性。 Sex，女性的幸存率很高，达到74%（Classifying） SibSp和Parch，没有相关性。最好从这些特性(Creating#1)派生一个特性或一组特性。 Pclass1234train_df[['Pclass', 'Survived']] .groupby(['Pclass'], as_index=False) .mean() .sort_values(by='Survived', ascending=False) Output: Pclass Survived 0 1 0.629630 1 2 0.472826 2 3 0.242363 Sex1234train_df[["Sex", "Survived"]] .groupby(['Sex'], as_index=False) .mean() .sort_values(by='Survived', ascending=False) Output: Sex Survived 0 female 0.742038 1 male 0.188908 SibSp1234train_df[["SibSp", "Survived"]] .groupby(['SibSp'], as_index=False) .mean() .sort_values(by='Survived', ascending=False) Output: SibSp Survived 1 1 0.535885 2 2 0.464286 0 0 0.345395 3 3 0.250000 4 4 0.166667 5 5 0.000000 6 8 0.000000 通过可视化来分析校正数值型特征对于特征『Age』，使用sns.FacetGrid分析：12g = sns.FacetGrid(train_df, col='Survived')g.map(plt.hist, 'Age', bins=20) Output: 从图中观察到： 婴儿（小于4岁）的幸存率很高； 最老的乘客（等于80岁）幸存了； 大量的15-25岁的乘客没幸存； 大部分乘客的年龄分布在15-35岁； 结论： 可以将特征『Age』放到最终的模型中； 『Age』列为空的，需要补全； 可以将『Age』特征进行分段； 校正数值型和序数型特征对特征『Pclass』进行分析123456# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)grid.map(plt.hist, 'Age', alpha=.5, bins=20)grid.add_legend(); Output: 从图中观察到： Pclass=3的占大多数，但是大多数都没幸存； 在Pclass=2和Pclass=3中的婴儿，大部分都幸存了； Pclass=1的乘客大多数都幸存了； Pclass在乘客的年龄分布上有所不同。 结论： 考虑将『Pclass』放到最终的模型中。 关联分类型特征对特征『Embarked』进行分析123456# grid = sns.FacetGrid(train_df, col='Embarked')grid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6)grid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')grid.add_legend() Output: 可以观察到： 女性乘客的幸存率较高； 对C和Q口而言，Pclass=3的男性比Pclass=2的有更高的幸存率（译者注：从图上显示的是从C口上船的乘客中，男性比女性幸存率高）； 对Pclass=3的男性乘客而言，Embarked对生存率有不同的影响。 结论： 将『Sex』放到最终的模型中； 补全『Embarked』特征，加到模型中。（译者注：没看懂。） 关联分类型和数值型特征我们还可能希望将分类特性(与非数值)和数字特性关联起来。我们可以考虑关联『Embarked』(非数字分类型)，『Sex』(非数字分类型)，Fare(连续的数值型)，与『Survived』(数字分类型)。 123456# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette=&#123;0: 'k', 1: 'w'&#125;)grid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)grid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)grid.add_legend() Output: 可以从图中观察到： 花费高的乘客具有较高的幸存率； 『Embarked』与幸存率有关。 结论： 考虑将『Fare』特征进行分段。 Wrangle data删除特征删除特征『Cabin』和『Ticket』123456789print("Before", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)train_df = train_df.drop(['Ticket', 'Cabin'], axis=1)test_df = test_df.drop(['Ticket', 'Cabin'], axis=1)combine = [train_df, test_df]print("After", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape) Output:12Before (891, 12) (418, 11) (891, 12) (418, 11)After (891, 10), (418, 9), (891, 10), (418, 9) 创造特征我们在放弃『Name』和『PassengerId』的特征之前，想要分析『Name』特征是否可以被设计来提取『title』并测试『title』与幸存之间的关系。在下面的代码中，我们使用正则表达式从『Name』特征中提取『Title』特征。正则表达式&quot;\w+\.&quot;匹配第一个单词是.结尾的。 12345for dataset in combine: dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\.', expand=False)pd.crosstab(train_df['Title'], train_df['Sex']) Output: Sex female male Title - - Capt 0 1 Col 0 2 Countess 1 0 Don 0 1 Dr 1 6 Jonkheer 0 1 Lady 1 0 Major 0 2 Master 0 40 Miss 182 0 Mlle 2 0 Mme 1 0 Mr 0 517 Mrs 125 0 Ms 1 0 Rev 0 6 Sir 0 1 上表中存在一些title的数量很少的情况，可以将这些数量很少的title用Rare代替：123456789for dataset in combine: dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\ 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare') dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss') dataset['Title'] = dataset['Title'].replace('Ms', 'Miss') dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs') train_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean() Output: Title Survived 0 Master 0.575000 1 Miss 0.702703 2 Mr 0.156673 3 Mrs 0.793651 4 Rare 0.347826 从上表可以观察到： 女性（Miss,Mrs）的幸存率较高 将上述『Title』特征转换成序列特征123456title_mapping = &#123;"Mr": 1, "Miss": 2, "Mrs": 3, "Master": 4, "Rare": 5&#125;for dataset in combine: dataset['Title'] = dataset['Title'].map(title_mapping) dataset['Title'] = dataset['Title'].fillna(0)train_df.head() Output: PassengerId Survived Pclass Name Sex Age SibSp Parch Fare Embarked Title 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 7.2500 S 1 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th… female 38.0 1 0 71.2833 C 3 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 7.9250 S 2 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 53.1000 S 3 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 8.0500 S 1 至此，我们可以删除特征『Name』和『PassengerId』。 1234train_df = train_df.drop(['Name', 'PassengerId'], axis=1)test_df = test_df.drop(['Name'], axis=1)combine = [train_df, test_df]train_df.shape, test_df.shape Output:1((891, 9), (418, 9)) 观察到（译者注：这块没看懂）: Most titles band Age groups accurately. For example: Master title has Age mean of 5 years. Survival among Title Age bands varies slightly. Certain titles mostly survived (Mme, Lady, Sir) or did not (Don, Rev, Jonkheer). 补全连续的数值型特征首先补全『Age』特征，考虑三种方法： 一种简单的方法是在一定的均值和标准差之间生成随机数。 更准确的猜测缺失值的方法是使用其他相关的特性。在我们的案例中，我们注意到年龄、性别和Pclass之间的相关性。根据Pclass和Gender将数据分为多个子集，然后在子集中取年龄的中值。例如，对于Pclass=0且Gender为male的样本，在Pclass=0且Gender为male的子集中，取Age的中值；然后对于Pclass=0且Gender为female的样本以此类推。。。 结合方法1和2。与其直接基于中值法猜测年龄值，不如根据Pclass和Age分类后，再使用均值和标准差之间的随机数。 方法1和方法3将引入随机噪声。实践中，这几个方法略有差异，我们更倾向于方法2. 12345# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')grid = sns.FacetGrid(train_df, row='Pclass', col='Sex', size=2.2, aspect=1.6)grid.map(plt.hist, 'Age', alpha=.5, bins=20)grid.add_legend() Output: Pclass有3种取值，Gender有2种取值，初始化一个2*3的数组，用来存储Age的中值：1234567891011121314151617181920212223242526guess_ages = np.zeros((2,3))guess_agesfor dataset in combine: for i in range(0, 2): for j in range(0, 3): guess_df = dataset[(dataset['Sex'] == i) &amp; \ (dataset['Pclass'] == j+1)]['Age'].dropna() # age_mean = guess_df.mean() # age_std = guess_df.std() # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std) age_guess = guess_df.median() # Convert random age float to nearest .5 age guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5 for i in range(0, 2): for j in range(0, 3): dataset.loc[ (dataset.Age.isnull()) &amp; (dataset.Sex == i) &amp; (dataset.Pclass == j+1),\ 'Age'] = guess_ages[i,j] dataset['Age'] = dataset['Age'].astype(int)#train_df.head() 将年龄分组：12345train_df['AgeBand'] = pd.cut(train_df['Age'], 5)train_df[['AgeBand', 'Survived']] .groupby(['AgeBand'], as_index=False) .mean() .sort_values(by='AgeBand', ascending=True) Output: AgeBand Survived 0 (-0.08, 16.0] 0.550000 1 (16.0, 32.0] 0.337374 2 (32.0, 48.0] 0.412037 3 (48.0, 64.0] 0.434783 4 (64.0, 80.0] 0.090909 将『AgeBand』转换为连续数值型特征：1234567for dataset in combine: dataset.loc[ dataset['Age'] &lt;= 16, 'Age'] = 0 dataset.loc[(dataset['Age'] &gt; 16) &amp; (dataset['Age'] &lt;= 32), 'Age'] = 1 dataset.loc[(dataset['Age'] &gt; 32) &amp; (dataset['Age'] &lt;= 48), 'Age'] = 2 dataset.loc[(dataset['Age'] &gt; 48) &amp; (dataset['Age'] &lt;= 64), 'Age'] = 3 dataset.loc[ dataset['Age'] &gt; 64, 'Age']train_df.head() 删除『AgeBand』特征：123train_df = train_df.drop(['AgeBand'], axis=1)combine = [train_df, test_df]train_df.head() 组合创造出新的特征 我们能将『Parch』和『SibSp』组合创造出新的特征『FamilySize』。 1234567for dataset in combine: dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1train_df[['FamilySize', 'Survived']] .groupby(['FamilySize'], as_index=False) .mean() .sort_values(by='Survived', ascending=False) Output: FamilySize Survived 3 4 0.724138 2 3 0.578431 1 2 0.552795 6 7 0.333333 0 1 0.303538 4 5 0.200000 5 6 0.136364 7 8 0.000000 8 11 0.000000 可以观察到，『FamilySize』特征与是否幸存无线性关系。 创造『IsAlone』特征1234567for dataset in combine: dataset['IsAlone'] = 0 dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1train_df[['IsAlone', 'Survived']] .groupby(['IsAlone'], as_index=False) .mean() Output: IsAlone Survived 0 0 0.505650 1 1 0.303538 至此，可以删掉『Parch』、『SibSp』、『FamilySize』特征，保留『IsAlone』特征。12345train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)test_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)combine = [train_df, test_df]train_df.head() 补全分类型特征『Embarked』特征具有三种取值:S、Q、C。训练集中有2个样本为空值，我们可以简单地用最多的一种取值代替。1234567891011freq_port = train_df.Embarked.dropna().mode()[0]freq_port'S'for dataset in combine: dataset['Embarked'] = dataset['Embarked'].fillna(freq_port) train_df[['Embarked', 'Survived']] .groupby(['Embarked'], as_index=False) .mean() .sort_values(by='Survived', ascending=False) Output: Embarked Survived 0 C 0.553571 1 Q 0.389610 2 S 0.339009 将分类型特征转换成数值型特征将『Embarked』特征转换成数值型特征，起个新名字『Port』。 123for dataset in combine: dataset['Embarked'] = dataset['Embarked'] .map( &#123;'S': 0, 'C': 1, 'Q': 2&#125; ).astype(int) 快速地补全、转换数值型特征用中值补全测试数据集『Fare』特征中唯一一个缺失值。 123test_df['Fare'].fillna( test_df['Fare'].dropna().median(), inplace=True) 创造『FareBand』特征：12345train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)train_df[['FareBand', 'Survived']] .groupby(['FareBand'], as_index=False) .mean() .sort_values(by='FareBand', ascending=True) Output: FareBand Survived 0 (-0.001, 7.91] 0.197309 1 (7.91, 14.454] 0.303571 2 (14.454, 31.0] 0.454955 3 (31.0, 512.329] 0.581081 可见『FareBand』特征与是否幸存线性有关。 将『FareBand』特征转换成连续数值型特征1234567891011for dataset in combine: dataset.loc[ dataset['Fare'] &lt;= 7.91, 'Fare'] = 0 dataset.loc[(dataset['Fare'] &gt; 7.91) &amp; (dataset['Fare'] &lt;= 14.454), 'Fare'] = 1 dataset.loc[(dataset['Fare'] &gt; 14.454) &amp; (dataset['Fare'] &lt;= 31), 'Fare'] = 2 dataset.loc[ dataset['Fare'] &gt; 31, 'Fare'] = 3 dataset['Fare'] = dataset['Fare'].astype(int)train_df = train_df.drop(['FareBand'], axis=1)combine = [train_df, test_df] train_df.head(10) 建模、预测、解决问题现在我们有60多种预测建模算法可供选择。我们必须了解问题的类型和解决方案的要求，以缩小到我们可以评估的少数几个模型。我们的问题是分类和回归问题。我们想要确定输出(Survived)与其他变量或特征(Sex、Age、Port……)之间的关系。我们也在实践一种机器学习方法，称为监督式学习，因为我们正在用给定的数据集训练我们的模型。有了这两个标准——监督学习加分类和回归，我们可以将模型的选择缩小到少数。这些包括: Logistics Regression KNN or k-Nearest Neighbors Support Vector Machines Naive Bayes classifier Decision Tree Random Forrest Perceptron Artificial neural network RVM or Relevance Vector Machine 12345X_train = train_df.drop("Survived", axis=1)Y_train = train_df["Survived"]X_test = test_df.drop("PassengerId", axis=1).copy()X_train.shape, Y_train.shape, X_test.shape((891, 8), (891,), (418, 8)) Logistic Regression12345678# Logistic Regressionlogreg = LogisticRegression()logreg.fit(X_train, Y_train)Y_pred = logreg.predict(X_test)acc_log = round(logreg.score(X_train, Y_train) * 100, 2)acc_log80.359999999999999 我们可以使用Logistic Regression来验证我们的假设和创造的特性。这可以通过分析特征的系数来实现。 正系数增加了响应的对数概率(从而增加了概率)，负系数减小了响应的对数概率(从而降低了概率)。12345coeff_df = pd.DataFrame(train_df.columns.delete(0))coeff_df.columns = ['Feature']coeff_df["Correlation"] = pd.Series(logreg.coef_[0])coeff_df.sort_values(by='Correlation', ascending=False) Output: Feature Correlation 1 Sex 2.201527 5 Title 0.398234 2 Age 0.287163 4 Embarked 0.261762 6 IsAlone 0.129140 3 Fare -0.085150 7 Age*Class -0.311200 0 Pclass -0.749007 『Sex』是最高的正系数，表示随着『Sex』值的增加(男性:0，女性:1)，幸存的概率增加最多。 当『Pclass』增加时，幸存的概率减少最多。 这种『Age*Class』是一个很好的人工特征模型，因为它与生存的负相关系数是第二高的。 『Title』是第二高正相关的。译者注：特征的重要性与系数的绝对值大小有关 SVM12345678# Support Vector Machinessvc = SVC()svc.fit(X_train, Y_train)Y_pred = svc.predict(X_test)acc_svc = round(svc.score(X_train, Y_train) * 100, 2)acc_svc83.840000000000003 KNN123456knn = KNeighborsClassifier(n_neighbors = 3)knn.fit(X_train, Y_train)Y_pred = knn.predict(X_test)acc_knn = round(knn.score(X_train, Y_train) * 100, 2)acc_knn84.739999999999995 Naive Bayes12345678# Gaussian Naive Bayesgaussian = GaussianNB()gaussian.fit(X_train, Y_train)Y_pred = gaussian.predict(X_test)acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)acc_gaussian72.280000000000001 Perceptron12345678# Perceptronperceptron = Perceptron()perceptron.fit(X_train, Y_train)Y_pred = perceptron.predict(X_test)acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)acc_perceptron78.0 Linear SVC12345678# Linear SVClinear_svc = LinearSVC()linear_svc.fit(X_train, Y_train)Y_pred = linear_svc.predict(X_test)acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)acc_linear_svc79.120000000000005 Stochastic Gradient Descent123456sgd = SGDClassifier()sgd.fit(X_train, Y_train)Y_pred = sgd.predict(X_test)acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)acc_sgd77.670000000000002 Decision Tree123456decision_tree = DecisionTreeClassifier()decision_tree.fit(X_train, Y_train)Y_pred = decision_tree.predict(X_test)acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)acc_decision_tree86.760000000000005 Random Forest1234567random_forest = RandomForestClassifier(n_estimators=100)random_forest.fit(X_train, Y_train)Y_pred = random_forest.predict(X_test)random_forest.score(X_train, Y_train)acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)acc_random_forest86.760000000000005 模型评估我们现在可以对所有的模型的结果进行排序，以选出最适合我们的模型。虽然决策树和随机森林得分相同，但我们选择使用随机森林，因为决策树容易过拟合。 123456789models = pd.DataFrame(&#123; 'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', 'Random Forest', 'Naive Bayes', 'Perceptron', 'Stochastic Gradient Decent', 'Linear SVC', 'Decision Tree'], 'Score': [acc_svc, acc_knn, acc_log, acc_random_forest, acc_gaussian, acc_perceptron, acc_sgd, acc_linear_svc, acc_decision_tree]&#125;)models.sort_values(by='Score', ascending=False) Output: Model Score 3 Random Forest 86.76 8 Decision Tree 86.76 1 KNN 84.74 0 Support Vector Machines 83.84 2 Logistic Regression 80.36 7 Linear SVC 79.12 5 Perceptron 78.00 6 Stochastic Gradient Decent 77.67 4 Naive Bayes 72.28 计算结果12345submission = pd.DataFrame(&#123; "PassengerId": test_df["PassengerId"], "Survived": Y_pred &#125;)# submission.to_csv('../output/submission.csv', index=False) 我们提交给比赛网站Kaggle的结果是在6082个样本中命中了3,883个。 参考文献： A journey through Titanic Getting Started with Pandas: Kaggle’s Titanic Competition Titanic Best Working Classifier 【部分表述有待完善。。。】]]></content>
      <categories>
        <category>kaggle</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>kaggle</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo 命令手册]]></title>
    <url>%2F2018%2F03%2F16%2Fhexo%E5%91%BD%E4%BB%A4%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[安装hexo，新建博客，安装server插件，启动博客 12345$ npm install hexo-cli -g$ hexo init blog$ cd blog$ npm install$ hexo server 草稿 123$ hexo new draft &lt;title&gt;$ hexo server --draft$ hexo publish &lt;title&gt; 插件安装与卸载 12$ npm install &lt;plubin name&gt;$ npm uninstall &lt;plugin name&gt;]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>npm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[797.All Paths From Source to Target]]></title>
    <url>%2F2018%2F03%2F16%2F797-All-Paths-From-Source-to-Target%2F</url>
    <content type="text"><![CDATA[LeetCode: 797.All Paths From Source to Target 问题描述Given a directed, acyclic graph of N nodes. Find all possible paths from node 0 to node N-1, and return them in any order. The graph is given as follows: the nodes are 0, 1, …, graph.length - 1. graph[i] is a list of all nodes j for which the edge (i, j) exists. Example:12345678Input: [[1,2], [3], [3], []] Output: [[0,1,3],[0,2,3]] Explanation: The graph looks like this:0---&gt;1| |v v2---&gt;3There are two paths: 0 -&gt; 1 -&gt; 3 and 0 -&gt; 2 -&gt; 3. Note: The number of nodes in the graph will be in the range [2, 15]. You can print different paths in any order, but you should keep the order of nodes inside one path. 分析使用dfs 代码1234567891011121314151617181920212223242526272829303132333435363738class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; allPathsSourceTarget(int[][] graph) &#123; int n = graph.length; // 结点个数 boolean[] visited = new boolean[n]; // 记录i结点是否被访问过 List&lt;Integer&gt; path = new ArrayList&lt;Integer&gt;(); // 路径 path.add(0); // 初始化路径 List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;List&lt;Integer&gt;&gt;(); dfs(graph, visited, path, 0, n - 1, result); return result; &#125; /** * int[] visited 表示节点i是否被访问过 * List&lt;Integer&gt; path 表示路径 * int curr 表示当前访问的结点 * int target 表示目标结点，也就是graph.length-1 * List&lt;List&lt;Integer&gt;&gt; result 存储所有的满足条件的路径 */ public void dfs(int[][] graph, boolean[] visited, List&lt;Integer&gt; path, int curr, int target, List&lt;List&lt;Integer&gt;&gt; result)&#123; //System.out.println("curr:" + curr); if (curr == target)&#123; result.add(new ArrayList&lt;Integer&gt;(path)); return; &#125; for(int i: graph[curr])&#123; //System.out.println("curr:" + curr + ", visite:" + i + ", status:" + visited[i]); if (!visited[i])&#123; // 如果i未被访问 visited[i] = true; // 访问i节点，将i添加到path中 path.add(i); dfs(graph, visited, path, i, target, result); visited[i] = false; // 不访问i节点，将i从path中删除 path.remove(path.size() - 1); &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[553. Optimal Division]]></title>
    <url>%2F2018%2F03%2F15%2F553-Optimal-Division%2F</url>
    <content type="text"><![CDATA[LeetCode: 553. Optimal Division 问题描述Given a list of positive integers, the adjacent integers will perform the float division. For example, [2,3,4] -&gt; 2 / 3 / 4. However, you can add any number of parenthesis at any position to change the priority of operations. You should find out how to add parenthesis to get the maximum result, and return the corresponding expression in string format. Your expression should NOT contain redundant parenthesis. Example: Input: [1000,100,10,2]Output: “1000/(100/10/2)”Explanation:1000/(100/10/2) = 1000/((100/10)/2) = 200However, the bold parenthesis in “1000/((100/10)/2)” are redundant,since they don’t influence the operation priority. So you should return &gt; “1000/(100/10/2)”. Other cases:1000/(100/10)/2 = 501000/(100/(10/2)) = 501000/100/10/2 = 0.51000/100/(10/2) = 2 Note: The length of the input array is [1, 10]. Elements in the given array will be in range [2, 1000]. There is only one optimal division for each test case. 分析用动态规划求解 代码 只求最大值，不用得到表达式 12345678910111213141516171819202122class Solution(object): def __init__(self): self.mat = [] def optimalDivision(self, nums): """ :type nums: List[int] :rtype: str """ n, mat = len(nums), [] for i in range(n): mat.append([0] * n) for margin in range(1, n): for i in range(n - margin): if margin == 1: mat[i][i+margin] = nums[i] / nums[i+margin] #右上角存最大值 mat[i+margin][i] = nums[i] / nums[i+margin] #左下角存最小值 else: mat[i][i+margin] = max(nums[i] / mat[i+margin][i+1], mat[i][i+margin-1] / nums[i+margin]) mat[i+margin][i] = min(nums[i] / mat[i+1][i+margin], mat[i+margin-1][i] / nums[i+margin]) return mat[0][n-1] 用分治，求得表达式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465class Solution(object): def __init__(self): self.mat = [] self.exp = [] def optimalDivision(self, nums): """ :type nums: List[int] :rtype: str """ def dev(nums, start, end): #print nums, 'start:', start, 'end:', end maxDev, minDev, expMax, expMin = 0, 10e8, '', '' if start == end: #print 'minDev:', nums[start], 'maxDev:', nums[start], 'expMin:', nums[start], 'expMax:', nums[start] self.mat[start][end], self.mat[end][start] = nums[start], nums[start] self.exp[start][end], self.exp[end][start] = str(nums[start]), str(nums[start]) maxDev, minDev, expMax, expMin = nums[start], nums[start], str(nums[start]), str(nums[start]) elif start + 1 == end: valDev = float(nums[start]) / nums[end] expDev = str(nums[start]) + "/" + str(nums[end]) self.mat[start][end] = valDev #右上角存最大值 self.mat[end][start] = valDev #左下角存最小值 self.exp[start][end] = expDev #右上角存最大值 self.exp[end][start] = expDev #左下角存最小值 #print 'minDev:', valDev, 'maxDev:', valDev, 'expMin:', expDev, 'expMax:', expDev maxDev, minDev, expMax, expMin = valDev, valDev, expDev, expDev else: for split in range(start, end): if self.mat[start][split] == 0: left = dev(nums, start, split) else: left = self.mat[start][split], self.mat[split][start], self.exp[start][split], self.exp[split][start] if self.mat[split+1][end] == 0: right = dev(nums, split+1, end) else: right = self.mat[split+1][end], self.mat[end][split+1], self.exp[split+1][end], self.exp[end][split+1] valMin, valMax = float(left[1]) / right[0], float(left[0]) / right[1] if valMin &lt; minDev: minDev = valMin if "/" in right[2]: expMin = left[3] + '/(' + right[2] + ')' else: expMin = left[3] + '/' + right[2] if valMax &gt; maxDev: maxDev = valMax if "/" in right[3]: expMax = left[2] + '/(' + right[3] + ')' else: expMin = left[2] + '/' + right[3] self.mat[start][end], self.mat[end][start] = maxDev, minDev self.exp[start][end], self.exp[end][start] = expMax, expMin #print 'minDev:', minDev, 'maxDev:', maxDev, 'expMin:', expMin, 'expMax:', expMax return maxDev, minDev, expMax, expMin n = len(nums) for i in range(n): self.mat.append([0] * n) self.exp.append([''] * n) result = dev(nums, 0, n-1) return result[2]]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[632.Smallest Range]]></title>
    <url>%2F2018%2F03%2F15%2F632-Smallest-Range%2F</url>
    <content type="text"><![CDATA[LeetCode: 632.Smallest Range 问题描述You have k lists of sorted integers in ascending order. Find the smallest range that includes at least one number from each of the k lists. We define the range [a,b] is smaller than range [c,d] if b-a &lt; d-c or a &lt; c if b-a == d-c. Example 1: Input:[[4,10,15,24,26], [0,9,12,20], [5,18,22,30]]Output: [20,24]Explanation:List 1: [4, 10, 15, 24,26], 24 is in range [20,24].List 2: [0, 9, 12, 20], 20 is in range [20,24].List 3: [5, 18, 22, 30], 22 is in range [20,24]. Note: The given list may contain duplicates, so ascending order means &gt;= here. 1 &lt;= k &lt;= 3500 -105 &lt;= value of elements &lt;= 105. For Java users, please note that the input type has been changed to List&lt;List&gt;. And after you reset the code template, you’ll see this point. 题意给定k个数组，找出一个最小的区间，使得区间内包含每个数字内至少一个数。 分析用一个优先队列，里面存k个分别来自k个数组的数。 每次从队列里弹出一个最小值，并从弹出值的数组里，添加下一个值。 每次弹出时，计算但是的range，如果比之前的小，就替换掉之前的range，作为一个新结果。 队列，能满足区间里同时来自k个数组的k个数； 最小区间，通过这个来满足：每次往里队列里添加的都是同budga下一个数（也就是紧接着最小的数），如果当期range小于上一个range就替换之。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445public class Solution &#123; public int[] smallestRange(List&lt;List&lt;Integer&gt;&gt; nums) &#123; PriorityQueue&lt;Element&gt; priorityQueue = new PriorityQueue&lt;Element&gt;(new Comparator&lt;Element&gt;() &#123; @Override public int compare(Element o1, Element o2) &#123; return o1.value - o2.value; &#125; &#125;); int maxValue = Integer.MIN_VALUE; for (int i = 0; i &lt; nums.size(); i++) &#123; Element element = new Element(nums.get(i).get(0), 0, i); priorityQueue.offer(element); maxValue = Math.max(maxValue, nums.get(i).get(0)); &#125; int range = Integer.MAX_VALUE; int start = -1, end = -1; while(priorityQueue.size() == nums.size())&#123; Element popElement = priorityQueue.poll(); if (maxValue - popElement.value &lt; range) &#123; start = popElement.value; end = maxValue; range = maxValue - popElement.value; &#125; int index = popElement.index + 1; if (index &lt; nums.get(popElement.budge).size())&#123; Element element = new Element(nums.get(popElement.budge).get(index), index, popElement.budge); priorityQueue.offer(element); maxValue = Math.max(maxValue, element.value); &#125; &#125; return new int[]&#123;start, end&#125;; &#125;&#125;class Element&#123; public int value; public int index; public int budge; public Element(int v, int i, int b)&#123; this.value = v; this.index = i; this.budge = b; &#125;&#125;]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
</search>
