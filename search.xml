<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[李宏毅机器学习2017学习笔记:2.Bias and Variance]]></title>
    <url>%2F2018%2F03%2F26%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02017%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-2-Bias-and-Variance%2F</url>
    <content type="text"><![CDATA[Error来自哪里？ 来自于bias 来自于variance Estimator(估计量)在估计宝可梦的CP值的例子中，正确的函数为 $\hat f$ ，这个我们无法知道。 我们只能从训练数据中学到一个最好的函数 $f^\ast$ .所以，$f^\ast$ 是 $\hat f$的一个estimator。 『估计量』 的 Bia 和 Variance假设有一个变量$x$，满足：$x$的均值为$\mu$，均方差为$\sigma^2$。 如何估计均值$\mu$呢？取N个点：${x^1, x^2,…,x^N}$N个点取均值，结果不会等于$\mu$，当N无限大时，均值会无限接近$\mu$：$$ m = \frac 1 N \sum_n x^n \neq \mu $$虽然每个$m$与$\mu$都不相等，但是$m$的期望等于均值$\mu$。所以用$m$来估计$\mu$，是无偏的。$$ E[m] = E \left[\frac 1 N \sum_n x^n \right] = \frac 1 N \sum_n E[x^n] = \mu $$ 就像打靶的时候，瞄的点是$\mu$，但是由于风、肌肉抖动等的影响，实际打中的地方会散布在瞄的$\mu$的周围。 散布得多散，取决于$m$的方差：$$ Var(m) = \frac {\sigma^2} N $$ $m$的方差取决于样本的数量： 当N比较小时，散布比较开； 当N比较大时，散布比较紧。 如何估计均方差$\sigma^2$呢？$s^2$表示：$$ s^2 = \frac 1 N \sum_n (x^n - m)^2 $$用$s^2$来估计均方差，是有偏的：$$ E[s^2] = \frac {N-1} N \sigma^2 \neq \sigma^2 $$当N很大时，$s^2$的期望会很接近$\sigma^2$. 小结我们的目标是估测靶的中心$\hat f$，对N组数据分别估测N个$f ^\ast$. 每个$f^\ast$与$\hat f$之间存在误差，这个误差来自于%E[f\ast]%与$\hat f$的bias; 另外一个误差来自于$f^\ast$与$\overline f$的variance. Note：如何得到多个$f^\ast$呢？在不同的数据集上估计$f$。 举例：分别用以下3种模型，学习100次，得到多个$f^\ast$，画图如下：从图中可以看到： 从上到下，模型的复杂程度越来越高； 简单的模型散步得很紧密，复杂的模型散布得比较开； 简单的模型受到数据(x)的影响较小（最极端的例子$f(x)=c$，最简单的模型，完全不受数据影响）。 对5000个 $f^\ast$ 求平均，画出来的蓝色线如下图：比较bias： 左边的模型比较简单，求平均之后离 $\hat f$ 较远，bias较大； 右边的模型比较复杂，求平均之后离 $\hat f$ 较近，bias较小。 比较variance： 左边的模型输出值均在 $\hat f$ 附近，variance较小； 右边的模型输出值散布较开，variance较大。 如何处理bias偏大的情况？分析方法： 如果模型不能很好地拟合训练数据，说明bias很大。【欠拟合】 如果模型能拟合训练数据，但是在测试数据上误差较大，那么很可能variance很大。【过拟合】 如果是bais很大，那么需要使模型更加复杂： 加更多的特征 用更加复杂的模型 如果是variance很大，那么： 收集更大的模型。如下图，100个样本训练的模型比10个样本训练的模型散布更加紧凑。 正则化。如下图，从左往右，正则项系数逐渐增大。 如何选择模型？【交叉验证】作业提供了一个训练集和公开的测试集，提交作业的时候，需要在私有的测试集上对提交的结果进行测试。 如果在训练集上训练了3个模型，在测试集上，『Model 3』 的 Error=0.5 最小，于是将『Model 3』的结果提交上去，结果Error&gt;0.5。怎么办？ N折交叉验证使用『N折交叉验证』来选择最优的模型，过程如下。 将训练集分成N份； 对于每个模型而言，依次将第i份拿出来作验证，在其他的N-1数据上训练，得到N个训练误差。求平均，得到平均训练误差； 对于多个训练模型而言，取平均训练误差最小的模型。 总结 如果在训练集上误差较大，那么bias较大，说明是欠拟合。考虑增加特征，或者换更复杂的模型； 如果在训练集上误差较小，在测试集上误差较大，过拟合。考虑收集更多的数据，或者采用正则化。 多个候选模型之间如何选择？采用N折交叉验证。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>李宏毅</tag>
        <tag>Bias</tag>
        <tag>Variance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李宏毅机器学习2017学习笔记:1.Regression Demo]]></title>
    <url>%2F2018%2F03%2F26%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02017%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-Regression-Demo%2F</url>
    <content type="text"><![CDATA[视频简介视频中先固定learning rate，迭代10w次： 首先，设置了一个比较小的learning rate，lr=1e-6，没有得到最优解，就停止了； 然后，放大lrarning rate，lr=1e-5，结果出现震荡的情况，无法得到最优解。 最后，采用AdaGrad方法调整learning rate，得到了最优解。 固定learning rate123456789lr = 0.000001for i in range(iteration): # calculate gradient b_grad = ... w_grad = ... # update weight b = b - lr * b_grad w = w - lr * w_ grad 动态调整learning rate视频中使用的方法是：AdaGrad 123456789101112131415lr = 1lr_b = 0lr_w = 0for i in range(iteration): # calculate gradient b_grad = ... w_grad = ... # update lr_b, lr_w with AdaGrad lr_b = lr_b + b_grad ** 2 lr_w = lr_w + w_grad ** 2 # update weight b = b - lr/np.sqrt(lr_b) * b_grad w = w - lr/np.sqrt(lr_w) * w_ grad]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>李宏毅</tag>
        <tag>Regression</tag>
        <tag>调参</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[李宏毅机器学习2017学习笔记:1.Regression]]></title>
    <url>%2F2018%2F03%2F24%2F%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02017%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-1-Regression%2F</url>
    <content type="text"><![CDATA[构建最基本的回归模型1.问题描述：如何根据宝可梦的CP值预测进化后的CP值？ 2.模型假设：$$y = b + w * X_{cp}$$ 为方便表示和计算，用$w_0$替代$b$，只需要在$X_{cp}$前面添加一维数值1，模型变为：$$y = w * X_{cp}$$ 3.损失函数：$$L(f) = L(w) = \frac12 \sum_{i=1}^{10} \left(\hat {y}^i - w · x_{cp}^i\right) ^2$$ 4.优化方法（梯度下降法）：目标函数：$$ f^\ast = arg \min_{f}^{} L(f) $$代入模型方程：$$ w^\ast= arg \min_{w} \frac12 \sum_{i=1}^{10}\left(\hat {y}^i - w · x_{cp}^i\right)^2$$ 求梯度:$$ \frac {\partial L} {\partial w} = \sum_{i=1}^{10} \left( \hat {y}^i - w · x_{cp}^i\right) (-x_{cp}^i)$$参数更新:$$w_{t+1} = w_t - \alpha · \frac {\partial L} {\partial w}$$其中，$\alpha$为步长。 5.结果：在测试集上的平均误差为35. 对回归模型进行优化选择更加复杂的模型 增加一维特征$(X_{cp})^2$$$y = b + w_1·X_{cp} + w_2 · (X_{cp})^2$$在训练集上和测试集上的平均误差分别15.4和18.4 再增加一维特征$(X_{cp})^3$在训练集上和测试集上的平均误差分别15.3和18.1 再增加一维特征$(X_{cp})^4$在训练集上和测试集上的平均误差分别14.9和28.8 再增加一维特征$(X_{cp})^5$在训练集上和测试集上的平均误差分别12.8和232.1 上述实验，可以看出： 更复杂的模型，可以得到更小的训练误差； 更复杂的模型，可能导致过拟合，在测试集上表现不好。 解决办法：收集更多的数据； 收集更多的特征后发现，下图中，同一个x，对应两个不同的y，所以推测存在隐藏的因子（宝可梦的种类）。 加上种类属性，重新建模：也就是说，每一类宝可梦分别对应一个回归模型.在训练集和测试集上的误差分别为3.8和14.3 继续猜测，可能还有隐藏因子，例如『高度』、『体重』等。验证一下，过拟合了！ 正则化（Regularization）：以上猜测隐藏因子的方法不一定猜得到，正则化一般来说是有用的。 在损失函数后面添加一项对参数的约束：$$L(f) = L(w) = \frac12 \sum_{i=1}^{10} \left(\hat {y}^i - w · x_{cp}^i\right) ^2 + \lambda \sum(w_i)^2$$ $w_i$很小，意味着函数比较平滑。 什么是平滑的函数？当输入变化很大时，输出的变化不大，则这个函数是平滑的。如下图，当左右两边都加上$\delta x_i$时，如果$w_i$比较小，那么$y$的变化也会比较小。 需要给bias加regularization?不需要，因为bias只影响函数上下移动，并不影响函数的平滑度。 小结： 宝可梦进化后的CP值与进化前的CP值和宝可梦的类型有关，也可能存在其他的隐藏因子； 梯度下降方法； 我们最终在测试集上得到的误差为11.1，那么这个模型应用于新的数据集、更大的数据集上，误差会变大还是变小？ 下一节：误差来自哪里？]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>李宏毅</tag>
        <tag>Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[异地同步博客工程源码]]></title>
    <url>%2F2018%2F03%2F19%2F%E5%BC%82%E5%9C%B0%E5%90%8C%E6%AD%A5%E5%8D%9A%E5%AE%A2%E5%B7%A5%E7%A8%8B%E6%BA%90%E7%A0%81%2F</url>
    <content type="text"><![CDATA[hexo主目录结构12345678|-- _config.yml |-- package.json |-- scaffolds|-- source |-- _posts |-- _drafts |-- themes|-- .gitignore _config.yml，全局配置文件。，网站的很多信息都在这里配置，诸如网站名称，副标题，描述，作者，语言，主题，部署等等参数。 package.json，框架的参数和依赖插件 scaffolds，是“脚手架、骨架”的意思，当你新建一篇文章（hexo new ‘title’）的时候，hexo是根据这个目录下的文件进行构建的。 source目录，博客文件存储的目录。其中，_posts是发布出来的博客的目录，_drafts是草稿存储的目录。 themes，存储主题的目录。 安装其他插件之后，会有其他的目录或文件：1|-- .deploy_git .deploy_git，hexo-deploy-git插件自动生成的目录 db.json，NexT主题生成的文件 需要同步的文件和目录需要同步的文件和目录包括： _config_yml package.json source themes scaffolds 剩下不需要同步的文件，添加到.gitignore文件中：1234567.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/ 使用github进行异地同步本机上传源码我们在xxx.github.io这个repo下建一个source分支，来保存博客源码。 新建source分支 1$ git checkout -b source 上传博客文件 123$ git add --all$ git commit -m "init blog data"$ git push origin source 上传第三方主题文件因为第三方主题文件是从github上clone下来的一个完整的repo，那么在MyBlog目录下是无法直接将主题文件上传到自建的reop上的。如果执行git add ./themes/next/*会报fatal: Pathspec ‘themes/next/_config.yml’ is in submodule的错误。 如果以后不想更新主题，最简单的方式是先清空next主题目录下的.git文件夹，然后清空缓存，最后提交主题文件。12345$ rm -rf ./themes/next/.git$ git rm -rf --cached ./themes/next/$ git commit -m &quot;add theme NexT&quot;$ git push origin master 如果想以后更新主题，那么在添加主题的时候，不能直接git clone第三方主题。 Fork该第三方主题仓库，这样就会在自己账号下生成一个同名的仓库，并对应一个url，我们应该git clone自己账号下的url。 这样做的原因是：我们很有可能在原来主题基础上做一些自定义的小改动，为了保持多终端的同步，我们需要将这些改动提交到远程仓库。而第三方仓库我们是无法直接push的。 这样就会出现git仓库的嵌套问题，我们通过git submodule来解决这个问题.下面这条命令会在themes/next目录下clone自己账号下的hexo-theme-next1$ git submodule add git@github.com:xxx/hexo-theme-next.git themes/next 查看submodule列表：git submodule补充一个删除submodule的方法： git submodule deinit themes/next 将第三方主题提交到source分支：123$ git add ./themes/*$ git commit -m 'add theme next and hueman'$ git push origin source 远程repo的source分支中并不会添加第三方主题的源码，而是类似指针一样，在themes目录下多两个（添加了2个第三方主题）链接到其他repo的文件夹： 修改了第三方主题文件之后，需要切换到第三方主题的目录下提交。1234$ cd ./themes/next$ git add _config.yml$ git commit -m 'config theme next'$ git push origin master 另一台机器下载 安装nodejs和git环境 从source分支clone博客源码无法clone第三方主题文件。 1$ git clone -b source https://github.com/xxx/MyBlogData.git MyBlog 安装hexo相关插件 12$ npm install# 安装package.json文件中配置的依赖包 clone第三方主题第三方主题在submodule里，需要执行下面语句将第三方主题文件clone下来。 12345$ git submodule initSubmodule 'themes/hueman' (git@github.com:xxx/hexo-theme-hueman.git) registered for path 'themes/hueman'Submodule 'themes/next' (git@github.com:xxx/hexo-theme-next.git) registered for path 'themes/next'$ git submodule update 我们修改的第三方主题文件提交到master分支了，需要切换到master分支将最新的代码pull下来。12345678910$ cd ./themes/next$ git branch* (HEAD detached at 9f7f5ae) master$ git checkout master$ git branch* master$ git pull 总结 将源码push在source分支 使用submodule解决第三方主题的问题 修改第三方主题文件的时候，需要切换到第三方主题的目录 参考：关于博客同步的解决办法: https://devtian.me/2015/03/17/blog-sync-solution/]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>Hexo</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Titanic Data Science Solution(翻译)]]></title>
    <url>%2F2018%2F03%2F18%2FTitanic-Data-Science-Solution-%E7%BF%BB%E8%AF%91%2F</url>
    <content type="text"><![CDATA[原文链接：Titanic Data Science Solution 工作流kaggle比赛工作流包含7个阶段： 理解问题； 获取训练数据和测试数据； 数据清理； 分析、确定特征； 建模、训练、预测； 可视化、报告、提出解决问题的步骤和最终的方案； 提交结果。 理解问题仔细审题，理解是分类问题还是回归问题，或者其它。 获取数据123train_df = pd.read_csv('../input/train.csv')test_df = pd.read_csv('../input/test.csv')combine = [train_df, test_df] 数据清理pandas包含一些获取数据描述的方法。 数据有哪些特征？1print(train_df.columns.values) [‘PassengerId’ ‘Survived’ ‘Pclass’ ‘Name’ ‘Sex’ ‘Age’ ‘SibSp’‘Parch’ ‘Ticket’ ‘Fare’ ‘Cabin’ ‘Embarked’] 预览数据：1train_df.head() 哪些特征是分类的？有些特征能将数据集分成多个子集。例如性别。可以对这些特征进行可视化，分析数据的分布。 分类的：Survived（是否幸存），Sex（性别），Embarked（登船口） 序列的：Pclass（舱位等级） 哪些特征是数值的？这些数值特征是离散的、连续的、还是时间序列的？ 连续的：Age（年龄），Fare（票价） 离散的：SibSp，Parch 哪些特征的数据类型是混乱的？有些特征的数据类型既有数字的，也有字母的，这些特征在数据清理环节需要被处理。 Ticket：数字和字母混合的 Cabin（船舱）：字母的 哪些特征包含错误数据？对于大型数据集来说比较困难，但是从较小的数据集中查看一些示例可能得出哪些特性需要改正。 “Name”这个特征可能包含错误数据，因为有很多种方式来描述一个人的名字，如简称，名字字符串也可能附有圆括号或引号 哪些特征包含空值、null、NaN等？包含空值的特征，是具体情况采用不同的方式进行填补。 这三个特征包含空值：Cabin &gt; Age &gt; Embarked Carbin和Age在测试集中不完整 各个特征的数据类型是什么？123train_df.info()print("_" * 50)test_df.info() Output：12345678910111213141516171819202122232425262728293031323334&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;RangeIndex: 891 entries, 0 to 890Data columns (total 12 columns):PassengerId 891 non-null int64Survived 891 non-null int64Pclass 891 non-null int64Name 891 non-null objectSex 891 non-null objectAge 714 non-null float64SibSp 891 non-null int64Parch 891 non-null int64Ticket 891 non-null objectFare 891 non-null float64Cabin 204 non-null objectEmbarked 889 non-null objectdtypes: float64(2), int64(5), object(5)memory usage: 83.6+ KB________________________________________&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;RangeIndex: 418 entries, 0 to 417Data columns (total 11 columns):PassengerId 418 non-null int64Pclass 418 non-null int64Name 418 non-null objectSex 418 non-null objectAge 332 non-null float64SibSp 418 non-null int64Parch 418 non-null int64Ticket 418 non-null objectFare 417 non-null float64Cabin 91 non-null objectEmbarked 418 non-null objectdtypes: float64(2), int64(4), object(5)memory usage: 36.0+ KB 数值特征的分布是什么？这一步在早期分析中，有助于我们充分理解数据。 样本总数为891，占实际所有人数2224的40% “Survived”是一种具有0或1值的分类特征 样本中的幸存率大约是38%，实际的幸存率为32% 大多数乘客（超过75%）没有和父母或孩子一起旅行 近三成的乘客有兄弟姐妹 和/或 配偶 票价差异很大，很少有乘客（&lt;1%）支付高达512美元的费用 年龄在65-80岁之间的老人很少（&lt;1%） 123456train_df.describe()# Review survived rate using `percentiles=[.61, .62]` # knowing our problem description mentions 38% survival rate.# Review Parch distribution using `percentiles=[.75, .8]`# SibSp distribution `[.68, .69]`# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]` Output: PassengerId Survived Pclass Age SibSp Parch Fare count 891.000000 891.000000 891.000000 714.000000 891.000000 891.000000 891.000000 mean 446.000000 0.383838 2.308642 29.699118 0.523008 0.381594 32.204208 std 257.353842 0.486592 0.836071 14.526497 1.102743 0.806057 49.693429 min 1.000000 0.000000 1.000000 0.420000 0.000000 0.000000 0.000000 25% 223.500000 0.000000 2.000000 20.125000 0.000000 0.000000 7.910400 50% 446.000000 0.000000 3.000000 28.000000 0.000000 0.000000 14.454200 75% 668.500000 1.000000 3.000000 38.000000 1.000000 0.000000 31.000000 max 891.000000 1.000000 3.000000 80.000000 8.000000 6.000000 512.329200 分类特征的分布是什么样的？ Name是唯一的（下表中，name的count为891，与样本总数一致） Sex的取值只有两种，其中male占多数，占比577/891=64.9% Cabin(船舱)有重复的，其中204个样本有船舱号，不同的船舱号有104个。所以存在多个样本的船舱号一样的情况。译者注：同一个船舱号中的人可能都幸存。由此甚至可以推出，相同姓氏的人可能都幸存 Embarked（登船口）有三种取值，其中从S口登船的人最多，有664个 Ticket，有(891-681)/891=22%的样本的Tickt信息重复。译者注：可能是登记错误导致的数据错误 1train_df.describe(include=['O']) Output: Name Sex Ticket Cabin Embarked count 891 891 891 204 889 unique 891 2 681 147 3 top Lester, Mr. James male 347082 G6 S freq 1 577 7 4 644 基于数据分析进行假设基于上述简单的分析得到一些假设，然后对数据进行深入的分析，进行验证。 Correlating（寻找特征的关联性） 我们想知道每个特征与结果的关系。我们希望在项目的早期就这样做，并将这些快速的相关性与项目后面的建模相关性进行匹配。 Completing（将缺失数据的特征进行补全） 我们可能想要完整的”Age”特征，因为它肯定与生存相关。 我们可能想要将“Embarked（登船口）”补全，因为它也可能与生存或另一个重要的特征相关。 Correcting（纠正数据） 在我们的分析中，可能要扔掉“Ticket”特征，因为它包含了高比率的重复(22%)，并且Ticket很可能与Survived无关 在训练和测试数据集中，Carbin（舱室）特征可能会被删除，因为它高度不完整或包含许多空值。译者注：训练模型的时候可以把Carbin特征扔掉，后期模型融合的时候这个特征还是可以用的 “PassengerId”特征可以删除，因为它对生存没有帮助。 “Name”特征是相对不标准的，可能不会直接导致生存，所以可能会扔掉。 Creating（创造特征） 我们可能要基于Parch和SibSp创建一个新的特征，叫做“Family”，的家庭，以获得家庭成员的总数。 我们可能要从“Name”特征中提取“Title”作为一个新特征。 我们可能要为年龄层创造新的特征。这将一个连续的数字特征变成一个有序分类的特征。译者注：根据年龄建立直方图，每10岁为一个bin 我们可能还想创建一个Fare range的特征。译者注：与Age特征类似 Classifying（分类）我们还可以根据前面提到的问题描述增加我们的假设。 女性(性=女性)更可能存活。 儿童(Age小于多少)更有可能存活。 舱位等级越高的乘客(Pclass=1)更有可能幸存下来。 分别分析各个特征 Pclass，这个特征具有明显的相关性，Pclass=1的幸存率&gt;0.5(Classifying#3)。可以在模型中使用这个特性。 Sex，女性的幸存率很高，达到74%（Classifying） SibSp和Parch，没有相关性。最好从这些特性(Creating#1)派生一个特性或一组特性。 Pclass1234train_df[['Pclass', 'Survived']] .groupby(['Pclass'], as_index=False) .mean() .sort_values(by='Survived', ascending=False) Output: Pclass Survived 0 1 0.629630 1 2 0.472826 2 3 0.242363 Sex1234train_df[["Sex", "Survived"]] .groupby(['Sex'], as_index=False) .mean() .sort_values(by='Survived', ascending=False) Output: Sex Survived 0 female 0.742038 1 male 0.188908 SibSp1234train_df[["SibSp", "Survived"]] .groupby(['SibSp'], as_index=False) .mean() .sort_values(by='Survived', ascending=False) Output: SibSp Survived 1 1 0.535885 2 2 0.464286 0 0 0.345395 3 3 0.250000 4 4 0.166667 5 5 0.000000 6 8 0.000000 通过可视化来分析校正数值型特征对于特征『Age』，使用sns.FacetGrid分析：12g = sns.FacetGrid(train_df, col='Survived')g.map(plt.hist, 'Age', bins=20) Output: 从图中观察到： 婴儿（小于4岁）的幸存率很高； 最老的乘客（等于80岁）幸存了； 大量的15-25岁的乘客没幸存； 大部分乘客的年龄分布在15-35岁； 结论： 可以将特征『Age』放到最终的模型中； 『Age』列为空的，需要补全； 可以将『Age』特征进行分段； 校正数值型和序数型特征对特征『Pclass』进行分析123456# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)grid.map(plt.hist, 'Age', alpha=.5, bins=20)grid.add_legend(); Output: 从图中观察到： Pclass=3的占大多数，但是大多数都没幸存； 在Pclass=2和Pclass=3中的婴儿，大部分都幸存了； Pclass=1的乘客大多数都幸存了； Pclass在乘客的年龄分布上有所不同。 结论： 考虑将『Pclass』放到最终的模型中。 关联分类型特征对特征『Embarked』进行分析123456# grid = sns.FacetGrid(train_df, col='Embarked')grid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6)grid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')grid.add_legend() Output: 可以观察到： 女性乘客的幸存率较高； 对C和Q口而言，Pclass=3的男性比Pclass=2的有更高的幸存率（译者注：从图上显示的是从C口上船的乘客中，男性比女性幸存率高）； 对Pclass=3的男性乘客而言，Embarked对生存率有不同的影响。 结论： 将『Sex』放到最终的模型中； 补全『Embarked』特征，加到模型中。（译者注：没看懂。） 关联分类型和数值型特征我们还可能希望将分类特性(与非数值)和数字特性关联起来。我们可以考虑关联『Embarked』(非数字分类型)，『Sex』(非数字分类型)，Fare(连续的数值型)，与『Survived』(数字分类型)。 123456# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette=&#123;0: 'k', 1: 'w'&#125;)grid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)grid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)grid.add_legend() Output: 可以从图中观察到： 花费高的乘客具有较高的幸存率； 『Embarked』与幸存率有关。 结论： 考虑将『Fare』特征进行分段。 Wrangle data删除特征删除特征『Cabin』和『Ticket』123456789print("Before", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)train_df = train_df.drop(['Ticket', 'Cabin'], axis=1)test_df = test_df.drop(['Ticket', 'Cabin'], axis=1)combine = [train_df, test_df]print("After", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape) Output:12Before (891, 12) (418, 11) (891, 12) (418, 11)After (891, 10), (418, 9), (891, 10), (418, 9) 创造特征我们在放弃『Name』和『PassengerId』的特征之前，想要分析『Name』特征是否可以被设计来提取『title』并测试『title』与幸存之间的关系。在下面的代码中，我们使用正则表达式从『Name』特征中提取『Title』特征。正则表达式&quot;\w+\.&quot;匹配第一个单词是.结尾的。 12345for dataset in combine: dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\.', expand=False)pd.crosstab(train_df['Title'], train_df['Sex']) Output: Sex female male Title - - Capt 0 1 Col 0 2 Countess 1 0 Don 0 1 Dr 1 6 Jonkheer 0 1 Lady 1 0 Major 0 2 Master 0 40 Miss 182 0 Mlle 2 0 Mme 1 0 Mr 0 517 Mrs 125 0 Ms 1 0 Rev 0 6 Sir 0 1 上表中存在一些title的数量很少的情况，可以将这些数量很少的title用Rare代替：123456789for dataset in combine: dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\ 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare') dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss') dataset['Title'] = dataset['Title'].replace('Ms', 'Miss') dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs') train_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean() Output: Title Survived 0 Master 0.575000 1 Miss 0.702703 2 Mr 0.156673 3 Mrs 0.793651 4 Rare 0.347826 从上表可以观察到： 女性（Miss,Mrs）的幸存率较高 将上述『Title』特征转换成序列特征123456title_mapping = &#123;"Mr": 1, "Miss": 2, "Mrs": 3, "Master": 4, "Rare": 5&#125;for dataset in combine: dataset['Title'] = dataset['Title'].map(title_mapping) dataset['Title'] = dataset['Title'].fillna(0)train_df.head() Output: PassengerId Survived Pclass Name Sex Age SibSp Parch Fare Embarked Title 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 7.2500 S 1 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th… female 38.0 1 0 71.2833 C 3 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 7.9250 S 2 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 53.1000 S 3 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 8.0500 S 1 至此，我们可以删除特征『Name』和『PassengerId』。 1234train_df = train_df.drop(['Name', 'PassengerId'], axis=1)test_df = test_df.drop(['Name'], axis=1)combine = [train_df, test_df]train_df.shape, test_df.shape Output:1((891, 9), (418, 9)) 观察到（译者注：这块没看懂）: Most titles band Age groups accurately. For example: Master title has Age mean of 5 years. Survival among Title Age bands varies slightly. Certain titles mostly survived (Mme, Lady, Sir) or did not (Don, Rev, Jonkheer). 补全连续的数值型特征首先补全『Age』特征，考虑三种方法： 一种简单的方法是在一定的均值和标准差之间生成随机数。 更准确的猜测缺失值的方法是使用其他相关的特性。在我们的案例中，我们注意到年龄、性别和Pclass之间的相关性。根据Pclass和Gender将数据分为多个子集，然后在子集中取年龄的中值。例如，对于Pclass=0且Gender为male的样本，在Pclass=0且Gender为male的子集中，取Age的中值；然后对于Pclass=0且Gender为female的样本以此类推。。。 结合方法1和2。与其直接基于中值法猜测年龄值，不如根据Pclass和Age分类后，再使用均值和标准差之间的随机数。 方法1和方法3将引入随机噪声。实践中，这几个方法略有差异，我们更倾向于方法2. 12345# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')grid = sns.FacetGrid(train_df, row='Pclass', col='Sex', size=2.2, aspect=1.6)grid.map(plt.hist, 'Age', alpha=.5, bins=20)grid.add_legend() Output: Pclass有3种取值，Gender有2种取值，初始化一个2*3的数组，用来存储Age的中值：1234567891011121314151617181920212223242526guess_ages = np.zeros((2,3))guess_agesfor dataset in combine: for i in range(0, 2): for j in range(0, 3): guess_df = dataset[(dataset['Sex'] == i) &amp; \ (dataset['Pclass'] == j+1)]['Age'].dropna() # age_mean = guess_df.mean() # age_std = guess_df.std() # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std) age_guess = guess_df.median() # Convert random age float to nearest .5 age guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5 for i in range(0, 2): for j in range(0, 3): dataset.loc[ (dataset.Age.isnull()) &amp; (dataset.Sex == i) &amp; (dataset.Pclass == j+1),\ 'Age'] = guess_ages[i,j] dataset['Age'] = dataset['Age'].astype(int)#train_df.head() 将年龄分组：12345train_df['AgeBand'] = pd.cut(train_df['Age'], 5)train_df[['AgeBand', 'Survived']] .groupby(['AgeBand'], as_index=False) .mean() .sort_values(by='AgeBand', ascending=True) Output: AgeBand Survived 0 (-0.08, 16.0] 0.550000 1 (16.0, 32.0] 0.337374 2 (32.0, 48.0] 0.412037 3 (48.0, 64.0] 0.434783 4 (64.0, 80.0] 0.090909 将『AgeBand』转换为连续数值型特征：1234567for dataset in combine: dataset.loc[ dataset['Age'] &lt;= 16, 'Age'] = 0 dataset.loc[(dataset['Age'] &gt; 16) &amp; (dataset['Age'] &lt;= 32), 'Age'] = 1 dataset.loc[(dataset['Age'] &gt; 32) &amp; (dataset['Age'] &lt;= 48), 'Age'] = 2 dataset.loc[(dataset['Age'] &gt; 48) &amp; (dataset['Age'] &lt;= 64), 'Age'] = 3 dataset.loc[ dataset['Age'] &gt; 64, 'Age']train_df.head() 删除『AgeBand』特征：123train_df = train_df.drop(['AgeBand'], axis=1)combine = [train_df, test_df]train_df.head() 组合创造出新的特征 我们能将『Parch』和『SibSp』组合创造出新的特征『FamilySize』。 1234567for dataset in combine: dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1train_df[['FamilySize', 'Survived']] .groupby(['FamilySize'], as_index=False) .mean() .sort_values(by='Survived', ascending=False) Output: FamilySize Survived 3 4 0.724138 2 3 0.578431 1 2 0.552795 6 7 0.333333 0 1 0.303538 4 5 0.200000 5 6 0.136364 7 8 0.000000 8 11 0.000000 可以观察到，『FamilySize』特征与是否幸存无线性关系。 创造『IsAlone』特征1234567for dataset in combine: dataset['IsAlone'] = 0 dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1train_df[['IsAlone', 'Survived']] .groupby(['IsAlone'], as_index=False) .mean() Output: IsAlone Survived 0 0 0.505650 1 1 0.303538 至此，可以删掉『Parch』、『SibSp』、『FamilySize』特征，保留『IsAlone』特征。12345train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)test_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)combine = [train_df, test_df]train_df.head() 补全分类型特征『Embarked』特征具有三种取值:S、Q、C。训练集中有2个样本为空值，我们可以简单地用最多的一种取值代替。1234567891011freq_port = train_df.Embarked.dropna().mode()[0]freq_port'S'for dataset in combine: dataset['Embarked'] = dataset['Embarked'].fillna(freq_port) train_df[['Embarked', 'Survived']] .groupby(['Embarked'], as_index=False) .mean() .sort_values(by='Survived', ascending=False) Output: Embarked Survived 0 C 0.553571 1 Q 0.389610 2 S 0.339009 将分类型特征转换成数值型特征将『Embarked』特征转换成数值型特征，起个新名字『Port』。 123for dataset in combine: dataset['Embarked'] = dataset['Embarked'] .map( &#123;'S': 0, 'C': 1, 'Q': 2&#125; ).astype(int) 快速地补全、转换数值型特征用中值补全测试数据集『Fare』特征中唯一一个缺失值。 123test_df['Fare'].fillna( test_df['Fare'].dropna().median(), inplace=True) 创造『FareBand』特征：12345train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)train_df[['FareBand', 'Survived']] .groupby(['FareBand'], as_index=False) .mean() .sort_values(by='FareBand', ascending=True) Output: FareBand Survived 0 (-0.001, 7.91] 0.197309 1 (7.91, 14.454] 0.303571 2 (14.454, 31.0] 0.454955 3 (31.0, 512.329] 0.581081 可见『FareBand』特征与是否幸存线性有关。 将『FareBand』特征转换成连续数值型特征1234567891011for dataset in combine: dataset.loc[ dataset['Fare'] &lt;= 7.91, 'Fare'] = 0 dataset.loc[(dataset['Fare'] &gt; 7.91) &amp; (dataset['Fare'] &lt;= 14.454), 'Fare'] = 1 dataset.loc[(dataset['Fare'] &gt; 14.454) &amp; (dataset['Fare'] &lt;= 31), 'Fare'] = 2 dataset.loc[ dataset['Fare'] &gt; 31, 'Fare'] = 3 dataset['Fare'] = dataset['Fare'].astype(int)train_df = train_df.drop(['FareBand'], axis=1)combine = [train_df, test_df] train_df.head(10) 建模、预测、解决问题现在我们有60多种预测建模算法可供选择。我们必须了解问题的类型和解决方案的要求，以缩小到我们可以评估的少数几个模型。我们的问题是分类和回归问题。我们想要确定输出(Survived)与其他变量或特征(Sex、Age、Port……)之间的关系。我们也在实践一种机器学习方法，称为监督式学习，因为我们正在用给定的数据集训练我们的模型。有了这两个标准——监督学习加分类和回归，我们可以将模型的选择缩小到少数。这些包括: Logistics Regression KNN or k-Nearest Neighbors Support Vector Machines Naive Bayes classifier Decision Tree Random Forrest Perceptron Artificial neural network RVM or Relevance Vector Machine 12345X_train = train_df.drop("Survived", axis=1)Y_train = train_df["Survived"]X_test = test_df.drop("PassengerId", axis=1).copy()X_train.shape, Y_train.shape, X_test.shape((891, 8), (891,), (418, 8)) Logistic Regression12345678# Logistic Regressionlogreg = LogisticRegression()logreg.fit(X_train, Y_train)Y_pred = logreg.predict(X_test)acc_log = round(logreg.score(X_train, Y_train) * 100, 2)acc_log80.359999999999999 我们可以使用Logistic Regression来验证我们的假设和创造的特性。这可以通过分析特征的系数来实现。 正系数增加了响应的对数概率(从而增加了概率)，负系数减小了响应的对数概率(从而降低了概率)。12345coeff_df = pd.DataFrame(train_df.columns.delete(0))coeff_df.columns = ['Feature']coeff_df["Correlation"] = pd.Series(logreg.coef_[0])coeff_df.sort_values(by='Correlation', ascending=False) Output: Feature Correlation 1 Sex 2.201527 5 Title 0.398234 2 Age 0.287163 4 Embarked 0.261762 6 IsAlone 0.129140 3 Fare -0.085150 7 Age*Class -0.311200 0 Pclass -0.749007 『Sex』是最高的正系数，表示随着『Sex』值的增加(男性:0，女性:1)，幸存的概率增加最多。 当『Pclass』增加时，幸存的概率减少最多。 这种『Age*Class』是一个很好的人工特征模型，因为它与生存的负相关系数是第二高的。 『Title』是第二高正相关的。译者注：特征的重要性与系数的绝对值大小有关 SVM12345678# Support Vector Machinessvc = SVC()svc.fit(X_train, Y_train)Y_pred = svc.predict(X_test)acc_svc = round(svc.score(X_train, Y_train) * 100, 2)acc_svc83.840000000000003 KNN123456knn = KNeighborsClassifier(n_neighbors = 3)knn.fit(X_train, Y_train)Y_pred = knn.predict(X_test)acc_knn = round(knn.score(X_train, Y_train) * 100, 2)acc_knn84.739999999999995 Naive Bayes12345678# Gaussian Naive Bayesgaussian = GaussianNB()gaussian.fit(X_train, Y_train)Y_pred = gaussian.predict(X_test)acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)acc_gaussian72.280000000000001 Perceptron12345678# Perceptronperceptron = Perceptron()perceptron.fit(X_train, Y_train)Y_pred = perceptron.predict(X_test)acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)acc_perceptron78.0 Linear SVC12345678# Linear SVClinear_svc = LinearSVC()linear_svc.fit(X_train, Y_train)Y_pred = linear_svc.predict(X_test)acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)acc_linear_svc79.120000000000005 Stochastic Gradient Descent123456sgd = SGDClassifier()sgd.fit(X_train, Y_train)Y_pred = sgd.predict(X_test)acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)acc_sgd77.670000000000002 Decision Tree123456decision_tree = DecisionTreeClassifier()decision_tree.fit(X_train, Y_train)Y_pred = decision_tree.predict(X_test)acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)acc_decision_tree86.760000000000005 Random Forest1234567random_forest = RandomForestClassifier(n_estimators=100)random_forest.fit(X_train, Y_train)Y_pred = random_forest.predict(X_test)random_forest.score(X_train, Y_train)acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)acc_random_forest86.760000000000005 模型评估我们现在可以对所有的模型的结果进行排序，以选出最适合我们的模型。虽然决策树和随机森林得分相同，但我们选择使用随机森林，因为决策树容易过拟合。 123456789models = pd.DataFrame(&#123; 'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', 'Random Forest', 'Naive Bayes', 'Perceptron', 'Stochastic Gradient Decent', 'Linear SVC', 'Decision Tree'], 'Score': [acc_svc, acc_knn, acc_log, acc_random_forest, acc_gaussian, acc_perceptron, acc_sgd, acc_linear_svc, acc_decision_tree]&#125;)models.sort_values(by='Score', ascending=False) Output: Model Score 3 Random Forest 86.76 8 Decision Tree 86.76 1 KNN 84.74 0 Support Vector Machines 83.84 2 Logistic Regression 80.36 7 Linear SVC 79.12 5 Perceptron 78.00 6 Stochastic Gradient Decent 77.67 4 Naive Bayes 72.28 计算结果12345submission = pd.DataFrame(&#123; "PassengerId": test_df["PassengerId"], "Survived": Y_pred &#125;)# submission.to_csv('../output/submission.csv', index=False) 我们提交给比赛网站Kaggle的结果是在6082个样本中命中了3,883个。 参考文献： A journey through Titanic Getting Started with Pandas: Kaggle’s Titanic Competition Titanic Best Working Classifier 【部分表述有待完善。。。】]]></content>
      <categories>
        <category>kaggle</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>kaggle</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo 命令手册]]></title>
    <url>%2F2018%2F03%2F16%2Fhexo%E5%91%BD%E4%BB%A4%E6%89%8B%E5%86%8C%2F</url>
    <content type="text"><![CDATA[安装hexo，新建博客，安装server插件，启动博客 12345$ npm install hexo-cli -g$ hexo init blog$ cd blog$ npm install$ hexo server 草稿 123$ hexo new draft &lt;title&gt;$ hexo server --draft$ hexo publish &lt;title&gt; 插件安装与卸载 12$ npm install &lt;plubin name&gt;$ npm uninstall &lt;plugin name&gt;]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>npm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[797.All Paths From Source to Target]]></title>
    <url>%2F2018%2F03%2F16%2F797-All-Paths-From-Source-to-Target%2F</url>
    <content type="text"><![CDATA[LeetCode: 797.All Paths From Source to Target 问题描述Given a directed, acyclic graph of N nodes. Find all possible paths from node 0 to node N-1, and return them in any order. The graph is given as follows: the nodes are 0, 1, …, graph.length - 1. graph[i] is a list of all nodes j for which the edge (i, j) exists. Example:12345678Input: [[1,2], [3], [3], []] Output: [[0,1,3],[0,2,3]] Explanation: The graph looks like this:0---&gt;1| |v v2---&gt;3There are two paths: 0 -&gt; 1 -&gt; 3 and 0 -&gt; 2 -&gt; 3. Note: The number of nodes in the graph will be in the range [2, 15]. You can print different paths in any order, but you should keep the order of nodes inside one path. 分析使用dfs 代码1234567891011121314151617181920212223242526272829303132333435363738class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; allPathsSourceTarget(int[][] graph) &#123; int n = graph.length; // 结点个数 boolean[] visited = new boolean[n]; // 记录i结点是否被访问过 List&lt;Integer&gt; path = new ArrayList&lt;Integer&gt;(); // 路径 path.add(0); // 初始化路径 List&lt;List&lt;Integer&gt;&gt; result = new ArrayList&lt;List&lt;Integer&gt;&gt;(); dfs(graph, visited, path, 0, n - 1, result); return result; &#125; /** * int[] visited 表示节点i是否被访问过 * List&lt;Integer&gt; path 表示路径 * int curr 表示当前访问的结点 * int target 表示目标结点，也就是graph.length-1 * List&lt;List&lt;Integer&gt;&gt; result 存储所有的满足条件的路径 */ public void dfs(int[][] graph, boolean[] visited, List&lt;Integer&gt; path, int curr, int target, List&lt;List&lt;Integer&gt;&gt; result)&#123; //System.out.println("curr:" + curr); if (curr == target)&#123; result.add(new ArrayList&lt;Integer&gt;(path)); return; &#125; for(int i: graph[curr])&#123; //System.out.println("curr:" + curr + ", visite:" + i + ", status:" + visited[i]); if (!visited[i])&#123; // 如果i未被访问 visited[i] = true; // 访问i节点，将i添加到path中 path.add(i); dfs(graph, visited, path, i, target, result); visited[i] = false; // 不访问i节点，将i从path中删除 path.remove(path.size() - 1); &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[553. Optimal Division]]></title>
    <url>%2F2018%2F03%2F15%2F553-Optimal-Division%2F</url>
    <content type="text"><![CDATA[LeetCode: 553. Optimal Division 问题描述Given a list of positive integers, the adjacent integers will perform the float division. For example, [2,3,4] -&gt; 2 / 3 / 4. However, you can add any number of parenthesis at any position to change the priority of operations. You should find out how to add parenthesis to get the maximum result, and return the corresponding expression in string format. Your expression should NOT contain redundant parenthesis. Example: Input: [1000,100,10,2]Output: “1000/(100/10/2)”Explanation:1000/(100/10/2) = 1000/((100/10)/2) = 200However, the bold parenthesis in “1000/((100/10)/2)” are redundant,since they don’t influence the operation priority. So you should return &gt; “1000/(100/10/2)”. Other cases:1000/(100/10)/2 = 501000/(100/(10/2)) = 501000/100/10/2 = 0.51000/100/(10/2) = 2 Note: The length of the input array is [1, 10]. Elements in the given array will be in range [2, 1000]. There is only one optimal division for each test case. 分析用动态规划求解 代码 只求最大值，不用得到表达式 12345678910111213141516171819202122class Solution(object): def __init__(self): self.mat = [] def optimalDivision(self, nums): """ :type nums: List[int] :rtype: str """ n, mat = len(nums), [] for i in range(n): mat.append([0] * n) for margin in range(1, n): for i in range(n - margin): if margin == 1: mat[i][i+margin] = nums[i] / nums[i+margin] #右上角存最大值 mat[i+margin][i] = nums[i] / nums[i+margin] #左下角存最小值 else: mat[i][i+margin] = max(nums[i] / mat[i+margin][i+1], mat[i][i+margin-1] / nums[i+margin]) mat[i+margin][i] = min(nums[i] / mat[i+1][i+margin], mat[i+margin-1][i] / nums[i+margin]) return mat[0][n-1] 用分治，求得表达式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465class Solution(object): def __init__(self): self.mat = [] self.exp = [] def optimalDivision(self, nums): """ :type nums: List[int] :rtype: str """ def dev(nums, start, end): #print nums, 'start:', start, 'end:', end maxDev, minDev, expMax, expMin = 0, 10e8, '', '' if start == end: #print 'minDev:', nums[start], 'maxDev:', nums[start], 'expMin:', nums[start], 'expMax:', nums[start] self.mat[start][end], self.mat[end][start] = nums[start], nums[start] self.exp[start][end], self.exp[end][start] = str(nums[start]), str(nums[start]) maxDev, minDev, expMax, expMin = nums[start], nums[start], str(nums[start]), str(nums[start]) elif start + 1 == end: valDev = float(nums[start]) / nums[end] expDev = str(nums[start]) + "/" + str(nums[end]) self.mat[start][end] = valDev #右上角存最大值 self.mat[end][start] = valDev #左下角存最小值 self.exp[start][end] = expDev #右上角存最大值 self.exp[end][start] = expDev #左下角存最小值 #print 'minDev:', valDev, 'maxDev:', valDev, 'expMin:', expDev, 'expMax:', expDev maxDev, minDev, expMax, expMin = valDev, valDev, expDev, expDev else: for split in range(start, end): if self.mat[start][split] == 0: left = dev(nums, start, split) else: left = self.mat[start][split], self.mat[split][start], self.exp[start][split], self.exp[split][start] if self.mat[split+1][end] == 0: right = dev(nums, split+1, end) else: right = self.mat[split+1][end], self.mat[end][split+1], self.exp[split+1][end], self.exp[end][split+1] valMin, valMax = float(left[1]) / right[0], float(left[0]) / right[1] if valMin &lt; minDev: minDev = valMin if "/" in right[2]: expMin = left[3] + '/(' + right[2] + ')' else: expMin = left[3] + '/' + right[2] if valMax &gt; maxDev: maxDev = valMax if "/" in right[3]: expMax = left[2] + '/(' + right[3] + ')' else: expMin = left[2] + '/' + right[3] self.mat[start][end], self.mat[end][start] = maxDev, minDev self.exp[start][end], self.exp[end][start] = expMax, expMin #print 'minDev:', minDev, 'maxDev:', maxDev, 'expMin:', expMin, 'expMax:', expMax return maxDev, minDev, expMax, expMin n = len(nums) for i in range(n): self.mat.append([0] * n) self.exp.append([''] * n) result = dev(nums, 0, n-1) return result[2]]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[632.Smallest Range]]></title>
    <url>%2F2018%2F03%2F15%2F632-Smallest-Range%2F</url>
    <content type="text"><![CDATA[LeetCode: 632.Smallest Range 问题描述You have k lists of sorted integers in ascending order. Find the smallest range that includes at least one number from each of the k lists. We define the range [a,b] is smaller than range [c,d] if b-a &lt; d-c or a &lt; c if b-a == d-c. Example 1: Input:[[4,10,15,24,26], [0,9,12,20], [5,18,22,30]]Output: [20,24]Explanation:List 1: [4, 10, 15, 24,26], 24 is in range [20,24].List 2: [0, 9, 12, 20], 20 is in range [20,24].List 3: [5, 18, 22, 30], 22 is in range [20,24]. Note: The given list may contain duplicates, so ascending order means &gt;= here. 1 &lt;= k &lt;= 3500 -105 &lt;= value of elements &lt;= 105. For Java users, please note that the input type has been changed to List&lt;List&gt;. And after you reset the code template, you’ll see this point. 题意给定k个数组，找出一个最小的区间，使得区间内包含每个数字内至少一个数。 分析用一个优先队列，里面存k个分别来自k个数组的数。 每次从队列里弹出一个最小值，并从弹出值的数组里，添加下一个值。 每次弹出时，计算但是的range，如果比之前的小，就替换掉之前的range，作为一个新结果。 队列，能满足区间里同时来自k个数组的k个数； 最小区间，通过这个来满足：每次往里队列里添加的都是同budga下一个数（也就是紧接着最小的数），如果当期range小于上一个range就替换之。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445public class Solution &#123; public int[] smallestRange(List&lt;List&lt;Integer&gt;&gt; nums) &#123; PriorityQueue&lt;Element&gt; priorityQueue = new PriorityQueue&lt;Element&gt;(new Comparator&lt;Element&gt;() &#123; @Override public int compare(Element o1, Element o2) &#123; return o1.value - o2.value; &#125; &#125;); int maxValue = Integer.MIN_VALUE; for (int i = 0; i &lt; nums.size(); i++) &#123; Element element = new Element(nums.get(i).get(0), 0, i); priorityQueue.offer(element); maxValue = Math.max(maxValue, nums.get(i).get(0)); &#125; int range = Integer.MAX_VALUE; int start = -1, end = -1; while(priorityQueue.size() == nums.size())&#123; Element popElement = priorityQueue.poll(); if (maxValue - popElement.value &lt; range) &#123; start = popElement.value; end = maxValue; range = maxValue - popElement.value; &#125; int index = popElement.index + 1; if (index &lt; nums.get(popElement.budge).size())&#123; Element element = new Element(nums.get(popElement.budge).get(index), index, popElement.budge); priorityQueue.offer(element); maxValue = Math.max(maxValue, element.value); &#125; &#125; return new int[]&#123;start, end&#125;; &#125;&#125;class Element&#123; public int value; public int index; public int budge; public Element(int v, int i, int b)&#123; this.value = v; this.index = i; this.budge = b; &#125;&#125;]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LeetCode</tag>
      </tags>
  </entry>
</search>
