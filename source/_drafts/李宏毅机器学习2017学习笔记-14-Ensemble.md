---
title: '李宏毅机器学习2017学习笔记:14.Ensemble'
date: 2018-05-19 23:13:57
updated: 2018-05-19 23:13:57
description:
categories: Machine Learning
tags:
- Machine Learning
- 李宏毅
- Ensemble
- AdaBoost
- Random Forest
---

# Bagging

『复杂模型』的特点是Bias较小、Variance较大，容易Overfit。

Bagging的原理就是：通过将多个『复杂模型』求平均，来降低Variance。

做法是：
1. 训练阶段。在包含`N`个样本的样本集中，做多次重采样，每次采`n`个；
2. 测试阶段。对多个模型的结果求平均或者投票。

## Random Forest

### Descision Tree
只要决策树的深度没有限制，很容易训练出一个错误率为0的决策树模型。但是存在过拟合的缺点。

### Random Forest
随机森林是基于决策树的Bagging，过程如下：
1. 在包含`N`个样本的样本集中，通过重采样的方式采`n`个样本。
2. 对于每次采样，从`M`个特征中随机选择`m`个特征（`m<<M`），在这个样本集上训练一个决策树。
3. 重复`K`次，产生`K`个决策树。
4. 通过投票的方式，进行预测。

**两个『随机性』**
随机森林中引入的两个『随机性』：一个是重采样，另一个是随机选择特征。
这两个『随机性』使随机森林不容易过拟合，并且具有很好的抗噪能力（比如：对缺省值不敏感）。

**两个影响因素**
随机森林的错误率与两个因素有关：
* 森林中任意两棵树的相关性：相关性越小，错误率越小；
* 森林中每棵树的分类能力：分类能力越强，错误率越小。

减小特征选择个数`m`的大小，会减小树的相关性，同时减小分类能力。增加`m`，相关性增加，分类能力增加。所以关键问题是**如何选择最优的m**，这也是随机森林位移的一个参数。

### Out-of-bag（OOB）
如何选择最优的`m`，通过oob error来衡量。OOB就是每次没有被采样的样本集合。

随机森林的一个重要优点是，不需要进行交叉验证，或者不需要用一个独立的测试集来获得误差的无偏估计。它可以在内部进行评估，也就是在生成的过程中在OOB上对误差进行无偏估计。

### 特征选择

#### 基于OOB误差
对某个特征`X`的重要性，计算步骤如下：

1. 对每一棵决策树，计算`OOB`误差，记为`errOOB1`.
2. 随机对`OOB`中所有样本的特征`X`加入噪声干扰，再次计算`OOB`误差，记为`errOOB2`。
3. ​假设森林中有`n`棵树，则特征`X`的重要性为`∑（errOOB2-errOOB1）/ n`。

这个数值之所以能够说明特征的重要性是因为，如果加入随机噪声后，袋外数据准确率大幅度下降（即`errOOB2`上升），说明这个特征对于样本的预测结果有很大影响，进而说明重要程度比较高。

#### 基于Gini指数

[Scikit-learn中几种常用的特征选择方法](https://www.cnblogs.com/hhh5460/p/5186226.html)

# Boosting