---
title: '李宏毅机器学习2017学习笔记:14.Ensemble'
date: 2018-05-19 23:13:57
updated: 2018-05-19 23:13:57
description:
categories: Machine Learning
tags:
- Machine Learning
- 李宏毅
- Ensemble
- AdaBoost
- Random Forest
---

# Bagging

『复杂模型』的特点是Bias较小、Variance较大，容易Overfit。

Bagging的原理就是：通过将多个『复杂模型』求平均，来降低Variance。

做法是：
1. 训练阶段。在包含`N`个样本的样本集中，做多次重采样，每次采`n`个；
2. 测试阶段。对多个模型的结果求平均或者投票。

## Random Forest

### Descision Tree
只要决策树的深度没有限制，很容易训练出一个错误率为0的决策树模型。但是存在过拟合的缺点。

### Random Forest
随机森林是基于决策树的Bagging，过程如下：
1. 在包含`N`个样本的样本集中，通过重采样的方式采`n`个样本。
2. 对于每次采样，从`M`个特征中随机选择`m`个特征（`m<<M`），在这个样本集上训练一个决策树。
3. 重复`K`次，产生`K`个决策树。
4. 通过投票的方式，进行预测。

**两个『随机性』**
随机森林中引入的两个『随机性』：一个是重采样，另一个是随机选择特征。
这两个『随机性』使随机森林不容易过拟合，并且具有很好的抗噪能力（比如：对缺省值不敏感）。

**两个影响因素**
随机森林的错误率与两个因素有关：
* 森林中任意两棵树的相关性：相关性越小，错误率越小；
* 森林中每棵树的分类能力：分类能力越强，错误率越小。

减小特征选择个数`m`的大小，会减小树的相关性，同时减小分类能力。增加`m`，相关性增加，分类能力增加。所以关键问题是**如何选择最优的m**，这也是随机森林位移的一个参数。

### Out-of-bag（OOB）
如何选择最优的`m`，通过oob error来衡量。OOB就是每次没有被采样的样本集合。

随机森林的一个重要优点是，不需要进行交叉验证，或者不需要用一个独立的测试集来获得误差的无偏估计。它可以在内部进行评估，也就是在生成的过程中在OOB上对误差进行无偏估计。

### 特征选择

#### 基于OOB误差
对某个特征`X`的重要性，计算步骤如下：

1. 对每一棵决策树，计算`OOB`误差，记为`errOOB1`.
2. 随机对`OOB`中所有样本的特征`X`加入噪声干扰，再次计算`OOB`误差，记为`errOOB2`。
3. ​假设森林中有`n`棵树，则特征`X`的重要性为`∑（errOOB2-errOOB1）/ n`。

这个数值之所以能够说明特征的重要性是因为，如果加入随机噪声后，袋外数据准确率大幅度下降（即`errOOB2`上升），说明这个特征对于样本的预测结果有很大影响，进而说明重要程度比较高。

#### 基于Gini指数

[Scikit-learn中几种常用的特征选择方法](https://www.cnblogs.com/hhh5460/p/5186226.html)

# Boosting
Boosting将多个『弱模型』进行组合，得到一个强分类器。

**理论前提：**如果一个分类器的错误率小于0.5，经过Boosting后，它就能得到一个错误率为0的分类器。

对于二分类而言，我们总是能找到一个$\varepsilon < 0.5$的分类器。因为如果$\varepsilon > 0.5$，只需要将预测结果翻转，就能使$\varepsilon < 0.5$了。

**Boosting的框架：**
1. 得到第一个弱分类器$f_1(x)$
2. 找到另一个分类器$f_2(x)$去辅助$f_1(x)$。$f_2(x)$起到对$f_1(x)$弥补的作用。
3. 按照以上方法，**依次**得到`k`分类器$f_3(x), f_4(x)$...$f_k(x)$

**备注：**
* Bagging的每个子分类器都是强分类器；Boosting的是弱分类器。
* Bagging的分类器是无序的；Boosting的则是序列的。

**在上一个分类器的基础上，如何得到不同的分类器？**：制造不同的训练数据集。
具体方法：
1. 重采样（也可以理解成改变样本的权重）
2. 改变每个样本的权重
3. 在改变权重的基础上，也要响应地修改损失函数.
<img src="reweight_1.jpg" width="300px">

## AdaBoosting
AdaBoosting的原理是通过改变数据的分布，来使上次被分错的样本被分对。

具体做法是：根据模型的准确率和样本是否被分对，来修改样本的权重。如果某个样本被分错，则增加该样本的权重；否则降低该样本的权重。

假设：$f_1(x)$的错误率为：$\varepsilon_1$
$$\varepsilon_1 = \frac {\sum_n u_1^n \delta(f_1(x^n) \neq \hat y^n)} {Z_1}$$
其中，$u_1$为样本的权重，$Z_1$是权重的和，$Z_1=\sum_n u_1^n$

为了使$f_2(x)$对$f_1(x)$起到『互补』的作用，我们要增加分错的样本的权重。
同时，更新权重会使$f_1(x)$的错误率上升。上升到什么程度呢？最多达到$\varepsilon_1 = 0.5$，也就是：
$$\frac {\sum_n u_2^n \delta(f_1(x^n) \neq \hat y^n)} {Z_2} = 0.5$$
更新权重后，训练得到$f_2(x)$.

举个例子，如下图：
1. 第一轮，每个样本的权重都为`1`，$f_1(x)$的错误率$\varepsilon_1=0.25$。
2. 更新权重后，$f_1(x)$的错误率变为$\varepsilon_1=0.5$，$f_1(x)$的$\varepsilon_1<0.5$
<img src="reweight_2.jpg" width="500px">

### 如何更新权重
分错的样本权重增加，分对的样本权重降低。

设:更新前的权重为$u_1$，更新后的权重为$u_2$

更新前：
<img src="reweight_3.jpg" width="300px">

更新权重：
<img src="reweight_4.jpg" width="300px">

更新后，$f_1(x)$的$\varepsilon_1=0.5$：
<img src="reweight_5.jpg" width="300px">

上式的『分子部分』是分错的部分：
<img src="reweight_6.jpg" width="300px">
『分母部分』是分错+分对两部分：
<img src="reweight_7.jpg" width="450px">

也就是：
<img src="reweight_8.jpg" width="400px">

变换，提取出$d_1$：
<img src="reweight_9.jpg" width="300px">

$d_1$的平方，就是『分对样本的个数』除以『分错样本的个数』：
<img src="reweight_10.jpg" width="180px">
所以，可以求出$d_1$，大于1：
<img src="reweight_11.jpg" width="200px">

设：$\alpha = \ln d$，则权重更新时乘以$exp(\alpha)$或$exp(-\alpha)$，使表达式更简洁。
<img src="reweight_12.jpg" width="300px">

### AdaBoost的步骤
1. 训练`T`个弱分类器：
<img src="adaboost_1.jpg" width="500px">
2. 将`T`个弱分类器进行加权平均:
<img src="adaboost_2.jpg" width="200px">
对于每个弱分类器而言，错误率越低，$\alpha$越大，表示这个弱分类器的权重更大。这是合情合理的。

### 最终的分类器的错误率

