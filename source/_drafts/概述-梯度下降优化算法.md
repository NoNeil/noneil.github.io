---
title: '概述:梯度下降优化算法'
date: 2018-06-17 12:46:14
updated: 2018-06-17 12:46:14
description: 我们首先要研究的是不同的梯度下降法。 然后，我们将简要概述在培训期间的挑战。 随后，我们将介绍最常见的优化算法，展示它们解决这些挑战的动机，以及这如何导致他们的更新规则的推导。 我们还将对算法和架构进行简短的研究，以优化并行和分布式设置中的梯度下降法。 最后，我们将考虑有助于优化梯度下降法的其他策略。
categories: Machine Learning
tags:
  - Machine Learning
  - Gradient Descent
---

梯度下降法是最流行的优化算法之一，也是迄今为止最常用的优化神经网络的方法。同时，每一个最先进的深度学习库都包含了多种优化梯度下降法的实现(如Lasagne、Caffe、Keras)。

梯度是使目标函数增大的方向，梯度下降法就是沿着梯度的反方向去更新参数，使目标函数越来越小。
$$\theta_{t+1} = \theta_t - \eta \nabla_\theta J(\theta)$$
其中，梯度是决定目标函数改变的方向，$\eta$决定了每次更新的步长。

# 梯度下降法的三个变种

## Batch gradient descent（批量梯度下降法）
批量梯度下降法每次计算梯度时，都是使用整个训练集的所有样本。
$$\theta_{t+1} = \theta_t - \eta \nabla_\theta J(\theta)$$
批量梯度下降法每更新一次参数，都需要对所有样本进行一次梯度的计算，所以它比较慢。而且当数据量很大时，对内存的要求也比较高。

也不支持在线更新模型。

Python代码：
```Python
for i in range(nb_epochs):
  params_grad = evaluate_gradient(loss_function, data, params)
  params = params - learning_rate * params_grad
```

## Stochastic gradient descent（随机梯度下降法）
批量梯度下降法(SGD)每次计算梯度时，都只使用其中一个训练样本。
$$\theta_{t+1} = \theta_t - \eta \nabla_\theta J(\theta; x^{(i)}; y^{(i)})$$

SGD比批量梯度下降法更快，也能用于在线学习。

但是SGD的参数更新太频繁，会导致参数更新时产生较大的方差（参数波动大），从而会使目标函数产生剧烈的波动，如下图。
<img src="sgd_fluctuation.png" width="400px">

当批量梯度下降法收敛到局部最小值时，SGD的波动效应，一方面能使参数跳跃到新的或潜在更好的局部最小值；另一方面，这最终会使参数收敛到正确的最小值。然而，已经表明，当我们慢慢地降低学习率时，SGD显示出与批梯度下降相同的收敛行为，几乎可以肯定地收敛到非凸和凸优化的局部或全局最小值。

Python代码：
```python
for i in range(nb_epochs):
  np.random.shuffle(data)
  for example in data:
    params_grad = evaluate_gradient(loss_function, example, params)
    params = params - learning_rate * params_grad
```

## Mini-batch gradient descent（小批量梯度下降法）
小批量梯度下降法是兼顾了批量梯度下降法和SGD，每次更新参数时，使用一小批`n`个样本来计算梯度。
$$\theta_{t+1} = \theta_t - \eta \nabla_\theta J(\theta; x^{(i:i+n)}; y^{(i:i+n)})$$

优点：
* 降低了参数更新的方差，会使收敛更加稳定；
* 小批量数据可以使用高度优化的矩阵操作来进行计算。

一般批量的大小选择`50-256`之间，根据训练集的大小而定。
Python代码：
```python
for i in range(nb_epochs):
  np.random.shuffle(data)
  for batch in get_batches(data, batch_size=50):
    params_grad = evaluate_gradient(loss_function, batch, params)
    params = params - learning_rate * params_grad
```

# 存在的问题
小批量梯度下降法不能保证良好的收敛性，在实际使用过程中，面临如下挑战：
1. 如何选择合适的学习率？
2. 如何对不同的参数应用不同的学习率？因为，当数据稀疏并且特征具有不同的频次时，我们可能不希望将对所有的参数执行相同程度的更新，而是对于很少出现的特征，进行大幅地更新。
3. 如何解决局部最小值和鞍点的问题？Dauphin等人 [19]认为，事实上问题往往并不是由局部最小值产生，而是由于鞍点导致的。

# 梯度下降优化算法

## Momentum（动量）
SGD在遇到『峡谷』的时候，会表现出在峡谷斜坡上反复摇摆的情况。如下图：
<img src = "without_momentum.gif" width="400px">

加上『动量』之后，有助于抑制这种振荡。
$$v_t = \gamma v_{t-1} + \eta \nabla_\theta J(\theta) $$
$$\theta = \theta - v_t$$
其中，$\gamma$一般取0.9
根据『平行四边形法则』，加上『动量』后的方向，如下图：
<img src = "with_momentum_2.jpg" width="400px">

『动量』有一个很好的特性：对于梯度方向不变的维度，动量一直增加；对于梯度方向改变的维度，动量减少，参数更新的幅度也小。
因此，『动量』能加速收敛，减少振荡。

除此之外，当遇到局部最小值时，如果『动量』足够大，也能越过这个局部最小值点。

## Nesterov accelerated gradient（NAG）
然而，一个滚下山坡的球，盲目地沿着斜坡滚下去，是非常不能令人满意的。 我们希望有一个更聪明的球，一个球，它有一个关于它将去哪里的概念，这样它就知道在山坡再次上升之前减速。

> Momentum与NAG的区别：
> * 基于动量的方法，在计算算当前的速度方向时，是用『当前点』的梯度方向+上一次的速度方向。
> * 而 Nesterov accelerated gradient（NAG）在计算当前的速度方向时，是用『更新后的点的』的梯度方向+上一次的速度方向。
>
> 但是，『更新后的点的』的梯度方向未知，所以通过 $\nabla_\theta J(\theta - \gamma v_{t-1})$ 来近似计算。

$$v_t = \gamma v_{t-1} + \eta \nabla_\theta J(\theta - \gamma v_{t-1}) $$
$$\theta = \theta - v_t$$

## Adagrad

Adagrad对不同维度的参数，使用不同的学习率。
Adagrad对频率高的特征进行小的更新，对频率低的特征进行大的更新。

Dean([Dean, J., Corrado, G. S., Monga, R., Chen, K., Devin, M., Le, Q. V, … Ng, A. Y. (2012). Large Scale Distributed Deep Networks. NIPS 2012: Neural Information Processing Systems, 1–11. ](http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf) )等人发现，Adagrad 极大地提高了 SGD 的鲁棒性，并用它在谷歌(Google)训练大规模神经网络，这些网络学会了在 Youtube 视频中识别猫。 此外，Pennington 等人使用 Adagrad 来训练GloVe word embeddings，因为不频繁的出现的单词需要比常用词获得更大的更新。

我们用 $g_{t,i}$ 表示：第 $t$ 次迭代时，目标函数对参数的第 $i$ 维的偏导。
$$ g_{t,i} = \nabla_\theta J(\theta_{t,i}) $$

如果在SGD中对每一维参数分别更新的话，公式如下：
$$ \theta_{t+1,i} = \theta_{t,i} - \eta \cdot g_{t,i}$$

不同的是，Adagrad的步长是基于过去所有的梯度计算的，然后对每一维参数分别更新：
$$ \theta_{t+1,i} = \theta_{t,i} - \frac \eta {\sqrt{G_{t, ii} + \epsilon}} \cdot g_{t,i}$$

其中，$G_{t}$ 是对角矩阵，每个元素 $G_{t,ii}$ 都是过去所有梯度的平方和。 $\epsilon$ 是个拉普拉斯平滑常数。这里的平方根运算很重要，如果没有的话，算法的表现会很糟糕。

Adagrad的主要缺点是它在分母中不断积累的平方梯度: 由于每一个增加的项都是正的，积累的总和在迭代中不断增长。 这反过来又导致学习率不断减小，最终变得极小，使参数无法继续有效地更新。 下面的算法旨在解决这个缺陷。

## Adadelta

## RMSprop

## Adam

## AdaMax

## Nadam

## AMSGrad

## 可视化对比


# 并行和分布式SGD

# 更多优化SGD的策略

# 结论